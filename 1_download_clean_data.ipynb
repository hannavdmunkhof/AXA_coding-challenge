{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1487e09e",
   "metadata": {},
   "source": [
    "# AXA coding challenge\n",
    "Data:\n",
    "1. Citibike: https://s3.amazonaws.com/tripdata/index.html\n",
    "2. NYPD:  https://data.cityofnewyork.us/Public-Safety/Motor-Vehicle-Collisions-Crashes/h9gi-nx95/about_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6be0356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge\n"
     ]
    }
   ],
   "source": [
    "# Install packages (only once)\n",
    "#!pip install selenium webdriver-manager\n",
    "\n",
    "# Import modules\n",
    "import os # basic\n",
    "import datetime\n",
    "import zipfile\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "import dask.dataframe as dd\n",
    "\n",
    "from selenium import webdriver # for downloading files automatically\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "current_dir = os.getcwd() # current dir\n",
    "print('Current directory: ' + current_dir)\n",
    "extract_dir = current_dir + '/data/bike-tripdata'  # directory where extracted files from 1. will be saved\n",
    "cleaned_dir = extract_dir + '_cleaned' # directory where cleaned and concatenated df will be saved\n",
    "\n",
    "#pd.options.display.float_format = '{:.4f}'.format # set pd output to 2 decimals\n",
    "pd.reset_option('display.float_format')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "921e8034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "# to download files from an url\n",
    "def download_files(url, save_path):\n",
    "    response = requests.get(url, stream=True)\n",
    "    with open(save_path, 'wb') as file:\n",
    "        for chunk in response.iter_content(chunk_size=1024):\n",
    "            if chunk:\n",
    "                file.write(chunk)\n",
    "    print(f\"Downloaded {save_path}\")\n",
    "    \n",
    "# to clean column names\n",
    "def clean_column_names(df, column_mapping=None):\n",
    "    # strip whitespace, convert to lowercase, and replace spaces with underscores\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('-', '_')\n",
    "    \n",
    "    # apply manual column mapping if specified\n",
    "    if column_mapping:\n",
    "        df.rename(columns=column_mapping, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# check unique column names across all .csv files in list_files \n",
    "def list_unique_col_names(list_files):\n",
    "    unique_column_names = []\n",
    "    for csv_file in list_files:\n",
    "        file_path = os.path.join(extract_dir, csv_file)\n",
    "        df = pd.read_csv(file_path, nrows=1)\n",
    "        #print(df.columns) # visual check\n",
    "        [unique_column_names.append(col) for col in df.columns if col not in unique_column_names]\n",
    "    unique_column_names.sort()\n",
    "    \n",
    "    return unique_column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1af9203",
   "metadata": {},
   "source": [
    "## Download Citibike data automatically from url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e14c0153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "url = \"https://s3.amazonaws.com/tripdata/index.html\" # url to data files\n",
    "driver_path = 'C:/Drivers/chromedriver-win64_128/chromedriver.exe' # Chrome driver for web interaction, needed by selenium - must match Chrome version\n",
    "\n",
    "# Download files\n",
    "service = Service(driver_path) # initialize the Chrome driver\n",
    "driver = webdriver.Chrome(service=service)\n",
    "driver.get(url) # navigate to website\n",
    "time.sleep(5)  # give the page time to load the dynamic content\n",
    "html = driver.page_source # get the page source after JavaScript has executed\n",
    "soup = BeautifulSoup(html, 'html.parser') # parse the HTML\n",
    "\n",
    "# find all .zip links\n",
    "file_links = []\n",
    "for link in soup.find_all('a', href=True):\n",
    "    if link['href'].endswith('.zip'): # on this website, files are .zip format\n",
    "        file_links.append(link['href'])\n",
    "print(file_links[:2]) # check if the file paths are retrieved correctly by printing a few\n",
    "\n",
    "driver.quit() # close the browser\n",
    "\n",
    "if not os.path.exists(current_dir+'/downloads'): # directory to save the downloaded files\n",
    "    os.makedirs(current_dir+'/downloads')\n",
    "\n",
    "for file_link in file_links: # loop through all the zip links and download them\n",
    "    filename = os.path.join(current_dir+'/downloads', os.path.basename(file_link))\n",
    "    \n",
    "    if not file_link.startswith('http'): # if the link is relative, make it an absolute URL by appending the base URL\n",
    "        file_link = url + file_link\n",
    "\n",
    "    download_files(file_link, filename) # download the file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea75400",
   "metadata": {},
   "source": [
    "## Unzip & reorganize files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381447ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - alternatively (instead of next cell), unzip first and then reorganize files\n",
    "\n",
    "# Unzip files  \n",
    "# zip_dir = current_dir+'/downloads' # directory containing the zip files\n",
    "# extract_dir = current_dir+'/data' # directory where extracted files will be saved\n",
    "\n",
    "# for filename in os.listdir(zip_dir): # loop through all files in the directory\n",
    "#     if filename.endswith('.zip') :\n",
    "#         zip_file_path = os.path.join(zip_dir, filename)\n",
    "#         new_file_path = extract_dir + '/' + filename[:-4] + '.csv' # remove '.zip' and subfolders from the target path name\n",
    "#         os.makedirs(new_file_path, exist_ok=True)  # create the directory if it doesn't exist\n",
    "\n",
    "#         with zipfile.ZipFile(zip_file_path, 'r') as zip_ref: # extract the zip file\n",
    "#             for member in zip_ref.namelist():\n",
    "#                 if '_MACOSX' not in member: # skip any file or folder inside \"_MACOSX\" (for MAC computers, not needed)\n",
    "#                     zip_ref.extract(member, new_file_path) # extract to the specified directory\n",
    "\n",
    "#             print(f'Extracted: {member} to {new_file_path}')\n",
    "\n",
    "\n",
    "# # Move  files from subfolders in subfolders to 1 folder\n",
    "\n",
    "# import shutil\n",
    "\n",
    "# source_dir = current_dir + '/data'\n",
    "# destination_dir = current_dir + '/data_test'\n",
    "# os.makedirs(destination_dir, exist_ok=True)\n",
    "\n",
    "# for root, dirs, files in os.walk(source_dir):\n",
    "#     for file in files:\n",
    "#         if file.endswith('.csv') and not file.startswith('.'): # select .csv files, skip files starting with '.' \n",
    "#             if '_MACOSX' in root:\n",
    "#                 continue  # skip this directory and its contents, for MAC\n",
    "\n",
    "#             source_file = os.path.join(root, file)\n",
    "#             destination_file = os.path.join(destination_dir, file)\n",
    "            \n",
    "#             shutil.move(source_file, destination_file) # or shutil.copy\n",
    "#             print(f\"Moved: {source_file} -> {destination_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1c46ca43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 201309-citibike-tripdata.csv\n",
      "Extracted 201311-citibike-tripdata.csv\n",
      "Extracted 201307-citibike-tripdata.csv\n",
      "Extracted 201308-citibike-tripdata.csv\n",
      "Extracted 201306-citibike-tripdata.csv\n",
      "Extracted 201310-citibike-tripdata.csv\n",
      "Extracted 201312-citibike-tripdata.csv\n",
      "Extracted 201312-citibike-tripdata_1.csv\n",
      "Extracted 201311-citibike-tripdata_1.csv\n",
      "Extracted 201307-citibike-tripdata_1.csv\n",
      "Extracted 201310-citibike-tripdata_2.csv\n",
      "Extracted 201310-citibike-tripdata_1.csv\n",
      "Extracted 201309-citibike-tripdata_2.csv\n",
      "Extracted 201309-citibike-tripdata_1.csv\n",
      "Extracted 201308-citibike-tripdata_1.csv\n",
      "Extracted 201308-citibike-tripdata_2.csv\n",
      "Extracted 201306-citibike-tripdata_1.csv\n",
      "from 2013-citibike-tripdata.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted 201404-citibike-tripdata_1.csv\n",
      "Extracted 201412-citibike-tripdata_1.csv\n",
      "Extracted 201411-citibike-tripdata_1.csv\n",
      "Extracted 201407-citibike-tripdata_1.csv\n",
      "Extracted 201410-citibike-tripdata_1.csv\n",
      "Extracted 201409-citibike-tripdata_1.csv\n",
      "Extracted 201408-citibike-tripdata_1.csv\n",
      "Extracted 201406-citibike-tripdata_1.csv\n",
      "Extracted 201403-citibike-tripdata_1.csv\n",
      "Extracted 201401-citibike-tripdata_1.csv\n",
      "Extracted 201402-citibike-tripdata_1.csv\n",
      "Extracted 201405-citibike-tripdata_1.csv\n",
      "from 2014-citibike-tripdata.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted 201504-citibike-tripdata_1.csv\n",
      "Extracted 201512-citibike-tripdata_1.csv\n",
      "Extracted 201511-citibike-tripdata_1.csv\n",
      "Extracted 201507-citibike-tripdata_1.csv\n",
      "Extracted 201507-citibike-tripdata_2.csv\n",
      "Extracted 201510-citibike-tripdata_2.csv\n",
      "Extracted 201510-citibike-tripdata_1.csv\n",
      "Extracted 201509-citibike-tripdata_2.csv\n",
      "Extracted 201509-citibike-tripdata_1.csv\n",
      "Extracted 201508-citibike-tripdata_1.csv\n",
      "Extracted 201508-citibike-tripdata_2.csv\n",
      "Extracted 201506-citibike-tripdata_1.csv\n",
      "Extracted 201503-citibike-tripdata_1.csv\n",
      "Extracted 201501-citibike-tripdata_1.csv\n",
      "Extracted 201502-citibike-tripdata_1.csv\n",
      "Extracted 201505-citibike-tripdata_1.csv\n",
      "from 2015-citibike-tripdata.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted 201604-citibike-tripdata_1.csv\n",
      "Extracted 201604-citibike-tripdata_2.csv\n",
      "Extracted 201612-citibike-tripdata_1.csv\n",
      "Extracted 201611-citibike-tripdata_2.csv\n",
      "Extracted 201611-citibike-tripdata_1.csv\n",
      "Extracted 201607-citibike-tripdata_2.csv\n",
      "Extracted 201607-citibike-tripdata_1.csv\n",
      "Extracted 201610-citibike-tripdata_1.csv\n",
      "Extracted 201610-citibike-tripdata_2.csv\n",
      "Extracted 201609-citibike-tripdata_1.csv\n",
      "Extracted 201609-citibike-tripdata_2.csv\n",
      "Extracted 201608-citibike-tripdata_2.csv\n",
      "Extracted 201608-citibike-tripdata_1.csv\n",
      "Extracted 201606-citibike-tripdata_1.csv\n",
      "Extracted 201606-citibike-tripdata_2.csv\n",
      "Extracted 201603-citibike-tripdata_1.csv\n",
      "Extracted 201601-citibike-tripdata_1.csv\n",
      "Extracted 201602-citibike-tripdata_1.csv\n",
      "Extracted 201605-citibike-tripdata_2.csv\n",
      "Extracted 201605-citibike-tripdata_1.csv\n",
      "from 2016-citibike-tripdata.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted 201704-citibike-tripdata.csv_1.csv\n",
      "Extracted 201704-citibike-tripdata.csv_2.csv\n",
      "Extracted 201712-citibike-tripdata.csv_1.csv\n",
      "Extracted 201711-citibike-tripdata.csv_2.csv\n",
      "Extracted 201711-citibike-tripdata.csv_1.csv\n",
      "Extracted 201707-citibike-tripdata.csv_2.csv\n",
      "Extracted 201707-citibike-tripdata.csv_1.csv\n",
      "Extracted 201710-citibike-tripdata.csv_1.csv\n",
      "Extracted 201710-citibike-tripdata.csv_2.csv\n",
      "Extracted 201709-citibike-tripdata.csv_2.csv\n",
      "Extracted 201709-citibike-tripdata.csv_1.csv\n",
      "Extracted 201708-citibike-tripdata.csv_1.csv\n",
      "Extracted 201708-citibike-tripdata.csv_2.csv\n",
      "Extracted 201706-citibike-tripdata.csv_1.csv\n",
      "Extracted 201706-citibike-tripdata.csv_2.csv\n",
      "Extracted 201703-citibike-tripdata.csv_1.csv\n",
      "Extracted 201701-citibike-tripdata.csv_1.csv\n",
      "Extracted 201702-citibike-tripdata.csv_1.csv\n",
      "Extracted 201705-citibike-tripdata.csv_2.csv\n",
      "Extracted 201705-citibike-tripdata.csv_1.csv\n",
      "from 2017-citibike-tripdata.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted 201809-citibike-tripdata.csv\n",
      "Extracted 201801-citibike-tripdata.csv\n",
      "Extracted 201803-citibike-tripdata.csv\n",
      "Extracted 201805-citibike-tripdata.csv\n",
      "Extracted 201807-citibike-tripdata.csv\n",
      "Extracted 201811-citibike-tripdata.csv\n",
      "Extracted 201804-citibike-tripdata_1.csv\n",
      "Extracted 201804-citibike-tripdata_2.csv\n",
      "Extracted 201808-citibike-tripdata.csv\n",
      "Extracted 201802-citibike-tripdata.csv\n",
      "Extracted 201812-citibike-tripdata.csv\n",
      "Extracted 201804-citibike-tripdata.csv\n",
      "Extracted 201810-citibike-tripdata.csv\n",
      "Extracted 201806-citibike-tripdata.csv\n",
      "Extracted 201804-citibike-tripdata_1.csv\n",
      "Extracted 201804-citibike-tripdata_2.csv\n",
      "Extracted 201812-citibike-tripdata_1.csv\n",
      "Extracted 201812-citibike-tripdata_2.csv\n",
      "Extracted 201811-citibike-tripdata_2.csv\n",
      "Extracted 201811-citibike-tripdata_1.csv\n",
      "Extracted 201807-citibike-tripdata_2.csv\n",
      "Extracted 201807-citibike-tripdata_1.csv\n",
      "Extracted 201810-citibike-tripdata_1.csv\n",
      "Extracted 201810-citibike-tripdata_2.csv\n",
      "Extracted 201809-citibike-tripdata_1.csv\n",
      "Extracted 201809-citibike-tripdata_2.csv\n",
      "Extracted 201808-citibike-tripdata_2.csv\n",
      "Extracted 201808-citibike-tripdata_1.csv\n",
      "Extracted 201806-citibike-tripdata_1.csv\n",
      "Extracted 201806-citibike-tripdata_2.csv\n",
      "Extracted 201803-citibike-tripdata_1.csv\n",
      "Extracted 201801-citibike-tripdata_1.csv\n",
      "Extracted 201802-citibike-tripdata_1.csv\n",
      "Extracted 201805-citibike-tripdata_2.csv\n",
      "Extracted 201805-citibike-tripdata_1.csv\n",
      "from 2018-citibike-tripdata.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted 201904-citibike-tripdata_2.csv\n",
      "Extracted 201904-citibike-tripdata_1.csv\n",
      "Extracted 201912-citibike-tripdata_1.csv\n",
      "Extracted 201911-citibike-tripdata_1.csv\n",
      "Extracted 201911-citibike-tripdata_2.csv\n",
      "Extracted 201907-citibike-tripdata_1.csv\n",
      "Extracted 201907-citibike-tripdata_3.csv\n",
      "Extracted 201907-citibike-tripdata_2.csv\n",
      "Extracted 201910-citibike-tripdata_3.csv\n",
      "Extracted 201910-citibike-tripdata_2.csv\n",
      "Extracted 201910-citibike-tripdata_1.csv\n",
      "Extracted 201909-citibike-tripdata_3.csv\n",
      "Extracted 201909-citibike-tripdata_2.csv\n",
      "Extracted 201909-citibike-tripdata_1.csv\n",
      "Extracted 201908-citibike-tripdata_1.csv\n",
      "Extracted 201908-citibike-tripdata_2.csv\n",
      "Extracted 201908-citibike-tripdata_3.csv\n",
      "Extracted 201906-citibike-tripdata_2.csv\n",
      "Extracted 201906-citibike-tripdata_3.csv\n",
      "Extracted 201906-citibike-tripdata_1.csv\n",
      "Extracted 201903-citibike-tripdata_1.csv\n",
      "Extracted 201903-citibike-tripdata_2.csv\n",
      "Extracted 201901-citibike-tripdata_1.csv\n",
      "Extracted 201902-citibike-tripdata_1.csv\n",
      "Extracted 201905-citibike-tripdata_1.csv\n",
      "Extracted 201905-citibike-tripdata_2.csv\n",
      "from 2019-citibike-tripdata.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "from 2020-citibike-tripdata.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "from 2021-citibike-tripdata.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "from 2022-citibike-tripdata.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "from 2023-citibike-tripdata.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted 202401-citibike-tripdata.csv\n",
      "from 202401-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted 202402-citibike-tripdata.csv\n",
      "from 202402-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted 202403-citibike-tripdata.csv\n",
      "from 202403-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted 202404-citibike-tripdata.csv\n",
      "from 202404-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted 202405-citibike-tripdata_1.csv\n",
      "Extracted 202405-citibike-tripdata_2.csv\n",
      "Extracted 202405-citibike-tripdata_3.csv\n",
      "Extracted 202405-citibike-tripdata_4.csv\n",
      "Extracted 202405-citibike-tripdata_5.csv\n",
      "from 202405-citibike-tripdata.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted 202406-citibike-tripdata_5.csv\n",
      "Extracted 202406-citibike-tripdata_4.csv\n",
      "Extracted 202406-citibike-tripdata_1.csv\n",
      "Extracted 202406-citibike-tripdata_3.csv\n",
      "Extracted 202406-citibike-tripdata_2.csv\n",
      "from 202406-citibike-tripdata.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 202407-citibike-tripdata_1.csv\n",
      "Extracted 202407-citibike-tripdata_2.csv\n",
      "Extracted 202407-citibike-tripdata_3.csv\n",
      "Extracted 202407-citibike-tripdata_4.csv\n",
      "Extracted 202407-citibike-tripdata_5.csv\n",
      "from 202407-citibike-tripdata.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted 202408-citibike-tripdata_3.csv\n",
      "Extracted 202408-citibike-tripdata_2.csv\n",
      "Extracted 202408-citibike-tripdata_1.csv\n",
      "Extracted 202408-citibike-tripdata_5.csv\n",
      "Extracted 202408-citibike-tripdata_4.csv\n",
      "from 202408-citibike-tripdata.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201509-citibike-tripdata.csv\n",
      "from JC-201509-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201510-citibike-tripdata.csv\n",
      "from JC-201510-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201511-citibike-tripdata.csv\n",
      "from JC-201511-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201512-citibike-tripdata.csv\n",
      "from JC-201512-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-20161-citibike-tripdata.csv\n",
      "from JC-201601-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-20162-citibike-tripdata.csv\n",
      "from JC-201602-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-20163-citibike-tripdata.csv\n",
      "from JC-201603-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201604-citibike-tripdata.csv\n",
      "from JC-201604-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201605-citibike-tripdata.csv\n",
      "from JC-201605-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201606-citibike-tripdata.csv\n",
      "from JC-201606-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201607-citibike-tripdata.csv\n",
      "from JC-201607-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201608-citibike-tripdata.csv\n",
      "from JC-201608-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201609-citibike-tripdata.csv\n",
      "from JC-201609-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201610-citibike-tripdata.csv\n",
      "from JC-201610-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201611-citibike-tripdata.csv\n",
      "from JC-201611-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201612-citibike-tripdata.csv\n",
      "from JC-201612-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201701-citibike-tripdata.csv\n",
      "from JC-201701-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201702-citibike-tripdata.csv\n",
      "from JC-201702-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201703-citibike-tripdata.csv\n",
      "from JC-201703-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201704-citibike-tripdata.csv\n",
      "from JC-201704-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201705-citibike-tripdata.csv\n",
      "from JC-201705-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201706-citibike-tripdata.csv\n",
      "from JC-201706-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201707-citibike-tripdata.csv\n",
      "from JC-201707-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201708 citibike-tripdata.csv\n",
      "from JC-201708 citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201709-citibike-tripdata.csv\n",
      "from JC-201709-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201710-citibike-tripdata.csv\n",
      "from JC-201710-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201711-citibike-tripdata.csv\n",
      "from JC-201711-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201712-citibike-tripdata.csv\n",
      "from JC-201712-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201801-citibike-tripdata.csv\n",
      "from JC-201801-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201802-citibike-tripdata.csv\n",
      "from JC-201802-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201803-citibike-tripdata.csv\n",
      "from JC-201803-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201804-citibike-tripdata.csv\n",
      "from JC-201804-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201805-citibike-tripdata.csv\n",
      "from JC-201805-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201806-citibike-tripdata.csv\n",
      "from JC-201806-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201807-citibike-tripdata.csv\n",
      "from JC-201807-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201808-citibike-tripdata.csv\n",
      "from JC-201808-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201809-citibike-tripdata.csv\n",
      "from JC-201809-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201810-citibike-tripdata.csv\n",
      "from JC-201810-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201811-citibike-tripdata.csv\n",
      "from JC-201811-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201812-citibike-tripdata.csv\n",
      "from JC-201812-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201901-citibike-tripdata.csv\n",
      "from JC-201901-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201902-citibike-tripdata.csv\n",
      "from JC-201902-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201903-citibike-tripdata.csv\n",
      "from JC-201903-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201904-citibike-tripdata.csv\n",
      "from JC-201904-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201905-citibike-tripdata.csv\n",
      "from JC-201905-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201906-citibike-tripdata.csv\n",
      "from JC-201906-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201907-citibike-tripdata.csv\n",
      "from JC-201907-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201908-citibike-tripdata.csv\n",
      "from JC-201908-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201909-citibike-tripdata.csv\n",
      "from JC-201909-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201910-citibike-tripdata.csv\n",
      "from JC-201910-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201911-citibike-tripdata.csv\n",
      "from JC-201911-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201912-citibike-tripdata.csv\n",
      "from JC-201912-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202001-citibike-tripdata.csv\n",
      "from JC-202001-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202002-citibike-tripdata.csv\n",
      "from JC-202002-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202003-citibike-tripdata.csv\n",
      "from JC-202003-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202004-citibike-tripdata.csv\n",
      "from JC-202004-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202005-citibike-tripdata.csv\n",
      "from JC-202005-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202006-citibike-tripdata.csv\n",
      "from JC-202006-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted JC-202007-citibike-tripdata.csv\n",
      "from JC-202007-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202008-citibike-tripdata.csv\n",
      "from JC-202008-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202009-citibike-tripdata.csv\n",
      "from JC-202009-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202010-citibike-tripdata.csv\n",
      "from JC-202010-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202011-citibike-tripdata.csv\n",
      "from JC-202011-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202012-citibike-tripdata.csv\n",
      "from JC-202012-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202101-citibike-tripdata.csv\n",
      "from JC-202101-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202102-citibike-tripdata.csv\n",
      "from JC-202102-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202103-citibike-tripdata.csv\n",
      "from JC-202103-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202104-citibike-tripdata.csv\n",
      "from JC-202104-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202105-citibike-tripdata.csv\n",
      "from JC-202105-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202106-citibike-tripdata.csv\n",
      "from JC-202106-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202107-citibike-tripdata.csv\n",
      "from JC-202107-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202108-citibike-tripdata.csv\n",
      "from JC-202108-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202109-citibike-tripdata.csv\n",
      "from JC-202109-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202110-citibike-tripdata.csv\n",
      "from JC-202110-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202111-citibike-tripdata.csv\n",
      "from JC-202111-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202112-citibike-tripdata.csv\n",
      "from JC-202112-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202201-citibike-tripdata.csv\n",
      "from JC-202201-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202202-citibike-tripdata.csv\n",
      "from JC-202202-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202203-citibike-tripdata.csv\n",
      "from JC-202203-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202204-citibike-tripdata.csv\n",
      "from JC-202204-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202205-citibike-tripdata.csv\n",
      "from JC-202205-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202206-citibike-tripdata.csv\n",
      "from JC-202206-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202207-citbike-tripdata.csv\n",
      "from JC-202207-citbike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202208-citibike-tripdata.csv\n",
      "from JC-202208-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202209-citibike-tripdata.csv\n",
      "from JC-202209-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202210-citibike-tripdata.csv\n",
      "from JC-202210-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202211-citibike-tripdata.csv\n",
      "from JC-202211-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202212-citibike-tripdata.csv\n",
      "from JC-202212-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202301-citibike-tripdata.csv\n",
      "from JC-202301-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202302-citibike-tripdata.csv\n",
      "from JC-202302-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202303-citibike-tripdata.csv\n",
      "from JC-202303-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202304-citibike-tripdata.csv\n",
      "from JC-202304-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202305-citibike-tripdata.csv\n",
      "from JC-202305-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202306-citibike-tripdata.csv\n",
      "from JC-202306-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202307-citibike-tripdata.csv\n",
      "from JC-202307-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202308-citibike-tripdata.csv\n",
      "from JC-202308-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202309-citibike-tripdata.csv\n",
      "from JC-202309-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202310-citibike-tripdata.csv\n",
      "from JC-202310-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202311-citibike-tripdata.csv\n",
      "from JC-202311-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202312-citibike-tripdata.csv\n",
      "from JC-202312-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202401-citibike-tripdata.csv\n",
      "from JC-202401-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202402-citibike-tripdata.csv\n",
      "from JC-202402-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202403-citibike-tripdata.csv\n",
      "from JC-202403-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202404-citibike-tripdata.csv\n",
      "from JC-202404-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202406-citibike-tripdata.csv\n",
      "from JC-202406-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202407-citibike-tripdata.csv\n",
      "from JC-202407-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202408-citibike-tripdata.csv\n",
      "from JC-202408-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n"
     ]
    }
   ],
   "source": [
    "# Unzip files & reorganize simultaneously\n",
    "zip_dir = current_dir + '/downloads'  # directory containing the zip files\n",
    "extract_dir = current_dir + '/data/bike-tripdata'  # directory where extracted files will be saved\n",
    "\n",
    "os.makedirs(extract_dir, exist_ok=True)  # create the directory if it doesn't exist\n",
    "\n",
    "for filename in os.listdir(zip_dir):  # loop through all files in the directory\n",
    "    if filename.endswith('.zip'):\n",
    "        zip_file_path = os.path.join(zip_dir, filename)\n",
    "\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:  # extract the zip file\n",
    "            for member in zip_ref.namelist():\n",
    "                # skip any file or folder inside \"_MACOSX\" (for MAC computers, not needed), and files that do not end with .csv\n",
    "                if '_MACOSX' not in member and member.endswith('.csv'):  \n",
    "                    # get only the base name of the file (ignore folder structure in zip)\n",
    "                    base_member = os.path.basename(member)\n",
    "                    target_path = os.path.join(extract_dir, base_member)\n",
    "                    \n",
    "                    with zip_ref.open(member) as source, open(target_path, 'wb') as target:\n",
    "                        target.write(source.read())  # write the extracted content to the single folder\n",
    "\n",
    "                    print(f'Extracted {base_member}')\n",
    "    print(f'... from {filename} to {extract_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7985a3c7",
   "metadata": {},
   "source": [
    "## Visualize dataset for cleaning\n",
    "### 1. Check which unique column names exist across all files\n",
    "### 2. Correct column names (strip uppercase and convert space to underscore)\n",
    "### 3. Map names to manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "354591e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ride_id', 'rideable_type', 'started_at', 'ended_at', 'start_station_name', 'start_station_id', 'end_station_name', 'end_station_id', 'start_lat', 'start_lng', 'end_lat', 'end_lng', 'member_casual']\n"
     ]
    }
   ],
   "source": [
    "# Since CSV files do not contain the same column headers, check which ones exist in the dataset?\n",
    "list_files = [f for f in os.listdir(extract_dir) if f.endswith('.csv')]\n",
    "unique_column_names = []\n",
    "for csv_file in list_files:\n",
    "    file_path = os.path.join(extract_dir, csv_file)\n",
    "    df = pd.read_csv(file_path, nrows=3)\n",
    "    [unique_column_names.append(col) for col in df.columns if col not in unique_column_names]\n",
    "print(unique_column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854012b1",
   "metadata": {},
   "source": [
    "It turns out that the column names are not consistent, e.g. some files contain the column \"starttime\" while others contain the column \"Start Time\". This should be corrected. Additionally, column names should not contain spaces (\"start station latitude\" vs \"start_lat\"). Last, there are 2 strange column names which need to be checked: \"Unnamed: 0\" and \"rideable_type_duplicate_column_name_1\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d22e286",
   "metadata": {},
   "source": [
    "### Correct column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e319ed65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique column names: \n",
      "  ['end_lat', 'end_lng', 'end_station_id', 'end_station_name', 'ended_at', 'member_casual', 'ride_id', 'rideable_type', 'start_lat', 'start_lng', 'start_station_id', 'start_station_name', 'started_at']\n",
      " \n",
      "Unique column names after cleaning: \n",
      " ['end_datetime', 'end_station_id', 'end_station_latitude', 'end_station_longitude', 'end_station_name', 'ride_id', 'rideable_type', 'start_datetime', 'start_station_id', 'start_station_latitude', 'start_station_longitude', 'start_station_name', 'user_type']\n"
     ]
    }
   ],
   "source": [
    "unique_column_names = list_unique_col_names(list_files)\n",
    "print('Unique column names: \\n  ' + str(unique_column_names))\n",
    "column_mapping = {\n",
    "    'bikeid': 'bike_id',\n",
    "    'end_lat': 'end_station_latitude',\n",
    "    'end_lng': 'end_station_longitude',\n",
    "    'ended_at': 'end_datetime',\n",
    "    'member_casual': 'user_type',\n",
    "    'rideable_type_duplicate_column_name_1': 'duplicate_col',\n",
    "    'start_lat': 'start_station_latitude',\n",
    "    'start_lng': 'start_station_longitude',\n",
    "    'starttime': 'start_datetime',\n",
    "    'start_time': 'start_datetime',\n",
    "    'started_at': 'start_datetime',\n",
    "    'stoptime': 'end_datetime',\n",
    "    'stop_time': 'end_datetime',\n",
    "    'tripduration': 'trip_duration',\n",
    "    'unnamed:_0': 'unnamed', # this is just an index column without name, present in some files -> can be discarded later\n",
    "    'usertype': 'user_type'\n",
    "}\n",
    "\n",
    "# Exploratory correction, see if it solves the inconsistencies\n",
    "unique_column_names=[]\n",
    "for csv_file in list_files:\n",
    "    file_path = os.path.join(extract_dir, csv_file)\n",
    "    df = pd.read_csv(file_path, nrows=1)\n",
    "#     if 'rideable_type_duplicate_column_name_1' in df.columns: # check what column this is -> just a duplicate -> can be discarded\n",
    "#         print(df.head(2))\n",
    "    df = clean_column_names(df, column_mapping)\n",
    "    ##print(df.columns)\n",
    "    \n",
    "    [unique_column_names.append(col) for col in df.columns if col not in unique_column_names] # save new col names for checking\n",
    "    unique_column_names.sort()\n",
    "\n",
    "remove_names = ['unnamed','duplicate_col'] # column names to remove\n",
    "final_column_names = [name for name in unique_column_names if name not in remove_names] # list with final universal column names\n",
    "print(' ')\n",
    "print('Unique column names after cleaning: \\n ' + str(final_column_names)) # -> satisfied!\n",
    "final_column_names.extend(['year','month']) # add the columns year and month, as I will add them from start_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70143a60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: end_datetime\n",
      "Unique dtypes: {dtype('O')}\n",
      "\n",
      "Column: end_station_id\n",
      "Unique dtypes: {dtype('float64')}\n",
      "\n",
      "Column: end_station_latitude\n",
      "Unique dtypes: {dtype('float64')}\n",
      "\n",
      "Column: end_station_longitude\n",
      "Unique dtypes: {dtype('float64')}\n",
      "\n",
      "Column: end_station_name\n",
      "Unique dtypes: {dtype('O')}\n",
      "\n",
      "Column: ride_id\n",
      "Unique dtypes: {dtype('O')}\n",
      "\n",
      "Column: rideable_type\n",
      "Unique dtypes: {dtype('O')}\n",
      "\n",
      "Column: start_datetime\n",
      "Unique dtypes: {dtype('O')}\n",
      "\n",
      "Column: start_station_id\n",
      "Unique dtypes: {dtype('float64')}\n",
      "\n",
      "Column: start_station_latitude\n",
      "Unique dtypes: {dtype('float64')}\n",
      "\n",
      "Column: start_station_longitude\n",
      "Unique dtypes: {dtype('float64')}\n",
      "\n",
      "Column: start_station_name\n",
      "Unique dtypes: {dtype('O')}\n",
      "\n",
      "Column: user_type\n",
      "Unique dtypes: {dtype('O')}\n",
      "\n",
      "Column: year\n",
      "Unique dtypes: set()\n",
      "\n",
      "Column: month\n",
      "Unique dtypes: set()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check if data type of files are the same\n",
    "check_dtype = {col: [] for col in final_column_names} # create empty dict to store dtypes\n",
    "\n",
    "list_files = [f for f in os.listdir(extract_dir) if f.endswith('.csv')]\n",
    "for csv_file in list_files:\n",
    "#df.memory_usage(deep=True).sum()\n",
    "    file_path = os.path.join(extract_dir, csv_file)\n",
    "    df = pd.read_csv(file_path, nrows=3)\n",
    "    df = clean_column_names(df, column_mapping) # clean column names\n",
    "    to_remove = ['duplicate_col','unnamed'] \n",
    "    for col in to_remove:\n",
    "        if col in df.columns:\n",
    "            df.drop(col, axis=1, inplace=True) # drop these columns\n",
    "    for col in df.columns:\n",
    "        #print(f'{col}: {df[col].dtype}')\n",
    "        check_dtype[col].append(df[col].dtype)\n",
    "\n",
    "for col in check_dtype:\n",
    "    print(f\"Column: {col}\")\n",
    "    print(f\"Unique dtypes: {set(check_dtype[col])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3e8c43f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ride_id</th>\n",
       "      <th>rideable_type</th>\n",
       "      <th>start_datetime</th>\n",
       "      <th>end_datetime</th>\n",
       "      <th>start_station_name</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>end_station_name</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>start_station_latitude</th>\n",
       "      <th>start_station_longitude</th>\n",
       "      <th>end_station_latitude</th>\n",
       "      <th>end_station_longitude</th>\n",
       "      <th>user_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5078F3D302000BD2</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2024-01-22 18:43:19.012</td>\n",
       "      <td>2024-01-22 18:48:10.708</td>\n",
       "      <td>Frederick Douglass Blvd &amp; W 145 St</td>\n",
       "      <td>7954.1200</td>\n",
       "      <td>St Nicholas Ave &amp; W 126 St</td>\n",
       "      <td>7756.1000</td>\n",
       "      <td>40.8231</td>\n",
       "      <td>-73.9417</td>\n",
       "      <td>40.8114</td>\n",
       "      <td>-73.9519</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>814337105D37302A</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2024-01-11 19:19:18.721</td>\n",
       "      <td>2024-01-11 19:47:36.007</td>\n",
       "      <td>W 54 St &amp; 6 Ave</td>\n",
       "      <td>6771.1300</td>\n",
       "      <td>E 74 St &amp; 1 Ave</td>\n",
       "      <td>6953.0800</td>\n",
       "      <td>40.7618</td>\n",
       "      <td>-73.9770</td>\n",
       "      <td>40.7690</td>\n",
       "      <td>-73.9548</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A33A920E2B10710C</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2024-01-30 19:17:41.693</td>\n",
       "      <td>2024-01-30 19:32:49.857</td>\n",
       "      <td>E 11 St &amp; Ave B</td>\n",
       "      <td>5659.1100</td>\n",
       "      <td>W 10 St &amp; Washington St</td>\n",
       "      <td>5847.0600</td>\n",
       "      <td>40.7276</td>\n",
       "      <td>-73.9798</td>\n",
       "      <td>40.7334</td>\n",
       "      <td>-74.0085</td>\n",
       "      <td>casual</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ride_id  rideable_type           start_datetime  \\\n",
       "0  5078F3D302000BD2  electric_bike  2024-01-22 18:43:19.012   \n",
       "1  814337105D37302A  electric_bike  2024-01-11 19:19:18.721   \n",
       "2  A33A920E2B10710C  electric_bike  2024-01-30 19:17:41.693   \n",
       "\n",
       "              end_datetime                  start_station_name  \\\n",
       "0  2024-01-22 18:48:10.708  Frederick Douglass Blvd & W 145 St   \n",
       "1  2024-01-11 19:47:36.007                     W 54 St & 6 Ave   \n",
       "2  2024-01-30 19:32:49.857                     E 11 St & Ave B   \n",
       "\n",
       "   start_station_id            end_station_name  end_station_id  \\\n",
       "0         7954.1200  St Nicholas Ave & W 126 St       7756.1000   \n",
       "1         6771.1300             E 74 St & 1 Ave       6953.0800   \n",
       "2         5659.1100     W 10 St & Washington St       5847.0600   \n",
       "\n",
       "   start_station_latitude  start_station_longitude  end_station_latitude  \\\n",
       "0                 40.8231                 -73.9417               40.8114   \n",
       "1                 40.7618                 -73.9770               40.7690   \n",
       "2                 40.7276                 -73.9798               40.7334   \n",
       "\n",
       "   end_station_longitude user_type  \n",
       "0               -73.9519    member  \n",
       "1               -73.9548    member  \n",
       "2               -74.0085    casual  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head() # view the last csv file, still loaded in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b5597c0-6c2a-4563-aa1f-7cd2864331fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hanna\\AppData\\Local\\Temp\\ipykernel_24200\\3511905405.py:12: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'gender': set(),\n",
       " 'user_type': {'casual', 'member'},\n",
       " 'rideable_type': {'classic_bike', 'electric_bike'}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get universal entries for columns I am converting to str and then to category (global categories across csv files)\n",
    "dtype_cat =['gender', 'user_type', 'rideable_type']\n",
    "\n",
    "# get global categories (differing per file)\n",
    "categories_dict = {col: set() for col in dtype_cat }\n",
    "\n",
    "# Collect all unique categories across the DataFrames\n",
    "for csv_file in list_files:  \n",
    "    file_path = os.path.join(extract_dir, csv_file) # load individual file\n",
    "    \n",
    "    # Process the file in chunks to save memory\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = clean_column_names(df, column_mapping) # clean column names\n",
    "    to_remove = ['duplicate_col','unnamed'] \n",
    "    for col in to_remove:\n",
    "        if col in df.columns:\n",
    "            df.drop(col, axis=1, inplace=True) # drop these columns\n",
    "        \n",
    "    for col in df.columns:\n",
    "        if col in categories_dict:\n",
    "            df[col].fillna('unknown').dropna()   \n",
    "            categories_dict[col].update(df[col].unique())  # update unique categories of df\n",
    "            #categories_dict[col].add('unknown') # add category 'unknown' for missing data\n",
    "categories_dict['user_type'].discard(np.nan) # for some reason still nan as category here\n",
    "categories_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dae536",
   "metadata": {},
   "source": [
    "## Load csv files, clean column names, change dtypes, concatenate into 1 dask df and save as dask parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47bfae2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This cell executed in the Anaconda powershell (clean_concat.py), since its faster/requires less memory on my 16GB mem laptop ###\n",
    "\n",
    "if not os.path.exists(cleaned_dir):  # directory to save the cleaned df\n",
    "    os.makedirs(cleaned_dir)\n",
    "\n",
    "dtype_dict = { # desired dtypes\n",
    "    'int32': ['birth_year', 'trip_duration'],\n",
    "    'float32': ['start_station_id', 'end_station_id'], \n",
    "    'float64': ['start_station_latitude', 'start_station_longitude', 'end_station_latitude', 'end_station_longitude'],\n",
    "    'str': ['start_station_name', 'end_station_name', 'ride_id'], \n",
    "    'category': ['gender', 'user_type', 'rideable_type'],\n",
    "    'datetime64[ns]': ['start_datetime', 'end_datetime']\n",
    "}\n",
    "\n",
    "dtype_mapping = {} # dictionary with col: dtype, for changing data types per column\n",
    "for dtype, columns in dtype_dict.items():\n",
    "    for col in columns:\n",
    "        dtype_mapping[col] = dtype\n",
    "\n",
    "list_files = [f for f in os.listdir(extract_dir) if f.endswith('.csv')] # list of csv files in dir to loop over\n",
    "chunksize = 1_000_000  # load in chunks to save memory, in case csv file is huge\n",
    "\n",
    "ddf_list = []\n",
    "for csv_file in list_files[0:2]: \n",
    "    file_path = os.path.join(extract_dir, csv_file) # load individual file\n",
    "    print('loading ' +  csv_file)\n",
    "    \n",
    "    # Process the file in chunks to save memory\n",
    "    chunk_iter = pd.read_csv(file_path, chunksize=chunksize, low_memory=True, parse_dates=True)\n",
    "\n",
    "    for n, chunk in enumerate(chunk_iter):\n",
    "        ddf = clean_column_names(chunk, column_mapping)  # Clean column names\n",
    " \n",
    "        # Convert to Dask DataFrame for larger datasets\n",
    "        #ddf = dd.from_pandas(chunk, npartitions=1) #\n",
    "        #print('to dask converted')\n",
    "        # Drop unwanted columns\n",
    "        to_remove = ['duplicate_col', 'unnamed']\n",
    "        ddf = ddf.drop(columns=[col for col in to_remove if col in ddf.columns])\n",
    "        \n",
    "        missing_cols = set(final_column_names) - set(ddf.columns) # add missing (universal) columns from final_column_names and fill with nans\n",
    "        for col in missing_cols:\n",
    "            ddf[col] = np.nan\n",
    "                \n",
    "        # Convert column dtypes\n",
    "        for col in ddf.columns:\n",
    "            if col in dtype_dict['int32']:\n",
    "                ddf[col] = ddf[col].replace('\\\\N', np.nan)  # handle missing values\n",
    "                ddf[col] = pd.to_numeric(ddf[col], errors='coerce').astype('float32')\n",
    "                ddf[col] = ddf[col].fillna(0).round(0).astype('int32') # replace nan with the place filler 0, round and convert to int\n",
    "            elif col in dtype_dict['category']: # for categorical data, replace nans with 'unknown' cat\n",
    "                ddf[col] = ddf[col].fillna('unknown')  # Replace NaNs with 'unknown'\n",
    "                ddf[col] = ddf[col].astype('str') # convert to str first\n",
    "                ddf[col] = ddf[col].astype('category') # string to category, as it needs less memory\n",
    "                ddf[col] = ddf[col].cat.set_categories(new_categories=list(categories_dict[col])) # set global categories\n",
    "            elif col in dtype_dict['str']: # - added 6.10\n",
    "                ddf[col] = ddf[col].fillna('unknown')  # Replace NaNs with 'unknown'\n",
    "                ddf[col] = ddf[col].astype(dtype_mapping[col]) # \n",
    "            elif col in dtype_mapping.keys():\n",
    "                if col in dtype_dict['float32'] or col in col in dtype_dict['float64']:\n",
    "                    ddf[col] = pd.to_numeric(ddf[col], errors='coerce')\n",
    "                ddf[col] = ddf[col].astype(dtype_mapping[col]) \n",
    "                \n",
    "        ddf = ddf.drop_duplicates().sort_values(by='start_datetime')# drop duplicates and sort to start rental time/date\n",
    "        ddf = ddf.reset_index(drop=True)\n",
    "        #ddf = ddf.set_index('start_datetime') # set start_datetime as index\n",
    "\n",
    "        ddf['year'] = ddf['start_datetime'].dt.year.astype('int32') # add year column for partitioning\n",
    "        ddf['month'] = ddf['start_datetime'].dt.month.astype('int8') # add month column for partitioning\n",
    "                                            \n",
    "        ddf = ddf[final_column_names] # ensure consistent column order   \n",
    "\n",
    "        ddf = dd.from_pandas(ddf, npartitions=5)\n",
    "        ddf_list.append(ddf)   \n",
    "    print('.... cleaned, converted to dask df and appended to ddf_list')     \n",
    "\n",
    "ddf_comb = dd.concat(ddf_list, ignore_index=True) # concatenate all dask dfs\n",
    "#del ddf_list\n",
    "ddf_comb = ddf_comb.drop_duplicates() # remove duplicates\n",
    "\n",
    "# check if all dtypes are consistent across ddfs\n",
    "check_dtype = {col: [] for col in final_column_names}\n",
    "for ddf in ddf_list:\n",
    "    for col in ddf.columns:\n",
    "        check_dtype[col].append(ddf[col].dtype)\n",
    "\n",
    "for col in check_dtype.keys():\n",
    "    print(col)\n",
    "    print(pd.Series(check_dtype[col]).unique())\n",
    "    if len(pd.Series(check_dtype[col]).unique()) > 1:\n",
    "        print(col + ' has inconsistent dtypes')\n",
    "    \n",
    "ddf_comb.to_parquet(cleaned_dir + '/combined_dask_df.parquet', engine='pyarrow', partition_on=['year', 'month'], write_index=False) # write, partitioned on year and month\n",
    "print('All files processed and saved to Parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e07be4-2642-41f6-bb3e-5d1d352fd6f9",
   "metadata": {},
   "source": [
    "## Load cleaned ddf (data bike rides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2afc839e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bike_id</th>\n",
       "      <th>birth_year</th>\n",
       "      <th>end_datetime</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>end_station_latitude</th>\n",
       "      <th>end_station_longitude</th>\n",
       "      <th>end_station_name</th>\n",
       "      <th>gender</th>\n",
       "      <th>ride_id</th>\n",
       "      <th>rideable_type</th>\n",
       "      <th>start_datetime</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>start_station_latitude</th>\n",
       "      <th>start_station_longitude</th>\n",
       "      <th>start_station_name</th>\n",
       "      <th>trip_duration</th>\n",
       "      <th>user_type</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14887.0000</td>\n",
       "      <td>1972</td>\n",
       "      <td>2013-06-01 10:50:32</td>\n",
       "      <td>531.0000</td>\n",
       "      <td>40.7189</td>\n",
       "      <td>-73.9927</td>\n",
       "      <td>Forsyth St &amp; Broome St</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-06-01 10:42:56</td>\n",
       "      <td>491.0000</td>\n",
       "      <td>40.7410</td>\n",
       "      <td>-73.9860</td>\n",
       "      <td>E 24 St &amp; Park Ave S</td>\n",
       "      <td>456</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>2013</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16981.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-06-01 12:12:29</td>\n",
       "      <td>336.0000</td>\n",
       "      <td>40.7305</td>\n",
       "      <td>-73.9991</td>\n",
       "      <td>Sullivan St &amp; Washington Sq</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-06-01 11:55:25</td>\n",
       "      <td>497.0000</td>\n",
       "      <td>40.7370</td>\n",
       "      <td>-73.9901</td>\n",
       "      <td>E 17 St &amp; Broadway</td>\n",
       "      <td>1024</td>\n",
       "      <td>Customer</td>\n",
       "      <td>2013</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14988.0000</td>\n",
       "      <td>1970</td>\n",
       "      <td>2013-06-01 12:59:14</td>\n",
       "      <td>471.0000</td>\n",
       "      <td>40.7129</td>\n",
       "      <td>-73.9570</td>\n",
       "      <td>Grand St &amp; Havemeyer St</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-06-01 12:17:56</td>\n",
       "      <td>244.0000</td>\n",
       "      <td>40.6920</td>\n",
       "      <td>-73.9654</td>\n",
       "      <td>Willoughby Ave &amp; Hall St</td>\n",
       "      <td>2478</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>2013</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16426.0000</td>\n",
       "      <td>1983</td>\n",
       "      <td>2013-06-01 13:30:06</td>\n",
       "      <td>259.0000</td>\n",
       "      <td>40.7012</td>\n",
       "      <td>-74.0123</td>\n",
       "      <td>South St &amp; Whitehall St</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-06-01 13:07:13</td>\n",
       "      <td>438.0000</td>\n",
       "      <td>40.7278</td>\n",
       "      <td>-73.9856</td>\n",
       "      <td>St Marks Pl &amp; 1 Ave</td>\n",
       "      <td>1373</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>2013</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19252.0000</td>\n",
       "      <td>1987</td>\n",
       "      <td>2013-06-01 13:36:17</td>\n",
       "      <td>432.0000</td>\n",
       "      <td>40.7262</td>\n",
       "      <td>-73.9838</td>\n",
       "      <td>E 7 St &amp; Avenue A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-06-01 13:14:23</td>\n",
       "      <td>520.0000</td>\n",
       "      <td>40.7599</td>\n",
       "      <td>-73.9765</td>\n",
       "      <td>W 52 St &amp; 5 Ave</td>\n",
       "      <td>1314</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>2013</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     bike_id  birth_year        end_datetime  end_station_id  \\\n",
       "0 14887.0000        1972 2013-06-01 10:50:32        531.0000   \n",
       "1 16981.0000           0 2013-06-01 12:12:29        336.0000   \n",
       "2 14988.0000        1970 2013-06-01 12:59:14        471.0000   \n",
       "3 16426.0000        1983 2013-06-01 13:30:06        259.0000   \n",
       "4 19252.0000        1987 2013-06-01 13:36:17        432.0000   \n",
       "\n",
       "   end_station_latitude  end_station_longitude             end_station_name  \\\n",
       "0               40.7189               -73.9927       Forsyth St & Broome St   \n",
       "1               40.7305               -73.9991  Sullivan St & Washington Sq   \n",
       "2               40.7129               -73.9570      Grand St & Havemeyer St   \n",
       "3               40.7012               -74.0123      South St & Whitehall St   \n",
       "4               40.7262               -73.9838            E 7 St & Avenue A   \n",
       "\n",
       "  gender ride_id rideable_type      start_datetime  start_station_id  \\\n",
       "0    NaN     nan           NaN 2013-06-01 10:42:56          491.0000   \n",
       "1    NaN     nan           NaN 2013-06-01 11:55:25          497.0000   \n",
       "2    NaN     nan           NaN 2013-06-01 12:17:56          244.0000   \n",
       "3    NaN     nan           NaN 2013-06-01 13:07:13          438.0000   \n",
       "4    NaN     nan           NaN 2013-06-01 13:14:23          520.0000   \n",
       "\n",
       "   start_station_latitude  start_station_longitude        start_station_name  \\\n",
       "0                 40.7410                 -73.9860      E 24 St & Park Ave S   \n",
       "1                 40.7370                 -73.9901        E 17 St & Broadway   \n",
       "2                 40.6920                 -73.9654  Willoughby Ave & Hall St   \n",
       "3                 40.7278                 -73.9856       St Marks Pl & 1 Ave   \n",
       "4                 40.7599                 -73.9765           W 52 St & 5 Ave   \n",
       "\n",
       "   trip_duration   user_type  year month  \n",
       "0            456  Subscriber  2013     6  \n",
       "1           1024    Customer  2013     6  \n",
       "2           2478  Subscriber  2013     6  \n",
       "3           1373  Subscriber  2013     6  \n",
       "4           1314  Subscriber  2013     6  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load cleaned data from saved file\n",
    "ddf = dd.read_parquet(cleaned_dir + '/combined_dask_df0810.parquet')\n",
    "#ddf = dd.read_parquet('path_to_parquet_file', columns=['category_column', 'numeric_column']) # read only specific columns\n",
    "#ddf_2020 = dd.read_parquet('path_to_parquet_file/year=2020') # read only specific partition\n",
    "\n",
    "ddf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6facd081-507a-4098-b5ea-aff97cec60d0",
   "metadata": {},
   "source": [
    "Now that all column names are consistent and all data is concatenated, check if categories in some columns are consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea2f9da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique categories:\n",
      "user_type - Index(['member', 'Customer', 'Subscriber', 'casual'], dtype='object')\n",
      "rideable_type - Index(['electric_bike', 'classic_bike', 'docked_bike'], dtype='object')\n",
      "Unique string:\n",
      "start_station id, unique: 3318\n",
      "start_station name, unique: 2581\n",
      "end_station_id, unique: 3359\n",
      "end_station name, unique: 2610\n"
     ]
    }
   ],
   "source": [
    "print('Unique categories:')\n",
    "print(f'user_type - {ddf[\"user_type\"].cat.as_known().cat.categories}')\n",
    "print(f'rideable_type - {ddf[\"rideable_type\"].cat.as_known().cat.categories}')\n",
    "\n",
    "print('Unique string:')\n",
    "print('start_station id, unique: '+str(ddf.start_station_id.nunique().compute()))\n",
    "print('start_station name, unique: '+str(ddf.start_station_name.nunique().compute()))\n",
    "print('end_station_id, unique: '+str(ddf.end_station_id.nunique().compute()))\n",
    "print('end_station name, unique: '+str(ddf.end_station_name.nunique().compute()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287d6885-941c-46aa-832b-0afc6e16cab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bike_id                            float64\n",
      "birth_year                           int32\n",
      "end_datetime                datetime64[ns]\n",
      "end_station_id                     float32\n",
      "end_station_latitude               float64\n",
      "end_station_longitude              float64\n",
      "end_station_name           string[pyarrow]\n",
      "gender                            category\n",
      "ride_id                    string[pyarrow]\n",
      "rideable_type                     category\n",
      "start_datetime              datetime64[ns]\n",
      "start_station_id                   float32\n",
      "start_station_latitude             float64\n",
      "start_station_longitude            float64\n",
      "start_station_name         string[pyarrow]\n",
      "trip_duration                        int32\n",
      "user_type                         category\n",
      "year                              category\n",
      "month                             category\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(ddf.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a83501c2-a573-4a9a-82ee-91557b7b83bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory usage of ddf: 17.28 GB\n"
     ]
    }
   ],
   "source": [
    "# Check memory usage of ddf\n",
    "memory_usage = ddf.memory_usage(deep=True).compute()\n",
    "total_memory_usage = memory_usage.sum() # total memory usage in bytes\n",
    "print(f\"Total memory usage of ddf: {total_memory_usage / (1024**3):.2f} GB\") # convert to GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ccbd18",
   "metadata": {},
   "source": [
    "Okay, this shows that the ddf still requires a lot of memory, therefore I will see if I can further reduce this (by changing to the most efficient dtype, and rounding floats)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eedd504c-5fe0-4f72-a3f7-6292b905da37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bike_id (min, max): 14529.0, 49985.0\n",
      "start_station_id (min, max): 72.0, 8897.0498046875\n",
      "end_station_id (min, max): 72.0, 8897.0498046875\n",
      "trip_duration (min, max): 0, 20260212\n"
     ]
    }
   ],
   "source": [
    "# Additional adjustments of dtype - check min and max, to decide which dtype can represent all values\n",
    "\n",
    "print(f\"bike_id (min, max): {ddf['bike_id'].dropna().min().compute()}, {ddf['bike_id'].dropna().max().compute()}\")\n",
    "print(f\"start_station_id (min, max): {ddf['start_station_id'].dropna().min().compute()}, {ddf['start_station_id'].dropna().max().compute()}\")\n",
    "print(f\"end_station_id (min, max): {ddf['end_station_id'].dropna().min().compute()}, {ddf['end_station_id'].dropna().max().compute()}\")\n",
    "print(f\"trip_duration (min, max): {ddf['trip_duration'].dropna().min().compute()}, {ddf['trip_duration'].dropna().max().compute()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bea7c17e-bf93-4571-b38a-56a222064104",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   3206.0000\n",
       "0   8451.0703\n",
       "0   3383.0000\n",
       "0   6889.1201\n",
       "0   4122.0298\n",
       "       ...   \n",
       "0   6190.0298\n",
       "0   8485.0098\n",
       "0   3900.0000\n",
       "1   7820.0498\n",
       "0   8472.0596\n",
       "Name: start_station_id, Length: 3319, dtype: float32"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf['start_station_id'].unique().compute() # -> decimals, I didnt expect that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bb93f87-2fdc-4a0f-8cd5-d4ded9d2661b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bike_id</th>\n",
       "      <th>birth_year</th>\n",
       "      <th>end_datetime</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>end_station_latitude</th>\n",
       "      <th>end_station_longitude</th>\n",
       "      <th>end_station_name</th>\n",
       "      <th>gender</th>\n",
       "      <th>ride_id</th>\n",
       "      <th>rideable_type</th>\n",
       "      <th>start_datetime</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>start_station_latitude</th>\n",
       "      <th>start_station_longitude</th>\n",
       "      <th>start_station_name</th>\n",
       "      <th>trip_duration</th>\n",
       "      <th>user_type</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [bike_id, birth_year, end_datetime, end_station_id, end_station_latitude, end_station_longitude, end_station_name, gender, ride_id, rideable_type, start_datetime, start_station_id, start_station_latitude, start_station_longitude, start_station_name, trip_duration, user_type, year, month]\n",
       "Index: []"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manual check by opening the first csv file, should be: start_station_id=3255, end_station_id=537\n",
    "ddf.loc[ddf['start_datetime'] == pd.to_datetime('2018-04-27 07:26:52')].compute() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b332602-d447-4cc6-af30-d9eca417448e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bike_id</th>\n",
       "      <th>birth_year</th>\n",
       "      <th>end_datetime</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>end_station_latitude</th>\n",
       "      <th>end_station_longitude</th>\n",
       "      <th>end_station_name</th>\n",
       "      <th>gender</th>\n",
       "      <th>ride_id</th>\n",
       "      <th>rideable_type</th>\n",
       "      <th>start_datetime</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>start_station_latitude</th>\n",
       "      <th>start_station_longitude</th>\n",
       "      <th>start_station_name</th>\n",
       "      <th>trip_duration</th>\n",
       "      <th>user_type</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19678</td>\n",
       "      <td>1983</td>\n",
       "      <td>2013-06-01 00:11:36</td>\n",
       "      <td>434</td>\n",
       "      <td>40.743174</td>\n",
       "      <td>-74.003664</td>\n",
       "      <td>9 Ave &amp; W 18 St</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-06-01 00:00:01</td>\n",
       "      <td>444</td>\n",
       "      <td>40.742354</td>\n",
       "      <td>-73.989151</td>\n",
       "      <td>Broadway &amp; W 24 St</td>\n",
       "      <td>695</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>2013</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16649</td>\n",
       "      <td>1984</td>\n",
       "      <td>2013-06-01 00:11:41</td>\n",
       "      <td>434</td>\n",
       "      <td>40.743174</td>\n",
       "      <td>-74.003664</td>\n",
       "      <td>9 Ave &amp; W 18 St</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-06-01 00:00:08</td>\n",
       "      <td>444</td>\n",
       "      <td>40.742354</td>\n",
       "      <td>-73.989151</td>\n",
       "      <td>Broadway &amp; W 24 St</td>\n",
       "      <td>693</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>2013</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bike_id  birth_year        end_datetime  end_station_id  \\\n",
       "0    19678        1983 2013-06-01 00:11:36             434   \n",
       "0    16649        1984 2013-06-01 00:11:41             434   \n",
       "\n",
       "   end_station_latitude  end_station_longitude end_station_name gender  \\\n",
       "0             40.743174             -74.003664  9 Ave & W 18 St    NaN   \n",
       "0             40.743174             -74.003664  9 Ave & W 18 St    NaN   \n",
       "\n",
       "  ride_id rideable_type      start_datetime  start_station_id  \\\n",
       "0     nan           NaN 2013-06-01 00:00:01               444   \n",
       "0     nan           NaN 2013-06-01 00:00:08               444   \n",
       "\n",
       "   start_station_latitude  start_station_longitude  start_station_name  \\\n",
       "0               40.742354               -73.989151  Broadway & W 24 St   \n",
       "0               40.742354               -73.989151  Broadway & W 24 St   \n",
       "\n",
       "   trip_duration   user_type  year month  \n",
       "0            695  Subscriber  2013     6  \n",
       "0            693  Subscriber  2013     6  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Additional adjustments of dtype, based on min and max value in the dataset\n",
    "\n",
    "# bike_id -> overlooked. Ideally, do it in the previous cleaning step, but since that takes a long time to run and I dont have much time left, Im doing it here for now\n",
    "ddf['bike_id'] = pd.to_numeric(ddf['bike_id'], errors='coerce')\n",
    "ddf['bike_id'] = ddf['bike_id'].round(0).astype('Int64')  \n",
    "\n",
    "# start_station_id and end_station_id -> Int64 which can handle NaNs (I didnt know earlier), and decimals are not needed\n",
    "ddf['start_station_id'] = ddf['start_station_id'].round(0).astype('Int64')\n",
    "ddf['end_station_id'] = ddf['end_station_id'].round(0).astype('Int64')\n",
    "\n",
    "# birth_year can be int16 (range: -32768 to 32767) as NaNs are set to 0 already\n",
    "ddf['birth_year'] = ddf['birth_year'].astype('int16')\n",
    "ddf['trip_duration'] = ddf['trip_duration'].astype('int32') #\n",
    "\n",
    "ddf = ddf.sort_values(by='start_datetime')# sort again to start rental time/date, since ddf does not seem sorted (time starts at 10am instead of midnight)\n",
    "\n",
    "#ddf.compute()\n",
    "ddf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e8bd4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique categories, after changing \"subscriber\" to \"member\" and \"customer\" to \"casual\", for consistency:\n",
      "user_type - Index(['member', 'casual'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bike_id</th>\n",
       "      <th>birth_year</th>\n",
       "      <th>end_datetime</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>end_station_latitude</th>\n",
       "      <th>end_station_longitude</th>\n",
       "      <th>end_station_name</th>\n",
       "      <th>gender</th>\n",
       "      <th>ride_id</th>\n",
       "      <th>rideable_type</th>\n",
       "      <th>start_datetime</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>start_station_latitude</th>\n",
       "      <th>start_station_longitude</th>\n",
       "      <th>start_station_name</th>\n",
       "      <th>trip_duration</th>\n",
       "      <th>user_type</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19678</td>\n",
       "      <td>1983</td>\n",
       "      <td>2013-06-01 00:11:36</td>\n",
       "      <td>434</td>\n",
       "      <td>40.743174</td>\n",
       "      <td>-74.003664</td>\n",
       "      <td>9 Ave &amp; W 18 St</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-06-01 00:00:01</td>\n",
       "      <td>444</td>\n",
       "      <td>40.742354</td>\n",
       "      <td>-73.989151</td>\n",
       "      <td>Broadway &amp; W 24 St</td>\n",
       "      <td>695</td>\n",
       "      <td>member</td>\n",
       "      <td>2013</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16649</td>\n",
       "      <td>1984</td>\n",
       "      <td>2013-06-01 00:11:41</td>\n",
       "      <td>434</td>\n",
       "      <td>40.743174</td>\n",
       "      <td>-74.003664</td>\n",
       "      <td>9 Ave &amp; W 18 St</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-06-01 00:00:08</td>\n",
       "      <td>444</td>\n",
       "      <td>40.742354</td>\n",
       "      <td>-73.989151</td>\n",
       "      <td>Broadway &amp; W 24 St</td>\n",
       "      <td>693</td>\n",
       "      <td>member</td>\n",
       "      <td>2013</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bike_id  birth_year        end_datetime  end_station_id  \\\n",
       "0    19678        1983 2013-06-01 00:11:36             434   \n",
       "0    16649        1984 2013-06-01 00:11:41             434   \n",
       "\n",
       "   end_station_latitude  end_station_longitude end_station_name gender  \\\n",
       "0             40.743174             -74.003664  9 Ave & W 18 St    NaN   \n",
       "0             40.743174             -74.003664  9 Ave & W 18 St    NaN   \n",
       "\n",
       "  ride_id rideable_type      start_datetime  start_station_id  \\\n",
       "0     nan           NaN 2013-06-01 00:00:01               444   \n",
       "0     nan           NaN 2013-06-01 00:00:08               444   \n",
       "\n",
       "   start_station_latitude  start_station_longitude  start_station_name  \\\n",
       "0               40.742354               -73.989151  Broadway & W 24 St   \n",
       "0               40.742354               -73.989151  Broadway & W 24 St   \n",
       "\n",
       "   trip_duration user_type  year month  \n",
       "0            695    member  2013     6  \n",
       "0            693    member  2013     6  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unify string in rideable_type and user_type columns\n",
    "\n",
    "#ddf['rideable_type'] = ddf['rideable_type'].astype('str').str.strip().str.lower().astype('category')# if needed\n",
    "\n",
    "def replace_user_type(df):\n",
    "    df = df.copy()  # make a copy to avoid SettingWithCopyWarning\n",
    "    df['user_type'] = df['user_type'].astype(str).str.strip().str.lower() # temporarily convert to string (object)\n",
    "    \n",
    "    # Replace 'subscriber' with 'member' and 'customer' with 'casual'\n",
    "    df.loc[df['user_type'] == 'subscriber', 'user_type'] = 'member'\n",
    "    df.loc[df['user_type'] == 'customer', 'user_type'] = 'casual'\n",
    "\n",
    "    df['user_type'] = pd.Categorical(df['user_type'], categories=['member', 'casual']) # convert back to category\n",
    "\n",
    "    return df\n",
    "\n",
    "# Use map_partitions to apply this function to the Dask DataFrame\n",
    "ddf = ddf.map_partitions(replace_user_type)\n",
    "\n",
    "print('Unique categories, after changing \"subscriber\" to \"member\" and \"customer\" to \"casual\", for consistency:')\n",
    "print(f'user_type - {ddf[\"user_type\"].cat.as_known().cat.categories}')\n",
    "ddf.head(2) # check if it worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b82eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.isna().sum().compute() # check how many nans in which columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce0e356-438f-42f8-ad15-eb6eac93ec7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf[ddf['end_station_latitude'].isna()].compute().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62184dca-ebd7-4f90-b244-4b960aeb574a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check memory usage of ddf after additional data cleaning\n",
    "memory_usage = ddf.memory_usage(deep=True).compute()\n",
    "total_memory_usage = memory_usage.sum() # total memory usage in bytes\n",
    "print(f\"Total memory usage of ddf after additional data cleaning: {total_memory_usage / (1024**3):.2f} GB\") # convert to GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40691221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some idea of the df content\n",
    "\n",
    "print('Mean trip duration: ')\n",
    "print(ddf.groupby('user_type').agg({'trip_duration': 'mean'}).compute())\n",
    "ddf.groupby('gender').agg({'trip_duration': 'mean'}).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0930bbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save further cleaned ddf again, overwriting\n",
    "ddf.to_parquet(cleaned_dir + '/combined_dask_df_cleaned_1010.parquet', engine='pyarrow', partition_on=['year', 'month'], write_index=False) # write, partitioned on year and month"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41850b0f",
   "metadata": {},
   "source": [
    "## Collision data\n",
    "### 1. Inspection\n",
    "### 2.Cleaning: column names, entries, data type, missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a44cc3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hanna\\AppData\\Local\\Temp\\ipykernel_3788\\1023804704.py:2: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2120518, 29)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRASH DATE</th>\n",
       "      <th>CRASH TIME</th>\n",
       "      <th>BOROUGH</th>\n",
       "      <th>ZIP CODE</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>LOCATION</th>\n",
       "      <th>ON STREET NAME</th>\n",
       "      <th>CROSS STREET NAME</th>\n",
       "      <th>OFF STREET NAME</th>\n",
       "      <th>...</th>\n",
       "      <th>CONTRIBUTING FACTOR VEHICLE 2</th>\n",
       "      <th>CONTRIBUTING FACTOR VEHICLE 3</th>\n",
       "      <th>CONTRIBUTING FACTOR VEHICLE 4</th>\n",
       "      <th>CONTRIBUTING FACTOR VEHICLE 5</th>\n",
       "      <th>COLLISION_ID</th>\n",
       "      <th>VEHICLE TYPE CODE 1</th>\n",
       "      <th>VEHICLE TYPE CODE 2</th>\n",
       "      <th>VEHICLE TYPE CODE 3</th>\n",
       "      <th>VEHICLE TYPE CODE 4</th>\n",
       "      <th>VEHICLE TYPE CODE 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>09/11/2021</td>\n",
       "      <td>2:39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WHITESTONE EXPRESSWAY</td>\n",
       "      <td>20 AVENUE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4455765</td>\n",
       "      <td>Sedan</td>\n",
       "      <td>Sedan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>03/26/2022</td>\n",
       "      <td>11:45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>QUEENSBORO BRIDGE UPPER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4513547</td>\n",
       "      <td>Sedan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>06/29/2022</td>\n",
       "      <td>6:55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>THROGS NECK BRIDGE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4541903</td>\n",
       "      <td>Sedan</td>\n",
       "      <td>Pick-up Truck</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>09/11/2021</td>\n",
       "      <td>9:35</td>\n",
       "      <td>BROOKLYN</td>\n",
       "      <td>11208.0000</td>\n",
       "      <td>40.6672</td>\n",
       "      <td>-73.8665</td>\n",
       "      <td>(40.667202, -73.8665)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1211      LORING AVENUE</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4456314</td>\n",
       "      <td>Sedan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12/14/2021</td>\n",
       "      <td>8:13</td>\n",
       "      <td>BROOKLYN</td>\n",
       "      <td>11233.0000</td>\n",
       "      <td>40.6833</td>\n",
       "      <td>-73.9173</td>\n",
       "      <td>(40.683304, -73.917274)</td>\n",
       "      <td>SARATOGA AVENUE</td>\n",
       "      <td>DECATUR STREET</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4486609</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   CRASH DATE CRASH TIME   BOROUGH   ZIP CODE  LATITUDE  LONGITUDE  \\\n",
       "0  09/11/2021       2:39       NaN        NaN       NaN        NaN   \n",
       "1  03/26/2022      11:45       NaN        NaN       NaN        NaN   \n",
       "2  06/29/2022       6:55       NaN        NaN       NaN        NaN   \n",
       "3  09/11/2021       9:35  BROOKLYN 11208.0000   40.6672   -73.8665   \n",
       "4  12/14/2021       8:13  BROOKLYN 11233.0000   40.6833   -73.9173   \n",
       "\n",
       "                  LOCATION           ON STREET NAME CROSS STREET NAME  \\\n",
       "0                      NaN    WHITESTONE EXPRESSWAY         20 AVENUE   \n",
       "1                      NaN  QUEENSBORO BRIDGE UPPER               NaN   \n",
       "2                      NaN       THROGS NECK BRIDGE               NaN   \n",
       "3    (40.667202, -73.8665)                      NaN               NaN   \n",
       "4  (40.683304, -73.917274)          SARATOGA AVENUE    DECATUR STREET   \n",
       "\n",
       "           OFF STREET NAME  ...  CONTRIBUTING FACTOR VEHICLE 2  \\\n",
       "0                      NaN  ...                    Unspecified   \n",
       "1                      NaN  ...                            NaN   \n",
       "2                      NaN  ...                    Unspecified   \n",
       "3  1211      LORING AVENUE  ...                            NaN   \n",
       "4                      NaN  ...                            NaN   \n",
       "\n",
       "   CONTRIBUTING FACTOR VEHICLE 3  CONTRIBUTING FACTOR VEHICLE 4  \\\n",
       "0                            NaN                            NaN   \n",
       "1                            NaN                            NaN   \n",
       "2                            NaN                            NaN   \n",
       "3                            NaN                            NaN   \n",
       "4                            NaN                            NaN   \n",
       "\n",
       "   CONTRIBUTING FACTOR VEHICLE 5  COLLISION_ID  VEHICLE TYPE CODE 1  \\\n",
       "0                            NaN       4455765                Sedan   \n",
       "1                            NaN       4513547                Sedan   \n",
       "2                            NaN       4541903                Sedan   \n",
       "3                            NaN       4456314                Sedan   \n",
       "4                            NaN       4486609                  NaN   \n",
       "\n",
       "   VEHICLE TYPE CODE 2  VEHICLE TYPE CODE 3 VEHICLE TYPE CODE 4  \\\n",
       "0                Sedan                  NaN                 NaN   \n",
       "1                  NaN                  NaN                 NaN   \n",
       "2        Pick-up Truck                  NaN                 NaN   \n",
       "3                  NaN                  NaN                 NaN   \n",
       "4                  NaN                  NaN                 NaN   \n",
       "\n",
       "  VEHICLE TYPE CODE 5  \n",
       "0                 NaN  \n",
       "1                 NaN  \n",
       "2                 NaN  \n",
       "3                 NaN  \n",
       "4                 NaN  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = current_dir + '/data/Motor_Vehicle_Collisions_-_Crashes_20240922.csv' # load file\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "print(df.shape)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "157e976e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['crash_date', 'crash_time', 'borough', 'zip_code', 'latitude',\n",
      "       'longitude', 'location', 'on_street_name', 'cross_street_name',\n",
      "       'off_street_name', 'number_of_persons_injured',\n",
      "       'number_of_persons_killed', 'number_of_pedestrians_injured',\n",
      "       'number_of_pedestrians_killed', 'number_of_cyclist_injured',\n",
      "       'number_of_cyclist_killed', 'number_of_motorist_injured',\n",
      "       'number_of_motorist_killed', 'contributing_factor_vehicle_1',\n",
      "       'contributing_factor_vehicle_2', 'contributing_factor_vehicle_3',\n",
      "       'contributing_factor_vehicle_4', 'contributing_factor_vehicle_5',\n",
      "       'collision_id', 'vehicle_type_code_1', 'vehicle_type_code_2',\n",
      "       'vehicle_type_code_3', 'vehicle_type_code_4', 'vehicle_type_code_5'],\n",
      "      dtype='object')\n",
      "crash_date                        object\n",
      "crash_time                        object\n",
      "borough                           object\n",
      "zip_code                          object\n",
      "latitude                         float64\n",
      "longitude                        float64\n",
      "location                          object\n",
      "on_street_name                    object\n",
      "cross_street_name                 object\n",
      "off_street_name                   object\n",
      "number_of_persons_injured        float64\n",
      "number_of_persons_killed         float64\n",
      "number_of_pedestrians_injured      int64\n",
      "number_of_pedestrians_killed       int64\n",
      "number_of_cyclist_injured          int64\n",
      "number_of_cyclist_killed           int64\n",
      "number_of_motorist_injured         int64\n",
      "number_of_motorist_killed          int64\n",
      "contributing_factor_vehicle_1     object\n",
      "contributing_factor_vehicle_2     object\n",
      "contributing_factor_vehicle_3     object\n",
      "contributing_factor_vehicle_4     object\n",
      "contributing_factor_vehicle_5     object\n",
      "collision_id                       int64\n",
      "vehicle_type_code_1               object\n",
      "vehicle_type_code_2               object\n",
      "vehicle_type_code_3               object\n",
      "vehicle_type_code_4               object\n",
      "vehicle_type_code_5               object\n",
      "dtype: object\n",
      "                               Nan_count    Total\n",
      "crash_date                             0  2120518\n",
      "crash_time                             0  2120518\n",
      "borough                           659498  2120518\n",
      "zip_code                          659758  2120518\n",
      "latitude                          247820  2120518\n",
      "longitude                         247820  2120518\n",
      "location                          247820  2120518\n",
      "on_street_name                    453598  2120518\n",
      "cross_street_name                 807416  2120518\n",
      "off_street_name                  1759293  2120518\n",
      "number_of_persons_injured             18  2120518\n",
      "number_of_persons_killed              31  2120518\n",
      "number_of_pedestrians_injured          0  2120518\n",
      "number_of_pedestrians_killed           0  2120518\n",
      "number_of_cyclist_injured              0  2120518\n",
      "number_of_cyclist_killed               0  2120518\n",
      "number_of_motorist_injured             0  2120518\n",
      "number_of_motorist_killed              0  2120518\n",
      "contributing_factor_vehicle_1       7107  2120518\n",
      "contributing_factor_vehicle_2     331898  2120518\n",
      "contributing_factor_vehicle_3    1968151  2120518\n",
      "contributing_factor_vehicle_4    2085953  2120518\n",
      "contributing_factor_vehicle_5    2111123  2120518\n",
      "collision_id                           0  2120518\n",
      "vehicle_type_code_1                14384  2120518\n",
      "vehicle_type_code_2               411360  2120518\n",
      "vehicle_type_code_3              1973843  2120518\n",
      "vehicle_type_code_4              2087182  2120518\n",
      "vehicle_type_code_5              2111410  2120518\n"
     ]
    }
   ],
   "source": [
    "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_') # clean column names\n",
    "print(df.columns)\n",
    "print(df.dtypes)\n",
    "\n",
    "# How many missing values (nans) and where\n",
    "summary_table = pd.DataFrame({\n",
    "    'Nan_count': df.isna().sum(),\n",
    "    'Total': df.shape[0]\n",
    "})\n",
    "print(summary_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fefd3a97-b41d-4074-b286-7ba293ffc6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        number_of_persons_injured  number_of_pedestrians_injured  \\\n",
      "182614                        NaN                              0   \n",
      "569936                        NaN                              0   \n",
      "619341                        NaN                              0   \n",
      "669416                        NaN                              0   \n",
      "712527                        NaN                              0   \n",
      "\n",
      "        number_of_cyclist_injured  number_of_motorist_injured  \n",
      "182614                          1                           0  \n",
      "569936                          0                           1  \n",
      "619341                          0                           1  \n",
      "669416                          0                           0  \n",
      "712527                          0                           1  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crash_date</th>\n",
       "      <th>crash_time</th>\n",
       "      <th>borough</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>location</th>\n",
       "      <th>on_street_name</th>\n",
       "      <th>cross_street_name</th>\n",
       "      <th>off_street_name</th>\n",
       "      <th>...</th>\n",
       "      <th>contributing_factor_vehicle_2</th>\n",
       "      <th>contributing_factor_vehicle_3</th>\n",
       "      <th>contributing_factor_vehicle_4</th>\n",
       "      <th>contributing_factor_vehicle_5</th>\n",
       "      <th>collision_id</th>\n",
       "      <th>vehicle_type_code_1</th>\n",
       "      <th>vehicle_type_code_2</th>\n",
       "      <th>vehicle_type_code_3</th>\n",
       "      <th>vehicle_type_code_4</th>\n",
       "      <th>vehicle_type_code_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows  29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [crash_date, crash_time, borough, zip_code, latitude, longitude, location, on_street_name, cross_street_name, off_street_name, number_of_persons_injured, number_of_persons_killed, number_of_pedestrians_injured, number_of_pedestrians_killed, number_of_cyclist_injured, number_of_cyclist_killed, number_of_motorist_injured, number_of_motorist_killed, contributing_factor_vehicle_1, contributing_factor_vehicle_2, contributing_factor_vehicle_3, contributing_factor_vehicle_4, contributing_factor_vehicle_5, collision_id, vehicle_type_code_1, vehicle_type_code_2, vehicle_type_code_3, vehicle_type_code_4, vehicle_type_code_5]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 29 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correct the columns number_of_persons_injured and number_of_persons_killed, that contain nan\n",
    "print(df.loc[df['number_of_persons_injured'].isna(),['number_of_persons_injured','number_of_pedestrians_injured','number_of_cyclist_injured',\n",
    "                                                'number_of_motorist_injured']].head()) # show \n",
    "\n",
    "df.loc[df['number_of_persons_injured'].isna(), 'number_of_persons_injured'] = df['number_of_pedestrians_injured'] + df['number_of_cyclist_injured'] + df['number_of_motorist_injured']\n",
    "df.loc[df['number_of_persons_killed'].isna(), 'number_of_persons_killed'] = df['number_of_pedestrians_killed'] + df['number_of_cyclist_killed'] + df['number_of_motorist_killed']\n",
    "                                                                               \n",
    "df[df['number_of_persons_injured'].isna()] # show again after correction                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99a6dc1f-8efc-4f14-b2a8-b1268e29f874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change column names 'longitude' and 'latitude' for later combining with the bike ride dataset, and number_of_/cyclist/motorist to avoid confusion\n",
    "df = df.rename(columns={'latitude': 'accident_latitude', 'longitude': 'accident_longitude', 'number_of_cyclist_injured':'number_of_cyclists_injured',\n",
    "                        'number_of_cyclist_killed':'number_of_cyclists_killed','number_of_motorist_injured':'number_of_motorists_injured',\n",
    "                       'number_of_motorist_killed':'number_of_motorists_killed'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff2d9d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan 11208.0 11233.0 10475.0 11207.0 10017.0 11413.0 11434.0 11217.0\n",
      " 11226.0]\n",
      "[ 2.  1.  0.  4.  3.  5.  7.  6.  9. 18.  8. 11. 17. 10. 14. 15. 12. 13.\n",
      " 40. 16. 20. 22. 31. 19. 27. 32. 24. 43. 21. 23. 34. 25.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2., 3., 4., 8., 5.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check format of entries in columns\n",
    "print(df['zip_code'].unique()[0:10])\n",
    "print(df['number_of_persons_injured'].unique())\n",
    "df['number_of_persons_killed'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90a86dec-ac7b-483e-8a54-9fb34db6b496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 duplicates removed\n",
      " Unique values in zip_code: \n",
      "[   nan 10036. 11223. 11215. 10451. 11234. 11375. 11233. 10007. 10017.\n",
      " 11220. 10022. 11416. 11434. 10456. 11217. 11226. 10014. 10470. 11208.\n",
      " 11102. 10018. 11412. 11372. 10016. 10002. 11203. 10304. 10464. 10065.\n",
      " 11691. 11214. 11207. 10303. 11209. 10012. 10305. 11206. 11101. 11379.\n",
      " 11238. 10027. 11428. 11377. 10312. 10011. 10001. 10009. 11222. 11105.\n",
      " 11201. 11385. 11103. 11368. 10466. 10019. 11230. 10033. 10468. 11225.\n",
      " 10314. 10128. 11218. 11213. 10467. 11004. 11373. 11216. 11435. 11429.\n",
      " 11421. 10310. 11411. 10475. 11219. 11370. 10462. 11354. 11369. 11001.\n",
      " 10452. 11358. 11697. 11365. 10029. 11414. 10013. 11355. 11229. 11374.\n",
      " 10032. 10003. 11357. 11235. 10309. 11694. 11211. 10004. 11224. 10461.\n",
      " 10301. 10469. 11237. 10031. 10472. 11417. 10023. 10024. 11433. 10306.\n",
      " 11232. 11236. 11364. 11104. 10035. 11419. 11436. 10021. 11422. 11106.\n",
      " 11367. 11361. 10459. 11231. 11432. 11204. 11413. 10455. 10037. 11420.\n",
      " 10026. 10454. 11362. 11228. 11221. 10005. 11249. 11212. 10020. 10010.\n",
      " 10473. 10465. 11210. 10457. 10453. 11205. 10028. 10463. 11356. 11423.\n",
      " 10039. 10474. 10075. 11427. 11378. 10025. 11692. 11418. 10302. 11366.\n",
      " 10038. 10458. 10471. 11415. 10030. 10006. 10308. 11426. 10034. 10460.\n",
      " 11239. 10307. 11360. 11363. 11693. 10040. 10119. 11040. 11109. 10000.\n",
      " 10069. 10280. 11430. 11005. 10281. 11695. 10282. 11359. 10111. 10803.\n",
      " 10153. 10048. 10168. 10162.]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>borough</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>accident_latitude</th>\n",
       "      <th>accident_longitude</th>\n",
       "      <th>location</th>\n",
       "      <th>on_street_name</th>\n",
       "      <th>cross_street_name</th>\n",
       "      <th>off_street_name</th>\n",
       "      <th>number_of_persons_injured</th>\n",
       "      <th>number_of_persons_killed</th>\n",
       "      <th>...</th>\n",
       "      <th>contributing_factor_vehicle_5</th>\n",
       "      <th>collision_id</th>\n",
       "      <th>vehicle_type_code_1</th>\n",
       "      <th>vehicle_type_code_2</th>\n",
       "      <th>vehicle_type_code_3</th>\n",
       "      <th>vehicle_type_code_4</th>\n",
       "      <th>vehicle_type_code_5</th>\n",
       "      <th>crash_datetime</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.6978</td>\n",
       "      <td>-73.8139</td>\n",
       "      <td>(40.6977532, -73.8139159)</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>2999940.0000</td>\n",
       "      <td>passenger vehicle</td>\n",
       "      <td>passenger vehicle</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>2012-07-01 00:05:00</td>\n",
       "      <td>2012</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>manhattan</td>\n",
       "      <td>10036.0000</td>\n",
       "      <td>40.7621</td>\n",
       "      <td>-73.9974</td>\n",
       "      <td>(40.7621266, -73.9973865)</td>\n",
       "      <td>11 avenue</td>\n",
       "      <td>west 44 street</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>37632.0000</td>\n",
       "      <td>passenger vehicle</td>\n",
       "      <td>bus</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>2012-07-01 00:05:00</td>\n",
       "      <td>2012</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>brooklyn</td>\n",
       "      <td>11223.0000</td>\n",
       "      <td>40.5889</td>\n",
       "      <td>-73.9727</td>\n",
       "      <td>(40.5888678, -73.9727446)</td>\n",
       "      <td>west 3 street</td>\n",
       "      <td>bouck court</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>116256.0000</td>\n",
       "      <td>passenger vehicle</td>\n",
       "      <td>sport utility / station wagon</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>2012-07-01 00:10:00</td>\n",
       "      <td>2012</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40.7336</td>\n",
       "      <td>-73.9238</td>\n",
       "      <td>(40.73361, -73.9238405)</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>3044659.0000</td>\n",
       "      <td>passenger vehicle</td>\n",
       "      <td>passenger vehicle</td>\n",
       "      <td>passenger vehicle</td>\n",
       "      <td>passenger vehicle</td>\n",
       "      <td>unknown</td>\n",
       "      <td>2012-07-01 00:10:00</td>\n",
       "      <td>2012</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>brooklyn</td>\n",
       "      <td>11215.0000</td>\n",
       "      <td>40.6774</td>\n",
       "      <td>-73.9830</td>\n",
       "      <td>(40.6774056, -73.9830482)</td>\n",
       "      <td>4 avenue</td>\n",
       "      <td>union street</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>175808.0000</td>\n",
       "      <td>unknown</td>\n",
       "      <td>bicycle</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>2012-07-01 00:20:00</td>\n",
       "      <td>2012</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     borough   zip_code  accident_latitude  accident_longitude  \\\n",
       "0    unknown        NaN            40.6978            -73.8139   \n",
       "1  manhattan 10036.0000            40.7621            -73.9974   \n",
       "2   brooklyn 11223.0000            40.5889            -73.9727   \n",
       "3    unknown        NaN            40.7336            -73.9238   \n",
       "4   brooklyn 11215.0000            40.6774            -73.9830   \n",
       "\n",
       "                    location on_street_name cross_street_name off_street_name  \\\n",
       "0  (40.6977532, -73.8139159)        unknown           unknown         unknown   \n",
       "1  (40.7621266, -73.9973865)      11 avenue    west 44 street         unknown   \n",
       "2  (40.5888678, -73.9727446)  west 3 street       bouck court         unknown   \n",
       "3    (40.73361, -73.9238405)        unknown           unknown         unknown   \n",
       "4  (40.6774056, -73.9830482)       4 avenue      union street         unknown   \n",
       "\n",
       "   number_of_persons_injured  number_of_persons_killed  ...  \\\n",
       "0                          1                         0  ...   \n",
       "1                          0                         0  ...   \n",
       "2                          0                         0  ...   \n",
       "3                          1                         0  ...   \n",
       "4                          0                         0  ...   \n",
       "\n",
       "   contributing_factor_vehicle_5  collision_id  vehicle_type_code_1  \\\n",
       "0                        unknown  2999940.0000    passenger vehicle   \n",
       "1                        unknown    37632.0000    passenger vehicle   \n",
       "2                        unknown   116256.0000    passenger vehicle   \n",
       "3                        unknown  3044659.0000    passenger vehicle   \n",
       "4                        unknown   175808.0000              unknown   \n",
       "\n",
       "             vehicle_type_code_2  vehicle_type_code_3  vehicle_type_code_4  \\\n",
       "0              passenger vehicle              unknown              unknown   \n",
       "1                            bus              unknown              unknown   \n",
       "2  sport utility / station wagon              unknown              unknown   \n",
       "3              passenger vehicle    passenger vehicle    passenger vehicle   \n",
       "4                        bicycle              unknown              unknown   \n",
       "\n",
       "  vehicle_type_code_5      crash_datetime  year month  \n",
       "0             unknown 2012-07-01 00:05:00  2012     7  \n",
       "1             unknown 2012-07-01 00:05:00  2012     7  \n",
       "2             unknown 2012-07-01 00:10:00  2012     7  \n",
       "3             unknown 2012-07-01 00:10:00  2012     7  \n",
       "4             unknown 2012-07-01 00:20:00  2012     7  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtype_dict = { # I also convert latitude and longitude to float32 here since it greatly improves efficiency- float32 can hold only 7 decimals, but this should be enough (accurate to ~10m)\n",
    "    'Int64': ['zip_code'],\n",
    "    'float64': ['accident_latitude', 'accident_longitude'],\n",
    "    'str': ['borough','on_street_name','cross_street_name','off_street_name','contributing_factor_vehicle_1', 'contributing_factor_vehicle_2',\n",
    "           'contributing_factor_vehicle_3','contributing_factor_vehicle_4','contributing_factor_vehicle_5','vehicle_type_code_1',\n",
    "           'vehicle_type_code_2','vehicle_type_code_3','vehicle_type_code_4','vehicle_type_code_5'], \n",
    "    'int8': ['number_of_persons_injured', 'number_of_persons_killed', 'number_of_pedestrians_injured', 'number_of_pedestrians_killed', \n",
    "             'number_of_cyclists_injured', 'number_of_cyclists_killed', 'number_of_motorists_injured', 'number_of_motorists_killed'],\n",
    "}\n",
    "\n",
    "dtype_mapping = {}\n",
    "for dtype, columns in dtype_dict.items():\n",
    "    for col in columns:\n",
    "        dtype_mapping[col] = dtype\n",
    "\n",
    "for col in df.columns:\n",
    "    if col not in dtype_mapping.keys():\n",
    "        continue\n",
    "    elif col == 'zip_code': # cant convert to int directly because it has string with empty entries\n",
    "        df[col] = pd.to_numeric(df[col].str.strip(), errors='coerce').astype('float32')\n",
    "        df[col] = df[col].round(0).astype('Int64');\n",
    "    elif col in dtype_dict['str']: # for categorical data, replace nans with 'unknown' cat\n",
    "        df[col].fillna('unknown', inplace=True)\n",
    "        df[col] = df[col].astype(dtype_mapping[col])\n",
    "        df[col]  = df[col].str.strip().str.lower() # clean string entries\n",
    "        # df[col] = df[col].astype('category') # string to category, as it needs less memory\n",
    "        # df[col] = df[col].cat.add_categories('unknown').fillna('unknown') # add unknown category for nans\n",
    "    elif col in dtype_dict['int8']:\n",
    "        df[col] = df[col].astype(dtype_mapping[col])\n",
    "        df[col].round(0)\n",
    "    else: \n",
    "        df[col] = df[col].astype(dtype_mapping[col])\n",
    "        df[col].fillna(np.nan, inplace=True)\n",
    "            \n",
    "df['crash_datetime'] = pd.to_datetime(df['crash_date'] + ' ' + df['crash_time']) #\n",
    "df.drop(['crash_date','crash_time'], axis=1, inplace=True)\n",
    "df['year'] = df['crash_datetime'].dt.year.astype('int32') # add year column \n",
    "df['month'] = df['crash_datetime'].dt.month.astype('int8') # add month column \n",
    "\n",
    "dupl_before =df.shape[0]\n",
    "df = df.drop_duplicates().sort_values(by='crash_datetime')# drop duplicates and sort to start rental time/date\n",
    "print(f'{dupl_before - df.shape[0]} duplicates removed')\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "print(' Unique values in zip_code: '); print(df.zip_code.unique())\n",
    "df.head() # show cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9d08181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Nan_count    Total\n",
      "borough                                0  2120518\n",
      "zip_code                         1721072  2120518\n",
      "accident_latitude                 247820  2120518\n",
      "accident_longitude                247820  2120518\n",
      "location                          247820  2120518\n",
      "on_street_name                         0  2120518\n",
      "cross_street_name                      0  2120518\n",
      "off_street_name                        0  2120518\n",
      "number_of_persons_injured              0  2120518\n",
      "number_of_persons_killed               0  2120518\n",
      "number_of_pedestrians_injured          0  2120518\n",
      "number_of_pedestrians_killed           0  2120518\n",
      "number_of_cyclists_injured             0  2120518\n",
      "number_of_cyclists_injured             0  2120518\n",
      "number_of_motorists_injured            0  2120518\n",
      "number_of_motorists_killed             0  2120518\n",
      "contributing_factor_vehicle_1          0  2120518\n",
      "contributing_factor_vehicle_2          0  2120518\n",
      "contributing_factor_vehicle_3          0  2120518\n",
      "contributing_factor_vehicle_4          0  2120518\n",
      "contributing_factor_vehicle_5          0  2120518\n",
      "collision_id                           0  2120518\n",
      "vehicle_type_code_1                    0  2120518\n",
      "vehicle_type_code_2                    0  2120518\n",
      "vehicle_type_code_3                    0  2120518\n",
      "vehicle_type_code_4                    0  2120518\n",
      "vehicle_type_code_5                    0  2120518\n",
      "crash_datetime                         0  2120518\n",
      "year                                   0  2120518\n",
      "month                                  0  2120518\n"
     ]
    }
   ],
   "source": [
    "# Check if df still has missing values (nans) and how many\n",
    "summary_table = pd.DataFrame({\n",
    "    'Nan_count': df.isna().sum(),\n",
    "    'Total': df.shape[0]\n",
    "})\n",
    "\n",
    "print(summary_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa76ab8-4560-4546-83c1-889c204081c3",
   "metadata": {},
   "source": [
    "Missing data in string columns handled well -> all replaced with 'unknown', no nans left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e799370c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zip_code</th>\n",
       "      <th>accident_latitude</th>\n",
       "      <th>accident_longitude</th>\n",
       "      <th>number_of_persons_injured</th>\n",
       "      <th>number_of_persons_killed</th>\n",
       "      <th>number_of_pedestrians_injured</th>\n",
       "      <th>number_of_pedestrians_killed</th>\n",
       "      <th>number_of_cyclists_injured</th>\n",
       "      <th>number_of_cyclists_injured</th>\n",
       "      <th>number_of_motorists_injured</th>\n",
       "      <th>number_of_motorists_killed</th>\n",
       "      <th>collision_id</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>399446.0000</td>\n",
       "      <td>1872698.0000</td>\n",
       "      <td>1872698.0000</td>\n",
       "      <td>2120518.0000</td>\n",
       "      <td>2120518.0000</td>\n",
       "      <td>2120518.0000</td>\n",
       "      <td>2120518.0000</td>\n",
       "      <td>2120518.0000</td>\n",
       "      <td>2120518.0000</td>\n",
       "      <td>2120518.0000</td>\n",
       "      <td>2120518.0000</td>\n",
       "      <td>2120518.0000</td>\n",
       "      <td>2120518.0000</td>\n",
       "      <td>2120518.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10801.2070</td>\n",
       "      <td>40.6211</td>\n",
       "      <td>-73.7401</td>\n",
       "      <td>0.3161</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0573</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>0.0276</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.2272</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>3193115.7500</td>\n",
       "      <td>2017.1821</td>\n",
       "      <td>6.6714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>568.1274</td>\n",
       "      <td>2.1173</td>\n",
       "      <td>4.0326</td>\n",
       "      <td>0.7056</td>\n",
       "      <td>0.0412</td>\n",
       "      <td>0.2455</td>\n",
       "      <td>0.0280</td>\n",
       "      <td>0.1659</td>\n",
       "      <td>0.0109</td>\n",
       "      <td>0.6668</td>\n",
       "      <td>0.0275</td>\n",
       "      <td>1503171.0000</td>\n",
       "      <td>3.2036</td>\n",
       "      <td>3.3976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>10000.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>-201.3600</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>22.0000</td>\n",
       "      <td>2012.0000</td>\n",
       "      <td>1.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>10065.0000</td>\n",
       "      <td>40.6677</td>\n",
       "      <td>-73.9748</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3166249.2500</td>\n",
       "      <td>2015.0000</td>\n",
       "      <td>4.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>11204.0000</td>\n",
       "      <td>40.7206</td>\n",
       "      <td>-73.9272</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3696517.5000</td>\n",
       "      <td>2017.0000</td>\n",
       "      <td>7.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>11235.0000</td>\n",
       "      <td>40.7696</td>\n",
       "      <td>-73.8667</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>4226881.7500</td>\n",
       "      <td>2019.0000</td>\n",
       "      <td>10.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>11697.0000</td>\n",
       "      <td>43.3444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>43.0000</td>\n",
       "      <td>8.0000</td>\n",
       "      <td>27.0000</td>\n",
       "      <td>6.0000</td>\n",
       "      <td>4.0000</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>43.0000</td>\n",
       "      <td>5.0000</td>\n",
       "      <td>4757303.0000</td>\n",
       "      <td>2024.0000</td>\n",
       "      <td>12.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         zip_code  accident_latitude  accident_longitude  \\\n",
       "count 399446.0000       1872698.0000        1872698.0000   \n",
       "mean   10801.2070            40.6211            -73.7401   \n",
       "std      568.1274             2.1173              4.0326   \n",
       "min    10000.0000             0.0000           -201.3600   \n",
       "25%    10065.0000            40.6677            -73.9748   \n",
       "50%    11204.0000            40.7206            -73.9272   \n",
       "75%    11235.0000            40.7696            -73.8667   \n",
       "max    11697.0000            43.3444              0.0000   \n",
       "\n",
       "       number_of_persons_injured  number_of_persons_killed  \\\n",
       "count               2120518.0000              2120518.0000   \n",
       "mean                      0.3161                    0.0015   \n",
       "std                       0.7056                    0.0412   \n",
       "min                       0.0000                    0.0000   \n",
       "25%                       0.0000                    0.0000   \n",
       "50%                       0.0000                    0.0000   \n",
       "75%                       0.0000                    0.0000   \n",
       "max                      43.0000                    8.0000   \n",
       "\n",
       "       number_of_pedestrians_injured  number_of_pedestrians_killed  \\\n",
       "count                   2120518.0000                  2120518.0000   \n",
       "mean                          0.0573                        0.0008   \n",
       "std                           0.2455                        0.0280   \n",
       "min                           0.0000                        0.0000   \n",
       "25%                           0.0000                        0.0000   \n",
       "50%                           0.0000                        0.0000   \n",
       "75%                           0.0000                        0.0000   \n",
       "max                          27.0000                        6.0000   \n",
       "\n",
       "       number_of_cyclists_injured  number_of_cyclists_injured  \\\n",
       "count                2120518.0000                2120518.0000   \n",
       "mean                       0.0276                      0.0001   \n",
       "std                        0.1659                      0.0109   \n",
       "min                        0.0000                      0.0000   \n",
       "25%                        0.0000                      0.0000   \n",
       "50%                        0.0000                      0.0000   \n",
       "75%                        0.0000                      0.0000   \n",
       "max                        4.0000                      2.0000   \n",
       "\n",
       "       number_of_motorists_injured  number_of_motorists_killed  collision_id  \\\n",
       "count                 2120518.0000                2120518.0000  2120518.0000   \n",
       "mean                        0.2272                      0.0006  3193115.7500   \n",
       "std                         0.6668                      0.0275  1503171.0000   \n",
       "min                         0.0000                      0.0000       22.0000   \n",
       "25%                         0.0000                      0.0000  3166249.2500   \n",
       "50%                         0.0000                      0.0000  3696517.5000   \n",
       "75%                         0.0000                      0.0000  4226881.7500   \n",
       "max                        43.0000                      5.0000  4757303.0000   \n",
       "\n",
       "              year        month  \n",
       "count 2120518.0000 2120518.0000  \n",
       "mean     2017.1821       6.6714  \n",
       "std         3.2036       3.3976  \n",
       "min      2012.0000       1.0000  \n",
       "25%      2015.0000       4.0000  \n",
       "50%      2017.0000       7.0000  \n",
       "75%      2019.0000      10.0000  \n",
       "max      2024.0000      12.0000  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0974109f-dc6f-49da-982d-31a38e8240ef",
   "metadata": {},
   "source": [
    "Check how many collisions involved a bike; list all entries that contain 'bi', as probably the entries are inconsistent, \n",
    "and they could be 'bike','bicycle' or 'e-bike'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68ae2d38-64b0-4601-9202-3c7b331465ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vehicle_type_code_1\n",
      "['bicycle' 'bike' 'motorbike' 'e bik' 'minibike' 'ebike' 'mobil' 'e- bi'\n",
      " 'e-bik' 'snowmobile' 'cabin' 'bicyc' 'e-bike' 'dirt bike' 'e bike'\n",
      " 'e bike uni' 'e bike w p' 'combinatio' 'liabitiy' 'pedal bike'\n",
      " 'mobility s']\n",
      "vehicle_type_code_2\n",
      "['bicycle' 'bike' 'motorbike' 'minibike' 'e bik' 'ebike' 'e-bik'\n",
      " 'snowmobile' 'mobil' 'big r' 'e/bik' 'e-bike' 'mobile foo' 'dirt bike'\n",
      " 'mobile' 'dirtbike' 'uni e-bike' 'e bike' 'gas bicycl' 'ambiance'\n",
      " 'dart bike' 'moped bike' 'gas bike' 'mobility s' 'citibike' 'e bike w p'\n",
      " 'scooter bi']\n",
      "vehicle_type_code_3\n",
      "['bicycle' 'bike' 'motorbike' 'e-bik' 'e-bike' 'dirt bike']\n",
      "vehicle_type_code_4\n",
      "['bicycle' 'bike' 'e-bike' 'snowmobile' 'motorbike']\n"
     ]
    }
   ],
   "source": [
    "for col in ['vehicle_type_code_1','vehicle_type_code_2','vehicle_type_code_3','vehicle_type_code_4']:\n",
    "    bike_entries = df[df[col].str.contains('bi', case=False, na=False)]\n",
    "    print(col); print(bike_entries[col].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fc5f75-33c7-480d-82a8-781c6dc8d2cf",
   "metadata": {},
   "source": [
    "As we can see, a lot of different words are used to describe 'bike', with many spelling mistakes. To filter out all bikes, we can select all entries containing 'bik' or 'bic' but exclude 'motorbike'. I will now correct these entries and create an additional column with 'bike'/'no_bike', when any of these 4 columns contain a bike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c8f2c2c8-4330-4002-8dec-da2c991d79b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['no_bike' 'bike']\n",
      "  bike_involved vehicle_type_code_1            vehicle_type_code_2  \\\n",
      "0       no_bike   passenger vehicle              passenger vehicle   \n",
      "1       no_bike   passenger vehicle                            bus   \n",
      "2       no_bike   passenger vehicle  sport utility / station wagon   \n",
      "\n",
      "  vehicle_type_code_3 vehicle_type_code_4  \n",
      "0             unknown             unknown  \n",
      "1             unknown             unknown  \n",
      "2             unknown             unknown  \n",
      "   bike_involved vehicle_type_code_1 vehicle_type_code_2 vehicle_type_code_3  \\\n",
      "4           bike             unknown             bicycle             unknown   \n",
      "42          bike                taxi             bicycle             unknown   \n",
      "98          bike   passenger vehicle             bicycle             unknown   \n",
      "\n",
      "   vehicle_type_code_4  \n",
      "4              unknown  \n",
      "42             unknown  \n",
      "98             unknown  \n"
     ]
    }
   ],
   "source": [
    "# Create a mask that checks for 'bik' or 'bic' in any of the 4 columns, excluding 'motorbike'\n",
    "df['bike_involved'] = 'no_bike'\n",
    "# Create a mask that checks for 'bik' or 'bic' in any of the 4 columns, excluding 'motorbike'\n",
    "mask = (\n",
    "    df[['vehicle_type_code_1', 'vehicle_type_code_2', 'vehicle_type_code_3', 'vehicle_type_code_4']]\n",
    "    .apply(lambda col: col.str.contains('bik|bic', case=False, na=False))\n",
    "    .any(axis=1)  # Check if any column contains 'bik' or 'bic'\n",
    ") & (\n",
    "    ~df[['vehicle_type_code_1', 'vehicle_type_code_2', 'vehicle_type_code_3', 'vehicle_type_code_4']]\n",
    "    .apply(lambda col: col.str.contains('motorbike', case=False, na=False))\n",
    "    .any(axis=1)  # Exclude rows where 'motorbike' appears\n",
    ")\n",
    "\n",
    "# Set 'bike_involved' to 'bike' where the condition is met\n",
    "df.loc[mask, 'bike_involved'] = 'bike'\n",
    "\n",
    "print(df['bike_involved'].unique())\n",
    "print(df.loc[df['bike_involved'] == 'no_bike',['bike_involved','vehicle_type_code_1', 'vehicle_type_code_2', 'vehicle_type_code_3', 'vehicle_type_code_4']].head(3))\n",
    "print(df.loc[df['bike_involved'] == 'bike',['bike_involved','vehicle_type_code_1', 'vehicle_type_code_2', 'vehicle_type_code_3', 'vehicle_type_code_4']].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3cd92008-4764-48c4-ba6f-a5491bc9836a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(cleaned_dir + '/collisions_cleaned.csv') # save cleaned version"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
