{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1487e09e",
   "metadata": {},
   "source": [
    "# AXA coding challenge\n",
    "Data:\n",
    "1. Citibike: https://s3.amazonaws.com/tripdata/index.html\n",
    "2. NYPD:  https://data.cityofnewyork.us/Public-Safety/Motor-Vehicle-Collisions-Crashes/h9gi-nx95/about_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6be0356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge\n"
     ]
    }
   ],
   "source": [
    "# Install packages (only once)\n",
    "#!pip install selenium webdriver-manager\n",
    "\n",
    "# Import modules\n",
    "import os # basic\n",
    "import zipfile\n",
    "import datetime\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "import dask.dataframe as dd\n",
    "\n",
    "from selenium import webdriver # for downloading files automatically\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "current_dir = os.getcwd() # current dir\n",
    "print('Current directory: ' + current_dir)\n",
    "extract_dir = current_dir + '/data/bike-tripdata'  # directory where extracted files from 1. will be saved\n",
    "cleaned_dir = extract_dir + '_cleaned' # directory where cleaned and concatenated df will be saved\n",
    "\n",
    "pd.options.display.float_format = '{:.4f}'.format # set pd output to 2 decimals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "921e8034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "# to download files from an url\n",
    "def download_files(url, save_path):\n",
    "    response = requests.get(url, stream=True)\n",
    "    with open(save_path, 'wb') as file:\n",
    "        for chunk in response.iter_content(chunk_size=1024):\n",
    "            if chunk:\n",
    "                file.write(chunk)\n",
    "    print(f\"Downloaded {save_path}\")\n",
    "    \n",
    "# to clean column names\n",
    "def clean_column_names(df, column_mapping=None):\n",
    "    # strip whitespace, convert to lowercase, and replace spaces with underscores\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('-', '_')\n",
    "    \n",
    "    # apply manual column mapping if specified\n",
    "    if column_mapping:\n",
    "        df.rename(columns=column_mapping, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# check unique column names across all .csv files in list_files \n",
    "def list_unique_col_names(list_files):\n",
    "    unique_column_names = []\n",
    "    for csv_file in list_files:\n",
    "        file_path = os.path.join(extract_dir, csv_file)\n",
    "        df = pd.read_csv(file_path, nrows=1)\n",
    "        #print(df.columns) # visual check\n",
    "        [unique_column_names.append(col) for col in df.columns if col not in unique_column_names]\n",
    "    unique_column_names.sort()\n",
    "    \n",
    "    return unique_column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1af9203",
   "metadata": {},
   "source": [
    "## Download Citibike data automatically from url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e14c0153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "url = \"https://s3.amazonaws.com/tripdata/index.html\" # url to data files\n",
    "driver_path = 'C:/Drivers/chromedriver-win64_128/chromedriver.exe' # Chrome driver for web interaction, needed by selenium - must match Chrome version\n",
    "\n",
    "# Download files\n",
    "service = Service(driver_path) # initialize the Chrome driver\n",
    "driver = webdriver.Chrome(service=service)\n",
    "driver.get(url) # navigate to website\n",
    "time.sleep(5)  # give the page time to load the dynamic content\n",
    "html = driver.page_source # get the page source after JavaScript has executed\n",
    "soup = BeautifulSoup(html, 'html.parser') # parse the HTML\n",
    "\n",
    "# find all .zip links\n",
    "file_links = []\n",
    "for link in soup.find_all('a', href=True):\n",
    "    if link['href'].endswith('.zip'): # on this website, files are .zip format\n",
    "        file_links.append(link['href'])\n",
    "print(file_links[:2]) # check if the file paths are retrieved correctly by printing a few\n",
    "\n",
    "driver.quit() # close the browser\n",
    "\n",
    "if not os.path.exists(current_dir+'/downloads'): # directory to save the downloaded files\n",
    "    os.makedirs(current_dir+'/downloads')\n",
    "\n",
    "for file_link in file_links: # loop through all the zip links and download them\n",
    "    filename = os.path.join(current_dir+'/downloads', os.path.basename(file_link))\n",
    "    \n",
    "    if not file_link.startswith('http'): # if the link is relative, make it an absolute URL by appending the base URL\n",
    "        file_link = url + file_link\n",
    "\n",
    "    download_files(file_link, filename) # download the file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea75400",
   "metadata": {},
   "source": [
    "## Unzip & reorganize files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381447ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - alternatively (instead of next cell), unzip first and then reorganize files\n",
    "\n",
    "# Unzip files  \n",
    "# zip_dir = current_dir+'/downloads' # directory containing the zip files\n",
    "# extract_dir = current_dir+'/data' # directory where extracted files will be saved\n",
    "\n",
    "# for filename in os.listdir(zip_dir): # loop through all files in the directory\n",
    "#     if filename.endswith('.zip') :\n",
    "#         zip_file_path = os.path.join(zip_dir, filename)\n",
    "#         new_file_path = extract_dir + '/' + filename[:-4] + '.csv' # remove '.zip' and subfolders from the target path name\n",
    "#         os.makedirs(new_file_path, exist_ok=True)  # create the directory if it doesn't exist\n",
    "\n",
    "#         with zipfile.ZipFile(zip_file_path, 'r') as zip_ref: # extract the zip file\n",
    "#             for member in zip_ref.namelist():\n",
    "#                 if '_MACOSX' not in member: # skip any file or folder inside \"_MACOSX\" (for MAC computers, not needed)\n",
    "#                     zip_ref.extract(member, new_file_path) # extract to the specified directory\n",
    "\n",
    "#             print(f'Extracted: {member} to {new_file_path}')\n",
    "\n",
    "\n",
    "# # Move  files from subfolders in subfolders to 1 folder\n",
    "\n",
    "# import shutil\n",
    "\n",
    "# source_dir = current_dir + '/data'\n",
    "# destination_dir = current_dir + '/data_test'\n",
    "# os.makedirs(destination_dir, exist_ok=True)\n",
    "\n",
    "# for root, dirs, files in os.walk(source_dir):\n",
    "#     for file in files:\n",
    "#         if file.endswith('.csv') and not file.startswith('.'): # select .csv files, skip files starting with '.' \n",
    "#             if '_MACOSX' in root:\n",
    "#                 continue  # skip this directory and its contents, for MAC\n",
    "\n",
    "#             source_file = os.path.join(root, file)\n",
    "#             destination_file = os.path.join(destination_dir, file)\n",
    "            \n",
    "#             shutil.move(source_file, destination_file) # or shutil.copy\n",
    "#             print(f\"Moved: {source_file} -> {destination_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1c46ca43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 201309-citibike-tripdata.csv\n",
      "Extracted 201311-citibike-tripdata.csv\n",
      "Extracted 201307-citibike-tripdata.csv\n",
      "Extracted 201308-citibike-tripdata.csv\n",
      "Extracted 201306-citibike-tripdata.csv\n",
      "Extracted 201310-citibike-tripdata.csv\n",
      "Extracted 201312-citibike-tripdata.csv\n",
      "Extracted 201312-citibike-tripdata_1.csv\n",
      "Extracted 201311-citibike-tripdata_1.csv\n",
      "Extracted 201307-citibike-tripdata_1.csv\n",
      "Extracted 201310-citibike-tripdata_2.csv\n",
      "Extracted 201310-citibike-tripdata_1.csv\n",
      "Extracted 201309-citibike-tripdata_2.csv\n",
      "Extracted 201309-citibike-tripdata_1.csv\n",
      "Extracted 201308-citibike-tripdata_1.csv\n",
      "Extracted 201308-citibike-tripdata_2.csv\n",
      "Extracted 201306-citibike-tripdata_1.csv\n",
      "from 2013-citibike-tripdata.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted 201404-citibike-tripdata_1.csv\n",
      "Extracted 201412-citibike-tripdata_1.csv\n",
      "Extracted 201411-citibike-tripdata_1.csv\n",
      "Extracted 201407-citibike-tripdata_1.csv\n",
      "Extracted 201410-citibike-tripdata_1.csv\n",
      "Extracted 201409-citibike-tripdata_1.csv\n",
      "Extracted 201408-citibike-tripdata_1.csv\n",
      "Extracted 201406-citibike-tripdata_1.csv\n",
      "Extracted 201403-citibike-tripdata_1.csv\n",
      "Extracted 201401-citibike-tripdata_1.csv\n",
      "Extracted 201402-citibike-tripdata_1.csv\n",
      "Extracted 201405-citibike-tripdata_1.csv\n",
      "from 2014-citibike-tripdata.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted 201504-citibike-tripdata_1.csv\n",
      "Extracted 201512-citibike-tripdata_1.csv\n",
      "Extracted 201511-citibike-tripdata_1.csv\n",
      "Extracted 201507-citibike-tripdata_1.csv\n",
      "Extracted 201507-citibike-tripdata_2.csv\n",
      "Extracted 201510-citibike-tripdata_2.csv\n",
      "Extracted 201510-citibike-tripdata_1.csv\n",
      "Extracted 201509-citibike-tripdata_2.csv\n",
      "Extracted 201509-citibike-tripdata_1.csv\n",
      "Extracted 201508-citibike-tripdata_1.csv\n",
      "Extracted 201508-citibike-tripdata_2.csv\n",
      "Extracted 201506-citibike-tripdata_1.csv\n",
      "Extracted 201503-citibike-tripdata_1.csv\n",
      "Extracted 201501-citibike-tripdata_1.csv\n",
      "Extracted 201502-citibike-tripdata_1.csv\n",
      "Extracted 201505-citibike-tripdata_1.csv\n",
      "from 2015-citibike-tripdata.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted 201604-citibike-tripdata_1.csv\n",
      "Extracted 201604-citibike-tripdata_2.csv\n",
      "Extracted 201612-citibike-tripdata_1.csv\n",
      "Extracted 201611-citibike-tripdata_2.csv\n",
      "Extracted 201611-citibike-tripdata_1.csv\n",
      "Extracted 201607-citibike-tripdata_2.csv\n",
      "Extracted 201607-citibike-tripdata_1.csv\n",
      "Extracted 201610-citibike-tripdata_1.csv\n",
      "Extracted 201610-citibike-tripdata_2.csv\n",
      "Extracted 201609-citibike-tripdata_1.csv\n",
      "Extracted 201609-citibike-tripdata_2.csv\n",
      "Extracted 201608-citibike-tripdata_2.csv\n",
      "Extracted 201608-citibike-tripdata_1.csv\n",
      "Extracted 201606-citibike-tripdata_1.csv\n",
      "Extracted 201606-citibike-tripdata_2.csv\n",
      "Extracted 201603-citibike-tripdata_1.csv\n",
      "Extracted 201601-citibike-tripdata_1.csv\n",
      "Extracted 201602-citibike-tripdata_1.csv\n",
      "Extracted 201605-citibike-tripdata_2.csv\n",
      "Extracted 201605-citibike-tripdata_1.csv\n",
      "from 2016-citibike-tripdata.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted 201704-citibike-tripdata.csv_1.csv\n",
      "Extracted 201704-citibike-tripdata.csv_2.csv\n",
      "Extracted 201712-citibike-tripdata.csv_1.csv\n",
      "Extracted 201711-citibike-tripdata.csv_2.csv\n",
      "Extracted 201711-citibike-tripdata.csv_1.csv\n",
      "Extracted 201707-citibike-tripdata.csv_2.csv\n",
      "Extracted 201707-citibike-tripdata.csv_1.csv\n",
      "Extracted 201710-citibike-tripdata.csv_1.csv\n",
      "Extracted 201710-citibike-tripdata.csv_2.csv\n",
      "Extracted 201709-citibike-tripdata.csv_2.csv\n",
      "Extracted 201709-citibike-tripdata.csv_1.csv\n",
      "Extracted 201708-citibike-tripdata.csv_1.csv\n",
      "Extracted 201708-citibike-tripdata.csv_2.csv\n",
      "Extracted 201706-citibike-tripdata.csv_1.csv\n",
      "Extracted 201706-citibike-tripdata.csv_2.csv\n",
      "Extracted 201703-citibike-tripdata.csv_1.csv\n",
      "Extracted 201701-citibike-tripdata.csv_1.csv\n",
      "Extracted 201702-citibike-tripdata.csv_1.csv\n",
      "Extracted 201705-citibike-tripdata.csv_2.csv\n",
      "Extracted 201705-citibike-tripdata.csv_1.csv\n",
      "from 2017-citibike-tripdata.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted 201809-citibike-tripdata.csv\n",
      "Extracted 201801-citibike-tripdata.csv\n",
      "Extracted 201803-citibike-tripdata.csv\n",
      "Extracted 201805-citibike-tripdata.csv\n",
      "Extracted 201807-citibike-tripdata.csv\n",
      "Extracted 201811-citibike-tripdata.csv\n",
      "Extracted 201804-citibike-tripdata_1.csv\n",
      "Extracted 201804-citibike-tripdata_2.csv\n",
      "Extracted 201808-citibike-tripdata.csv\n",
      "Extracted 201802-citibike-tripdata.csv\n",
      "Extracted 201812-citibike-tripdata.csv\n",
      "Extracted 201804-citibike-tripdata.csv\n",
      "Extracted 201810-citibike-tripdata.csv\n",
      "Extracted 201806-citibike-tripdata.csv\n",
      "Extracted 201804-citibike-tripdata_1.csv\n",
      "Extracted 201804-citibike-tripdata_2.csv\n",
      "Extracted 201812-citibike-tripdata_1.csv\n",
      "Extracted 201812-citibike-tripdata_2.csv\n",
      "Extracted 201811-citibike-tripdata_2.csv\n",
      "Extracted 201811-citibike-tripdata_1.csv\n",
      "Extracted 201807-citibike-tripdata_2.csv\n",
      "Extracted 201807-citibike-tripdata_1.csv\n",
      "Extracted 201810-citibike-tripdata_1.csv\n",
      "Extracted 201810-citibike-tripdata_2.csv\n",
      "Extracted 201809-citibike-tripdata_1.csv\n",
      "Extracted 201809-citibike-tripdata_2.csv\n",
      "Extracted 201808-citibike-tripdata_2.csv\n",
      "Extracted 201808-citibike-tripdata_1.csv\n",
      "Extracted 201806-citibike-tripdata_1.csv\n",
      "Extracted 201806-citibike-tripdata_2.csv\n",
      "Extracted 201803-citibike-tripdata_1.csv\n",
      "Extracted 201801-citibike-tripdata_1.csv\n",
      "Extracted 201802-citibike-tripdata_1.csv\n",
      "Extracted 201805-citibike-tripdata_2.csv\n",
      "Extracted 201805-citibike-tripdata_1.csv\n",
      "from 2018-citibike-tripdata.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted 201904-citibike-tripdata_2.csv\n",
      "Extracted 201904-citibike-tripdata_1.csv\n",
      "Extracted 201912-citibike-tripdata_1.csv\n",
      "Extracted 201911-citibike-tripdata_1.csv\n",
      "Extracted 201911-citibike-tripdata_2.csv\n",
      "Extracted 201907-citibike-tripdata_1.csv\n",
      "Extracted 201907-citibike-tripdata_3.csv\n",
      "Extracted 201907-citibike-tripdata_2.csv\n",
      "Extracted 201910-citibike-tripdata_3.csv\n",
      "Extracted 201910-citibike-tripdata_2.csv\n",
      "Extracted 201910-citibike-tripdata_1.csv\n",
      "Extracted 201909-citibike-tripdata_3.csv\n",
      "Extracted 201909-citibike-tripdata_2.csv\n",
      "Extracted 201909-citibike-tripdata_1.csv\n",
      "Extracted 201908-citibike-tripdata_1.csv\n",
      "Extracted 201908-citibike-tripdata_2.csv\n",
      "Extracted 201908-citibike-tripdata_3.csv\n",
      "Extracted 201906-citibike-tripdata_2.csv\n",
      "Extracted 201906-citibike-tripdata_3.csv\n",
      "Extracted 201906-citibike-tripdata_1.csv\n",
      "Extracted 201903-citibike-tripdata_1.csv\n",
      "Extracted 201903-citibike-tripdata_2.csv\n",
      "Extracted 201901-citibike-tripdata_1.csv\n",
      "Extracted 201902-citibike-tripdata_1.csv\n",
      "Extracted 201905-citibike-tripdata_1.csv\n",
      "Extracted 201905-citibike-tripdata_2.csv\n",
      "from 2019-citibike-tripdata.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "from 2020-citibike-tripdata.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "from 2021-citibike-tripdata.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "from 2022-citibike-tripdata.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "from 2023-citibike-tripdata.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted 202401-citibike-tripdata.csv\n",
      "from 202401-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted 202402-citibike-tripdata.csv\n",
      "from 202402-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted 202403-citibike-tripdata.csv\n",
      "from 202403-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted 202404-citibike-tripdata.csv\n",
      "from 202404-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted 202405-citibike-tripdata_1.csv\n",
      "Extracted 202405-citibike-tripdata_2.csv\n",
      "Extracted 202405-citibike-tripdata_3.csv\n",
      "Extracted 202405-citibike-tripdata_4.csv\n",
      "Extracted 202405-citibike-tripdata_5.csv\n",
      "from 202405-citibike-tripdata.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted 202406-citibike-tripdata_5.csv\n",
      "Extracted 202406-citibike-tripdata_4.csv\n",
      "Extracted 202406-citibike-tripdata_1.csv\n",
      "Extracted 202406-citibike-tripdata_3.csv\n",
      "Extracted 202406-citibike-tripdata_2.csv\n",
      "from 202406-citibike-tripdata.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 202407-citibike-tripdata_1.csv\n",
      "Extracted 202407-citibike-tripdata_2.csv\n",
      "Extracted 202407-citibike-tripdata_3.csv\n",
      "Extracted 202407-citibike-tripdata_4.csv\n",
      "Extracted 202407-citibike-tripdata_5.csv\n",
      "from 202407-citibike-tripdata.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted 202408-citibike-tripdata_3.csv\n",
      "Extracted 202408-citibike-tripdata_2.csv\n",
      "Extracted 202408-citibike-tripdata_1.csv\n",
      "Extracted 202408-citibike-tripdata_5.csv\n",
      "Extracted 202408-citibike-tripdata_4.csv\n",
      "from 202408-citibike-tripdata.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201509-citibike-tripdata.csv\n",
      "from JC-201509-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201510-citibike-tripdata.csv\n",
      "from JC-201510-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201511-citibike-tripdata.csv\n",
      "from JC-201511-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201512-citibike-tripdata.csv\n",
      "from JC-201512-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-20161-citibike-tripdata.csv\n",
      "from JC-201601-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-20162-citibike-tripdata.csv\n",
      "from JC-201602-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-20163-citibike-tripdata.csv\n",
      "from JC-201603-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201604-citibike-tripdata.csv\n",
      "from JC-201604-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201605-citibike-tripdata.csv\n",
      "from JC-201605-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201606-citibike-tripdata.csv\n",
      "from JC-201606-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201607-citibike-tripdata.csv\n",
      "from JC-201607-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201608-citibike-tripdata.csv\n",
      "from JC-201608-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201609-citibike-tripdata.csv\n",
      "from JC-201609-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201610-citibike-tripdata.csv\n",
      "from JC-201610-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201611-citibike-tripdata.csv\n",
      "from JC-201611-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201612-citibike-tripdata.csv\n",
      "from JC-201612-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201701-citibike-tripdata.csv\n",
      "from JC-201701-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201702-citibike-tripdata.csv\n",
      "from JC-201702-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201703-citibike-tripdata.csv\n",
      "from JC-201703-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201704-citibike-tripdata.csv\n",
      "from JC-201704-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201705-citibike-tripdata.csv\n",
      "from JC-201705-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201706-citibike-tripdata.csv\n",
      "from JC-201706-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201707-citibike-tripdata.csv\n",
      "from JC-201707-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201708 citibike-tripdata.csv\n",
      "from JC-201708 citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201709-citibike-tripdata.csv\n",
      "from JC-201709-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201710-citibike-tripdata.csv\n",
      "from JC-201710-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201711-citibike-tripdata.csv\n",
      "from JC-201711-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201712-citibike-tripdata.csv\n",
      "from JC-201712-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201801-citibike-tripdata.csv\n",
      "from JC-201801-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201802-citibike-tripdata.csv\n",
      "from JC-201802-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201803-citibike-tripdata.csv\n",
      "from JC-201803-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201804-citibike-tripdata.csv\n",
      "from JC-201804-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201805-citibike-tripdata.csv\n",
      "from JC-201805-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201806-citibike-tripdata.csv\n",
      "from JC-201806-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201807-citibike-tripdata.csv\n",
      "from JC-201807-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201808-citibike-tripdata.csv\n",
      "from JC-201808-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201809-citibike-tripdata.csv\n",
      "from JC-201809-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201810-citibike-tripdata.csv\n",
      "from JC-201810-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201811-citibike-tripdata.csv\n",
      "from JC-201811-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201812-citibike-tripdata.csv\n",
      "from JC-201812-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201901-citibike-tripdata.csv\n",
      "from JC-201901-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201902-citibike-tripdata.csv\n",
      "from JC-201902-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201903-citibike-tripdata.csv\n",
      "from JC-201903-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201904-citibike-tripdata.csv\n",
      "from JC-201904-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201905-citibike-tripdata.csv\n",
      "from JC-201905-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201906-citibike-tripdata.csv\n",
      "from JC-201906-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201907-citibike-tripdata.csv\n",
      "from JC-201907-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201908-citibike-tripdata.csv\n",
      "from JC-201908-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201909-citibike-tripdata.csv\n",
      "from JC-201909-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201910-citibike-tripdata.csv\n",
      "from JC-201910-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201911-citibike-tripdata.csv\n",
      "from JC-201911-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201912-citibike-tripdata.csv\n",
      "from JC-201912-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202001-citibike-tripdata.csv\n",
      "from JC-202001-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202002-citibike-tripdata.csv\n",
      "from JC-202002-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202003-citibike-tripdata.csv\n",
      "from JC-202003-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202004-citibike-tripdata.csv\n",
      "from JC-202004-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202005-citibike-tripdata.csv\n",
      "from JC-202005-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202006-citibike-tripdata.csv\n",
      "from JC-202006-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted JC-202007-citibike-tripdata.csv\n",
      "from JC-202007-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202008-citibike-tripdata.csv\n",
      "from JC-202008-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202009-citibike-tripdata.csv\n",
      "from JC-202009-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202010-citibike-tripdata.csv\n",
      "from JC-202010-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202011-citibike-tripdata.csv\n",
      "from JC-202011-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202012-citibike-tripdata.csv\n",
      "from JC-202012-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202101-citibike-tripdata.csv\n",
      "from JC-202101-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202102-citibike-tripdata.csv\n",
      "from JC-202102-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202103-citibike-tripdata.csv\n",
      "from JC-202103-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202104-citibike-tripdata.csv\n",
      "from JC-202104-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202105-citibike-tripdata.csv\n",
      "from JC-202105-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202106-citibike-tripdata.csv\n",
      "from JC-202106-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202107-citibike-tripdata.csv\n",
      "from JC-202107-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202108-citibike-tripdata.csv\n",
      "from JC-202108-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202109-citibike-tripdata.csv\n",
      "from JC-202109-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202110-citibike-tripdata.csv\n",
      "from JC-202110-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202111-citibike-tripdata.csv\n",
      "from JC-202111-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202112-citibike-tripdata.csv\n",
      "from JC-202112-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202201-citibike-tripdata.csv\n",
      "from JC-202201-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202202-citibike-tripdata.csv\n",
      "from JC-202202-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202203-citibike-tripdata.csv\n",
      "from JC-202203-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202204-citibike-tripdata.csv\n",
      "from JC-202204-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202205-citibike-tripdata.csv\n",
      "from JC-202205-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202206-citibike-tripdata.csv\n",
      "from JC-202206-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202207-citbike-tripdata.csv\n",
      "from JC-202207-citbike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202208-citibike-tripdata.csv\n",
      "from JC-202208-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202209-citibike-tripdata.csv\n",
      "from JC-202209-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202210-citibike-tripdata.csv\n",
      "from JC-202210-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202211-citibike-tripdata.csv\n",
      "from JC-202211-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202212-citibike-tripdata.csv\n",
      "from JC-202212-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202301-citibike-tripdata.csv\n",
      "from JC-202301-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202302-citibike-tripdata.csv\n",
      "from JC-202302-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202303-citibike-tripdata.csv\n",
      "from JC-202303-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202304-citibike-tripdata.csv\n",
      "from JC-202304-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202305-citibike-tripdata.csv\n",
      "from JC-202305-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202306-citibike-tripdata.csv\n",
      "from JC-202306-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202307-citibike-tripdata.csv\n",
      "from JC-202307-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202308-citibike-tripdata.csv\n",
      "from JC-202308-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202309-citibike-tripdata.csv\n",
      "from JC-202309-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202310-citibike-tripdata.csv\n",
      "from JC-202310-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202311-citibike-tripdata.csv\n",
      "from JC-202311-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202312-citibike-tripdata.csv\n",
      "from JC-202312-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202401-citibike-tripdata.csv\n",
      "from JC-202401-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202402-citibike-tripdata.csv\n",
      "from JC-202402-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202403-citibike-tripdata.csv\n",
      "from JC-202403-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202404-citibike-tripdata.csv\n",
      "from JC-202404-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202406-citibike-tripdata.csv\n",
      "from JC-202406-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202407-citibike-tripdata.csv\n",
      "from JC-202407-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202408-citibike-tripdata.csv\n",
      "from JC-202408-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n"
     ]
    }
   ],
   "source": [
    "# Unzip files & reorganize simultaneously\n",
    "zip_dir = current_dir + '/downloads'  # directory containing the zip files\n",
    "extract_dir = current_dir + '/data/bike-tripdata'  # directory where extracted files will be saved\n",
    "\n",
    "os.makedirs(extract_dir, exist_ok=True)  # create the directory if it doesn't exist\n",
    "\n",
    "for filename in os.listdir(zip_dir):  # loop through all files in the directory\n",
    "    if filename.endswith('.zip'):\n",
    "        zip_file_path = os.path.join(zip_dir, filename)\n",
    "\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:  # extract the zip file\n",
    "            for member in zip_ref.namelist():\n",
    "                # skip any file or folder inside \"_MACOSX\" (for MAC computers, not needed), and files that do not end with .csv\n",
    "                if '_MACOSX' not in member and member.endswith('.csv'):  \n",
    "                    # get only the base name of the file (ignore folder structure in zip)\n",
    "                    base_member = os.path.basename(member)\n",
    "                    target_path = os.path.join(extract_dir, base_member)\n",
    "                    \n",
    "                    with zip_ref.open(member) as source, open(target_path, \"wb\") as target:\n",
    "                        target.write(source.read())  # write the extracted content to the single folder\n",
    "\n",
    "                    print(f'Extracted {base_member}')\n",
    "    print(f'... from {filename} to {extract_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7985a3c7",
   "metadata": {},
   "source": [
    "## Visualize dataset for cleaning\n",
    "### 1. Check which unique column names exist across all files\n",
    "### 2. Correct column names (strip uppercase and convert space to underscore)\n",
    "### 3. Map names to manual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "354591e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tripduration', 'starttime', 'stoptime', 'start station id', 'start station name', 'start station latitude', 'start station longitude', 'end station id', 'end station name', 'end station latitude', 'end station longitude', 'bikeid', 'usertype', 'birth year', 'gender', 'Trip Duration', 'Start Time', 'Stop Time', 'Start Station ID', 'Start Station Name', 'Start Station Latitude', 'Start Station Longitude', 'End Station ID', 'End Station Name', 'End Station Latitude', 'End Station Longitude', 'Bike ID', 'User Type', 'Birth Year', 'Gender', 'ride_id', 'rideable_type', 'started_at', 'ended_at', 'start_station_name', 'start_station_id', 'end_station_name', 'end_station_id', 'start_lat', 'start_lng', 'end_lat', 'end_lng', 'member_casual', 'Unnamed: 0', 'rideable_type_duplicate_column_name_1']\n"
     ]
    }
   ],
   "source": [
    "# Since CSV files do not contain the same column headers, check which ones exist in the dataset?\n",
    "list_files = [f for f in os.listdir(extract_dir) if f.endswith('.csv')]\n",
    "unique_column_names = []\n",
    "for csv_file in list_files:\n",
    "    file_path = os.path.join(extract_dir, csv_file)\n",
    "    df = pd.read_csv(file_path, nrows=3)\n",
    "    [unique_column_names.append(col) for col in df.columns if col not in unique_column_names]\n",
    "print(unique_column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854012b1",
   "metadata": {},
   "source": [
    "It turns out that the column names are not consistent, e.g. some files contain the column \"starttime\" while others contain the column \"Start Time\". This should be corrected. Additionally, column names should not contain spaces (\"start station latitude\" vs \"start_lat\"). Last, there are 2 strange column names which need to be checked: \"Unnamed: 0\" and \"rideable_type_duplicate_column_name_1\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d22e286",
   "metadata": {},
   "source": [
    "### Correct column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e319ed65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique column names: \n",
      "  ['Bike ID', 'Birth Year', 'End Station ID', 'End Station Latitude', 'End Station Longitude', 'End Station Name', 'Gender', 'Start Station ID', 'Start Station Latitude', 'Start Station Longitude', 'Start Station Name', 'Start Time', 'Stop Time', 'Trip Duration', 'Unnamed: 0', 'User Type', 'bikeid', 'birth year', 'end station id', 'end station latitude', 'end station longitude', 'end station name', 'end_lat', 'end_lng', 'end_station_id', 'end_station_name', 'ended_at', 'gender', 'member_casual', 'ride_id', 'rideable_type', 'rideable_type_duplicate_column_name_1', 'start station id', 'start station latitude', 'start station longitude', 'start station name', 'start_lat', 'start_lng', 'start_station_id', 'start_station_name', 'started_at', 'starttime', 'stoptime', 'tripduration', 'usertype']\n",
      " \n",
      "Unique column names after cleaning: \n",
      " ['bike_id', 'birth_year', 'end_datetime', 'end_station_id', 'end_station_latitude', 'end_station_longitude', 'end_station_name', 'gender', 'ride_id', 'rideable_type', 'start_datetime', 'start_station_id', 'start_station_latitude', 'start_station_longitude', 'start_station_name', 'trip_duration', 'user_type']\n"
     ]
    }
   ],
   "source": [
    "unique_column_names = list_unique_col_names(list_files)\n",
    "print('Unique column names: \\n  ' + str(unique_column_names))\n",
    "column_mapping = {\n",
    "    'bikeid': 'bike_id',\n",
    "    'end_lat': 'end_station_latitude',\n",
    "    'end_lng': 'end_station_longitude',\n",
    "    'ended_at': 'end_datetime',\n",
    "    'member_casual': 'user_type',\n",
    "    'rideable_type_duplicate_column_name_1': 'duplicate_col',\n",
    "    'start_lat': 'start_station_latitude',\n",
    "    'start_lng': 'start_station_longitude',\n",
    "    'starttime': 'start_datetime',\n",
    "    'start_time': 'start_datetime',\n",
    "    'started_at': 'start_datetime',\n",
    "    'stoptime': 'end_datetime',\n",
    "    'stop_time': 'end_datetime',\n",
    "    'tripduration': 'trip_duration',\n",
    "    'unnamed:_0': 'unnamed', # this is just an index column without name, present in some files -> can be discarded later\n",
    "    'usertype': 'user_type'\n",
    "}\n",
    "\n",
    "# Exploratory correction, see if it solves the inconsistencies\n",
    "unique_column_names=[]\n",
    "for csv_file in list_files:\n",
    "    file_path = os.path.join(extract_dir, csv_file)\n",
    "    df = pd.read_csv(file_path, nrows=1)\n",
    "#     if 'rideable_type_duplicate_column_name_1' in df.columns: # check what column this is -> just a duplicate -> can be discarded\n",
    "#         print(df.head(2))\n",
    "    df = clean_column_names(df, column_mapping)\n",
    "    ##print(df.columns)\n",
    "    \n",
    "    [unique_column_names.append(col) for col in df.columns if col not in unique_column_names] # save new col names for checking\n",
    "    unique_column_names.sort()\n",
    "\n",
    "remove_names = ['unnamed','duplicate_col'] # column names to remove\n",
    "final_column_names = [name for name in unique_column_names if name not in remove_names] # list with final universal column names\n",
    "print(' ')\n",
    "print('Unique column names after cleaning: \\n ' + str(final_column_names)) # -> satisfied!\n",
    "final_column_names.extend(['year','month']) # add the columns year and month, as I will add them from start_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "70143a60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: bike_id\n",
      "Unique dtypes: {dtype('int64')}\n",
      "\n",
      "Column: birth_year\n",
      "Unique dtypes: {dtype('O'), dtype('int64'), dtype('float64')}\n",
      "\n",
      "Column: end_datetime\n",
      "Unique dtypes: {dtype('O')}\n",
      "\n",
      "Column: end_station_id\n",
      "Unique dtypes: {dtype('O'), dtype('int64'), dtype('float64')}\n",
      "\n",
      "Column: end_station_latitude\n",
      "Unique dtypes: {dtype('float64')}\n",
      "\n",
      "Column: end_station_longitude\n",
      "Unique dtypes: {dtype('float64')}\n",
      "\n",
      "Column: end_station_name\n",
      "Unique dtypes: {dtype('O')}\n",
      "\n",
      "Column: gender\n",
      "Unique dtypes: {dtype('int64')}\n",
      "\n",
      "Column: ride_id\n",
      "Unique dtypes: {dtype('O')}\n",
      "\n",
      "Column: rideable_type\n",
      "Unique dtypes: {dtype('O')}\n",
      "\n",
      "Column: start_datetime\n",
      "Unique dtypes: {dtype('O')}\n",
      "\n",
      "Column: start_station_id\n",
      "Unique dtypes: {dtype('O'), dtype('int64'), dtype('float64')}\n",
      "\n",
      "Column: start_station_latitude\n",
      "Unique dtypes: {dtype('float64')}\n",
      "\n",
      "Column: start_station_longitude\n",
      "Unique dtypes: {dtype('float64')}\n",
      "\n",
      "Column: start_station_name\n",
      "Unique dtypes: {dtype('O')}\n",
      "\n",
      "Column: trip_duration\n",
      "Unique dtypes: {dtype('int64')}\n",
      "\n",
      "Column: user_type\n",
      "Unique dtypes: {dtype('O')}\n",
      "\n",
      "Column: year\n",
      "Unique dtypes: set()\n",
      "\n",
      "Column: month\n",
      "Unique dtypes: set()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check if data type of files are the same\n",
    "check_dtype = {col: [] for col in final_column_names} # create empty dict to store dtypes\n",
    "\n",
    "list_files = [f for f in os.listdir(extract_dir) if f.endswith('.csv')]\n",
    "for csv_file in list_files:\n",
    "#df.memory_usage(deep=True).sum()\n",
    "    file_path = os.path.join(extract_dir, csv_file)\n",
    "    df = pd.read_csv(file_path, nrows=3)\n",
    "    df = clean_column_names(df, column_mapping) # clean column names\n",
    "    to_remove = ['duplicate_col','unnamed'] \n",
    "    for col in to_remove:\n",
    "        if col in df.columns:\n",
    "            df.drop(col, axis=1, inplace=True) # drop these columns\n",
    "    for col in df.columns:\n",
    "        #print(f'{col}: {df[col].dtype}')\n",
    "        check_dtype[col].append(df[col].dtype)\n",
    "\n",
    "for col in check_dtype:\n",
    "    print(f\"Column: {col}\")\n",
    "    print(f\"Unique dtypes: {set(check_dtype[col])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3e8c43f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ride_id</th>\n",
       "      <th>rideable_type</th>\n",
       "      <th>start_datetime</th>\n",
       "      <th>end_datetime</th>\n",
       "      <th>start_station_name</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>end_station_name</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>start_station_latitude</th>\n",
       "      <th>start_station_longitude</th>\n",
       "      <th>end_station_latitude</th>\n",
       "      <th>end_station_longitude</th>\n",
       "      <th>user_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17AE31FCAE74D287</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2024-08-07 13:22:55.656</td>\n",
       "      <td>2024-08-07 13:25:09.654</td>\n",
       "      <td>7 St &amp; Monroe St</td>\n",
       "      <td>HB304</td>\n",
       "      <td>4 St &amp; Grand St</td>\n",
       "      <td>HB301</td>\n",
       "      <td>40.7464</td>\n",
       "      <td>-74.0380</td>\n",
       "      <td>40.7423</td>\n",
       "      <td>-74.0351</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FD9859BDBE0CDF70</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2024-08-13 13:15:08.627</td>\n",
       "      <td>2024-08-13 13:17:44.971</td>\n",
       "      <td>7 St &amp; Monroe St</td>\n",
       "      <td>HB304</td>\n",
       "      <td>4 St &amp; Grand St</td>\n",
       "      <td>HB301</td>\n",
       "      <td>40.7464</td>\n",
       "      <td>-74.0380</td>\n",
       "      <td>40.7423</td>\n",
       "      <td>-74.0351</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAC5ECD095AE5572</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2024-08-12 20:07:26.975</td>\n",
       "      <td>2024-08-12 20:09:38.180</td>\n",
       "      <td>7 St &amp; Monroe St</td>\n",
       "      <td>HB304</td>\n",
       "      <td>4 St &amp; Grand St</td>\n",
       "      <td>HB301</td>\n",
       "      <td>40.7464</td>\n",
       "      <td>-74.0380</td>\n",
       "      <td>40.7423</td>\n",
       "      <td>-74.0351</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ride_id  rideable_type           start_datetime  \\\n",
       "0  17AE31FCAE74D287  electric_bike  2024-08-07 13:22:55.656   \n",
       "1  FD9859BDBE0CDF70  electric_bike  2024-08-13 13:15:08.627   \n",
       "2  AAC5ECD095AE5572  electric_bike  2024-08-12 20:07:26.975   \n",
       "\n",
       "              end_datetime start_station_name start_station_id  \\\n",
       "0  2024-08-07 13:25:09.654   7 St & Monroe St            HB304   \n",
       "1  2024-08-13 13:17:44.971   7 St & Monroe St            HB304   \n",
       "2  2024-08-12 20:09:38.180   7 St & Monroe St            HB304   \n",
       "\n",
       "  end_station_name end_station_id  start_station_latitude  \\\n",
       "0  4 St & Grand St          HB301                 40.7464   \n",
       "1  4 St & Grand St          HB301                 40.7464   \n",
       "2  4 St & Grand St          HB301                 40.7464   \n",
       "\n",
       "   start_station_longitude  end_station_latitude  end_station_longitude  \\\n",
       "0                 -74.0380               40.7423               -74.0351   \n",
       "1                 -74.0380               40.7423               -74.0351   \n",
       "2                 -74.0380               40.7423               -74.0351   \n",
       "\n",
       "  user_type  \n",
       "0    member  \n",
       "1    member  \n",
       "2    member  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5b5597c0-6c2a-4563-aa1f-7cd2864331fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hanna\\AppData\\Local\\Temp\\ipykernel_31052\\3511905405.py:12: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\Hanna\\AppData\\Local\\Temp\\ipykernel_31052\\3511905405.py:12: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\Hanna\\AppData\\Local\\Temp\\ipykernel_31052\\3511905405.py:12: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\Hanna\\AppData\\Local\\Temp\\ipykernel_31052\\3511905405.py:12: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\Hanna\\AppData\\Local\\Temp\\ipykernel_31052\\3511905405.py:12: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\Hanna\\AppData\\Local\\Temp\\ipykernel_31052\\3511905405.py:12: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\Hanna\\AppData\\Local\\Temp\\ipykernel_31052\\3511905405.py:12: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\Hanna\\AppData\\Local\\Temp\\ipykernel_31052\\3511905405.py:12: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\Hanna\\AppData\\Local\\Temp\\ipykernel_31052\\3511905405.py:12: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\Hanna\\AppData\\Local\\Temp\\ipykernel_31052\\3511905405.py:12: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\Hanna\\AppData\\Local\\Temp\\ipykernel_31052\\3511905405.py:12: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\Hanna\\AppData\\Local\\Temp\\ipykernel_31052\\3511905405.py:12: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\Hanna\\AppData\\Local\\Temp\\ipykernel_31052\\3511905405.py:12: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\Hanna\\AppData\\Local\\Temp\\ipykernel_31052\\3511905405.py:12: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\Hanna\\AppData\\Local\\Temp\\ipykernel_31052\\3511905405.py:12: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\Hanna\\AppData\\Local\\Temp\\ipykernel_31052\\3511905405.py:12: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\Hanna\\AppData\\Local\\Temp\\ipykernel_31052\\3511905405.py:12: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\Hanna\\AppData\\Local\\Temp\\ipykernel_31052\\3511905405.py:12: DtypeWarning: Columns (8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\Hanna\\AppData\\Local\\Temp\\ipykernel_31052\\3511905405.py:12: DtypeWarning: Columns (6,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\Hanna\\AppData\\Local\\Temp\\ipykernel_31052\\3511905405.py:12: DtypeWarning: Columns (6,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\Hanna\\AppData\\Local\\Temp\\ipykernel_31052\\3511905405.py:12: DtypeWarning: Columns (6,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\Hanna\\AppData\\Local\\Temp\\ipykernel_31052\\3511905405.py:12: DtypeWarning: Columns (6,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\Hanna\\AppData\\Local\\Temp\\ipykernel_31052\\3511905405.py:12: DtypeWarning: Columns (6,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n",
      "C:\\Users\\Hanna\\AppData\\Local\\Temp\\ipykernel_31052\\3511905405.py:12: DtypeWarning: Columns (6,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'gender': {0, 1, 2},\n",
       " 'user_type': {'Customer', 'Subscriber', 'casual', 'member'},\n",
       " 'rideable_type': {'classic_bike', 'docked_bike', 'electric_bike'}}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get universal entries for columns I am converting to str and then to category (global categories across csv files)\n",
    "dtype_cat =['gender', 'user_type', 'rideable_type']\n",
    "\n",
    "# get global categories (differing per file)\n",
    "categories_dict = {col: set() for col in dtype_cat }\n",
    "\n",
    "# Collect all unique categories across the DataFrames\n",
    "for csv_file in list_files:  \n",
    "    file_path = os.path.join(extract_dir, csv_file) # load individual file\n",
    "    \n",
    "    # Process the file in chunks to save memory\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = clean_column_names(df, column_mapping) # clean column names\n",
    "    to_remove = ['duplicate_col','unnamed'] \n",
    "    for col in to_remove:\n",
    "        if col in df.columns:\n",
    "            df.drop(col, axis=1, inplace=True) # drop these columns\n",
    "        \n",
    "    for col in df.columns:\n",
    "        if col in categories_dict:\n",
    "            df[col].fillna('unknown').dropna()   \n",
    "            categories_dict[col].update(df[col].unique())  # update unique categories of df\n",
    "            #categories_dict[col].add('unknown') # add category 'unknown' for missing data\n",
    "categories_dict['user_type'].discard(np.nan) # for some reason still nan as category here\n",
    "categories_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dae536",
   "metadata": {},
   "source": [
    "## Load csv files, clean column names, change dtypes, concatenate into 1 dask df and save as dask parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "47bfae2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading 201306-citibike-tripdata.csv\n",
      ".... cleaned, converted to dask df and appended to ddf_list\n",
      "loading 201306-citibike-tripdata_1.csv\n",
      ".... cleaned, converted to dask df and appended to ddf_list\n",
      "bike_id\n",
      "[dtype('int64')]\n",
      "birth_year\n",
      "[dtype('int32')]\n",
      "end_datetime\n",
      "[dtype('<M8[ns]')]\n",
      "end_station_id\n",
      "[dtype('O')]\n",
      "end_station_latitude\n",
      "[dtype('float32')]\n",
      "end_station_longitude\n",
      "[dtype('float32')]\n",
      "end_station_name\n",
      "[dtype('O')]\n",
      "gender\n",
      "[CategoricalDtype(categories=[0, 1, 2], ordered=False)]\n",
      "ride_id\n",
      "[dtype('float64')]\n",
      "rideable_type\n",
      "[dtype('float64')]\n",
      "start_datetime\n",
      "[dtype('<M8[ns]')]\n",
      "start_station_id\n",
      "[dtype('O')]\n",
      "start_station_latitude\n",
      "[dtype('float32')]\n",
      "start_station_longitude\n",
      "[dtype('float32')]\n",
      "start_station_name\n",
      "[dtype('O')]\n",
      "trip_duration\n",
      "[dtype('int32')]\n",
      "user_type\n",
      "[CategoricalDtype(categories=['Subscriber', 'member', 'casual', 'Customer'], ordered=False)]\n",
      "year\n",
      "[dtype('int32')]\n",
      "month\n",
      "[dtype('int8')]\n",
      "All files processed and saved to Parquet\n"
     ]
    }
   ],
   "source": [
    "### This cell executed in the Anaconda powershell (clean_concat.py), since it´s faster/requires less memory on my 16GB mem laptop ###\n",
    "\n",
    "if not os.path.exists(cleaned_dir):  # directory to save the cleaned df\n",
    "    os.makedirs(cleaned_dir)\n",
    "\n",
    "dtype_dict = { # I also convert latitude and longitude to float32 here since it greatly improves efficiency- float32 can hold only 7 decimals, but this should be enough (accurate to ~10m)\n",
    "    'int32': ['birth_year', 'trip_duration'],\n",
    "    'float32': ['end_station_latitude', 'end_station_longitude', 'start_station_latitude', 'start_station_longitude'],\n",
    "    'str': ['start_station_id', 'start_station_name', 'end_station_id', 'end_station_name', 'ride_id'], \n",
    "    'category': ['gender', 'user_type', 'rideable_type'],\n",
    "    'datetime64': ['start_datetime', 'end_datetime']\n",
    "}\n",
    "\n",
    "dtype_mapping = {} # dictionary with col: dtype, for changing data types per column\n",
    "for dtype, columns in dtype_dict.items():\n",
    "    for col in columns:\n",
    "        dtype_mapping[col] = dtype\n",
    "\n",
    "list_files = [f for f in os.listdir(extract_dir) if f.endswith('.csv')] # list of csv files in dir to loop over\n",
    "chunksize = 1_000_000  # load in chunks to save memory, in case csv file is huge\n",
    "\n",
    "ddf_list = []\n",
    "for csv_file in list_files[0:2]: \n",
    "    file_path = os.path.join(extract_dir, csv_file) # load individual file\n",
    "    print('loading ' +  csv_file)\n",
    "    \n",
    "    # Process the file in chunks to save memory\n",
    "    chunk_iter = pd.read_csv(file_path, chunksize=chunksize, low_memory=True, parse_dates=True)\n",
    "\n",
    "    for n, chunk in enumerate(chunk_iter):\n",
    "        ddf = clean_column_names(chunk, column_mapping)  # Clean column names\n",
    " \n",
    "        # Convert to Dask DataFrame for larger datasets\n",
    "        #ddf = dd.from_pandas(chunk, npartitions=1) #\n",
    "        #print('to dask converted')\n",
    "        # Drop unwanted columns\n",
    "        to_remove = ['duplicate_col', 'unnamed']\n",
    "        ddf = ddf.drop(columns=[col for col in to_remove if col in ddf.columns])\n",
    "        \n",
    "        missing_cols = set(final_column_names) - set(ddf.columns) # add missing (universal) columns from final_column_names and fill with nans\n",
    "        for col in missing_cols:\n",
    "            ddf[col] = np.nan\n",
    "                    \n",
    "        # Convert column dtypes\n",
    "        for col in ddf.columns:\n",
    "            if col in dtype_dict['int32']:\n",
    "                ddf[col] = ddf[col].replace('\\\\N', np.nan)  # handle missing values\n",
    "                ddf[col] = ddf[col].astype('float32')\n",
    "                ddf[col] = ddf[col].fillna(0).round(0).astype('int32') # replace nan with the place filler 0, round and convert to int\n",
    "            elif col in dtype_dict['category']: # for categorical data, replace nans with 'unknown' cat\n",
    "                ddf[col] = ddf[col].fillna('unknown')  # Replace NaNs with 'unknown'\n",
    "                ddf[col] = ddf[col].astype('str') # convert to str first\n",
    "                ddf[col] = ddf[col].astype('category') # string to category, as it needs less memory\n",
    "                ddf[col] = ddf[col].cat.set_categories(new_categories=list(categories_dict[col])) # set global categories\n",
    "            elif col in dtype_mapping.keys():\n",
    "                ddf[col] = ddf[col].astype(dtype_mapping[col])\n",
    "                \n",
    "        ddf = ddf.drop_duplicates().sort_values(by='start_datetime')# drop duplicates and sort to start rental time/date\n",
    "        ddf = ddf.reset_index(drop=True)\n",
    "        #ddf = ddf.set_index('start_datetime') # set start_datetime as index\n",
    "\n",
    "        ddf['year'] = ddf['start_datetime'].dt.year.astype('int32') # add year column for partitioning\n",
    "        ddf['month'] = ddf['start_datetime'].dt.month.astype('int8') # add month column for partitioning\n",
    "                                            \n",
    "        ddf = ddf[final_column_names] # ensure consistent column order   \n",
    "\n",
    "        ddf = dd.from_pandas(ddf, npartitions=5)\n",
    "        ddf_list.append(ddf)   \n",
    "    print('.... cleaned, converted to dask df and appended to ddf_list')     \n",
    "\n",
    "ddf_comb = dd.concat(ddf_list, ignore_index=True) # concatenate all dask dfs\n",
    "#del ddf_list\n",
    "ddf_comb = ddf_comb.drop_duplicates() # remove duplicates\n",
    "\n",
    "# check if all dtypes are consistent across ddfs\n",
    "check_dtype = {col: [] for col in final_column_names}\n",
    "for ddf in ddf_list:\n",
    "    for col in ddf.columns:\n",
    "        check_dtype[col].append(ddf[col].dtype)\n",
    "\n",
    "for col in check_dtype.keys():\n",
    "    print(col)\n",
    "    print(pd.Series(check_dtype[col]).unique())\n",
    "    if len(pd.Series(check_dtype[col]).unique()) > 1:\n",
    "        print(col + ' has inconsistent dtypes')\n",
    "    \n",
    "ddf_comb.to_parquet(cleaned_dir + '/combined_dask_df.parquet', engine='pyarrow', partition_on=['year', 'month'], write_index=False) # write, partitioned on year and month\n",
    "print('All files processed and saved to Parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e07be4-2642-41f6-bb3e-5d1d352fd6f9",
   "metadata": {},
   "source": [
    "## Load cleaned ddf (data bike rides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2afc839e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bike_id</th>\n",
       "      <th>birth_year</th>\n",
       "      <th>end_datetime</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>end_station_latitude</th>\n",
       "      <th>end_station_longitude</th>\n",
       "      <th>end_station_name</th>\n",
       "      <th>gender</th>\n",
       "      <th>ride_id</th>\n",
       "      <th>rideable_type</th>\n",
       "      <th>start_datetime</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>start_station_latitude</th>\n",
       "      <th>start_station_longitude</th>\n",
       "      <th>start_station_name</th>\n",
       "      <th>trip_duration</th>\n",
       "      <th>user_type</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15839.0000</td>\n",
       "      <td>1971</td>\n",
       "      <td>2019-01-01 00:07:07.581</td>\n",
       "      <td>3283.0</td>\n",
       "      <td>40.7882</td>\n",
       "      <td>-73.9704</td>\n",
       "      <td>W 89 St &amp; Columbus Ave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-01-01 00:01:47.401</td>\n",
       "      <td>3160.0</td>\n",
       "      <td>40.7790</td>\n",
       "      <td>-73.9737</td>\n",
       "      <td>Central Park West &amp; W 76 St</td>\n",
       "      <td>320</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32723.0000</td>\n",
       "      <td>1964</td>\n",
       "      <td>2019-01-01 00:10:00.608</td>\n",
       "      <td>518.0</td>\n",
       "      <td>40.7478</td>\n",
       "      <td>-73.9734</td>\n",
       "      <td>E 39 St &amp; 2 Ave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-01-01 00:04:43.736</td>\n",
       "      <td>519.0</td>\n",
       "      <td>40.7519</td>\n",
       "      <td>-73.9777</td>\n",
       "      <td>Pershing Square North</td>\n",
       "      <td>316</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27451.0000</td>\n",
       "      <td>1987</td>\n",
       "      <td>2019-01-01 00:15:55.438</td>\n",
       "      <td>3154.0</td>\n",
       "      <td>40.7731</td>\n",
       "      <td>-73.9586</td>\n",
       "      <td>E 77 St &amp; 3 Ave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-01-01 00:06:03.997</td>\n",
       "      <td>3171.0</td>\n",
       "      <td>40.7852</td>\n",
       "      <td>-73.9767</td>\n",
       "      <td>Amsterdam Ave &amp; W 82 St</td>\n",
       "      <td>591</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21579.0000</td>\n",
       "      <td>1990</td>\n",
       "      <td>2019-01-01 00:52:22.650</td>\n",
       "      <td>3709.0</td>\n",
       "      <td>40.7380</td>\n",
       "      <td>-73.9964</td>\n",
       "      <td>W 15 St &amp; 6 Ave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-01-01 00:07:03.545</td>\n",
       "      <td>504.0</td>\n",
       "      <td>40.7322</td>\n",
       "      <td>-73.9817</td>\n",
       "      <td>1 Ave &amp; E 16 St</td>\n",
       "      <td>2719</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35379.0000</td>\n",
       "      <td>1979</td>\n",
       "      <td>2019-01-01 00:12:39.502</td>\n",
       "      <td>503.0</td>\n",
       "      <td>40.7383</td>\n",
       "      <td>-73.9875</td>\n",
       "      <td>E 20 St &amp; Park Ave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-01-01 00:07:35.945</td>\n",
       "      <td>229.0</td>\n",
       "      <td>40.7274</td>\n",
       "      <td>-73.9938</td>\n",
       "      <td>Great Jones St</td>\n",
       "      <td>303</td>\n",
       "      <td>Subscriber</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     bike_id  birth_year            end_datetime end_station_id  \\\n",
       "0 15839.0000        1971 2019-01-01 00:07:07.581         3283.0   \n",
       "1 32723.0000        1964 2019-01-01 00:10:00.608          518.0   \n",
       "2 27451.0000        1987 2019-01-01 00:15:55.438         3154.0   \n",
       "3 21579.0000        1990 2019-01-01 00:52:22.650         3709.0   \n",
       "4 35379.0000        1979 2019-01-01 00:12:39.502          503.0   \n",
       "\n",
       "   end_station_latitude  end_station_longitude        end_station_name gender  \\\n",
       "0               40.7882               -73.9704  W 89 St & Columbus Ave    NaN   \n",
       "1               40.7478               -73.9734         E 39 St & 2 Ave    NaN   \n",
       "2               40.7731               -73.9586         E 77 St & 3 Ave    NaN   \n",
       "3               40.7380               -73.9964         W 15 St & 6 Ave    NaN   \n",
       "4               40.7383               -73.9875      E 20 St & Park Ave    NaN   \n",
       "\n",
       "  ride_id rideable_type          start_datetime start_station_id  \\\n",
       "0     nan           NaN 2019-01-01 00:01:47.401           3160.0   \n",
       "1     nan           NaN 2019-01-01 00:04:43.736            519.0   \n",
       "2     nan           NaN 2019-01-01 00:06:03.997           3171.0   \n",
       "3     nan           NaN 2019-01-01 00:07:03.545            504.0   \n",
       "4     nan           NaN 2019-01-01 00:07:35.945            229.0   \n",
       "\n",
       "   start_station_latitude  start_station_longitude  \\\n",
       "0                 40.7790                 -73.9737   \n",
       "1                 40.7519                 -73.9777   \n",
       "2                 40.7852                 -73.9767   \n",
       "3                 40.7322                 -73.9817   \n",
       "4                 40.7274                 -73.9938   \n",
       "\n",
       "            start_station_name  trip_duration   user_type  year month  \n",
       "0  Central Park West & W 76 St            320  Subscriber  2019     1  \n",
       "1        Pershing Square North            316  Subscriber  2019     1  \n",
       "2      Amsterdam Ave & W 82 St            591  Subscriber  2019     1  \n",
       "3              1 Ave & E 16 St           2719  Subscriber  2019     1  \n",
       "4               Great Jones St            303  Subscriber  2019     1  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load cleaned data from saved file\n",
    "ddf = dd.read_parquet(cleaned_dir + '/combined_dask_df.parquet')\n",
    "#ddf = dd.read_parquet('path_to_parquet_file', columns=['category_column', 'numeric_column']) # read only specific columns\n",
    "#ddf_2020 = dd.read_parquet('path_to_parquet_file/year=2020') # read only specific partition\n",
    "\n",
    "ddf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ccbd18",
   "metadata": {},
   "source": [
    "Now that all column names are consistent and all date is concatenated, check if categories in some columns are consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea2f9da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique categories:\n",
      "user_type - Index(['Subscriber', 'Customer', 'member', 'casual'], dtype='object')\n",
      "rideable_type - Index(['electric_bike', 'classic_bike', 'docked_bike'], dtype='object')\n",
      "Unique string:\n",
      "start_station id, unique: 4199\n",
      "start_station name, unique: 2424\n",
      "end_station_id, unique: 4252\n",
      "end_station name, unique: 2445\n"
     ]
    }
   ],
   "source": [
    "print('Unique categories:')\n",
    "print(f'user_type - {ddf[\"user_type\"].cat.as_known().cat.categories}')\n",
    "print(f'rideable_type - {ddf[\"rideable_type\"].cat.as_known().cat.categories}')\n",
    "\n",
    "print('Unique string:')\n",
    "print('start_station id, unique: '+str(ddf.start_station_id.nunique().compute()))\n",
    "print('start_station name, unique: '+str(ddf.start_station_name.nunique().compute()))\n",
    "print('end_station_id, unique: '+str(ddf.end_station_id.nunique().compute()))\n",
    "print('end_station name, unique: '+str(ddf.end_station_name.nunique().compute()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "287d6885-941c-46aa-832b-0afc6e16cab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bike_id                           float64\n",
      "birth_year                          int32\n",
      "end_datetime               datetime64[ns]\n",
      "end_station_id                     object\n",
      "end_station_latitude              float32\n",
      "end_station_longitude             float32\n",
      "end_station_name                   object\n",
      "gender                           category\n",
      "ride_id                            object\n",
      "rideable_type                    category\n",
      "start_datetime             datetime64[ns]\n",
      "start_station_id                   object\n",
      "start_station_latitude            float32\n",
      "start_station_longitude           float32\n",
      "start_station_name                 object\n",
      "trip_duration                       int32\n",
      "user_type                        category\n",
      "year                             category\n",
      "month                            category\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(ddf.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4e8bd4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique categories, after changing \"subscriber\" to \"member\" and \"customer\" to \"casual\", for consistency:\n",
      "user_type - Index(['member', 'casual'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bike_id</th>\n",
       "      <th>birth_year</th>\n",
       "      <th>end_datetime</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>end_station_latitude</th>\n",
       "      <th>end_station_longitude</th>\n",
       "      <th>end_station_name</th>\n",
       "      <th>gender</th>\n",
       "      <th>ride_id</th>\n",
       "      <th>rideable_type</th>\n",
       "      <th>start_datetime</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>start_station_latitude</th>\n",
       "      <th>start_station_longitude</th>\n",
       "      <th>start_station_name</th>\n",
       "      <th>trip_duration</th>\n",
       "      <th>user_type</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15839.0000</td>\n",
       "      <td>1971</td>\n",
       "      <td>2019-01-01 00:07:07.581</td>\n",
       "      <td>3283.0</td>\n",
       "      <td>40.7882</td>\n",
       "      <td>-73.9704</td>\n",
       "      <td>W 89 St &amp; Columbus Ave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-01-01 00:01:47.401</td>\n",
       "      <td>3160.0</td>\n",
       "      <td>40.7790</td>\n",
       "      <td>-73.9737</td>\n",
       "      <td>Central Park West &amp; W 76 St</td>\n",
       "      <td>320</td>\n",
       "      <td>member</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32723.0000</td>\n",
       "      <td>1964</td>\n",
       "      <td>2019-01-01 00:10:00.608</td>\n",
       "      <td>518.0</td>\n",
       "      <td>40.7478</td>\n",
       "      <td>-73.9734</td>\n",
       "      <td>E 39 St &amp; 2 Ave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-01-01 00:04:43.736</td>\n",
       "      <td>519.0</td>\n",
       "      <td>40.7519</td>\n",
       "      <td>-73.9777</td>\n",
       "      <td>Pershing Square North</td>\n",
       "      <td>316</td>\n",
       "      <td>member</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     bike_id  birth_year            end_datetime end_station_id  \\\n",
       "0 15839.0000        1971 2019-01-01 00:07:07.581         3283.0   \n",
       "1 32723.0000        1964 2019-01-01 00:10:00.608          518.0   \n",
       "\n",
       "   end_station_latitude  end_station_longitude        end_station_name gender  \\\n",
       "0               40.7882               -73.9704  W 89 St & Columbus Ave    NaN   \n",
       "1               40.7478               -73.9734         E 39 St & 2 Ave    NaN   \n",
       "\n",
       "  ride_id rideable_type          start_datetime start_station_id  \\\n",
       "0     nan           NaN 2019-01-01 00:01:47.401           3160.0   \n",
       "1     nan           NaN 2019-01-01 00:04:43.736            519.0   \n",
       "\n",
       "   start_station_latitude  start_station_longitude  \\\n",
       "0                 40.7790                 -73.9737   \n",
       "1                 40.7519                 -73.9777   \n",
       "\n",
       "            start_station_name  trip_duration user_type  year month  \n",
       "0  Central Park West & W 76 St            320    member  2019     1  \n",
       "1        Pershing Square North            316    member  2019     1  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unify string in rideable_type and user_type columns\n",
    "\n",
    "#ddf['rideable_type'] = ddf['rideable_type'].astype('str').str.strip().str.lower().astype('category')# if needed\n",
    "\n",
    "def replace_user_type(df):\n",
    "    df = df.copy()  # make a copy to avoid SettingWithCopyWarning\n",
    "    df['user_type'] = df['user_type'].astype(str).str.strip().str.lower() # temporarily convert to string (object)\n",
    "    \n",
    "    # Replace 'subscriber' with 'member' and 'customer' with 'casual'\n",
    "    df.loc[df['user_type'] == 'subscriber', 'user_type'] = 'member'\n",
    "    df.loc[df['user_type'] == 'customer', 'user_type'] = 'casual'\n",
    "\n",
    "    df['user_type'] = pd.Categorical(df['user_type'], categories=['member', 'casual']) # convert back to category\n",
    "\n",
    "    return df\n",
    "\n",
    "# Use map_partitions to apply this function to the Dask DataFrame\n",
    "ddf = ddf.map_partitions(replace_user_type)\n",
    "\n",
    "print('Unique categories, after changing \"subscriber\" to \"member\" and \"customer\" to \"casual\", for consistency:')\n",
    "print(f'user_type - {ddf[\"user_type\"].cat.as_known().cat.categories}')\n",
    "ddf.head(2) # check if it worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a6b82eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bike_id                    31328486\n",
       "birth_year                        0\n",
       "end_datetime                      0\n",
       "end_station_id                    0\n",
       "end_station_latitude          12755\n",
       "end_station_longitude         12755\n",
       "end_station_name                  0\n",
       "gender                     52620105\n",
       "ride_id                           0\n",
       "rideable_type              21291619\n",
       "start_datetime                    0\n",
       "start_station_id                  0\n",
       "start_station_latitude            0\n",
       "start_station_longitude           0\n",
       "start_station_name                0\n",
       "trip_duration                     0\n",
       "user_type                         0\n",
       "year                              0\n",
       "month                             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf.isna().sum().compute() # check how many nans in which columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6ce0e356-438f-42f8-ad15-eb6eac93ec7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bike_id</th>\n",
       "      <th>birth_year</th>\n",
       "      <th>end_datetime</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>end_station_latitude</th>\n",
       "      <th>end_station_longitude</th>\n",
       "      <th>end_station_name</th>\n",
       "      <th>gender</th>\n",
       "      <th>ride_id</th>\n",
       "      <th>rideable_type</th>\n",
       "      <th>start_datetime</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>start_station_latitude</th>\n",
       "      <th>start_station_longitude</th>\n",
       "      <th>start_station_name</th>\n",
       "      <th>trip_duration</th>\n",
       "      <th>user_type</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-02-07 08:38:56</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9A0FF842D38924E8</td>\n",
       "      <td>docked_bike</td>\n",
       "      <td>2021-02-06 09:44:17</td>\n",
       "      <td>JC082</td>\n",
       "      <td>40.7216</td>\n",
       "      <td>-74.0429</td>\n",
       "      <td>Manila &amp; 1st</td>\n",
       "      <td>0</td>\n",
       "      <td>casual</td>\n",
       "      <td>2021</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-02-08 16:44:41</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DF695B318E00BB00</td>\n",
       "      <td>docked_bike</td>\n",
       "      <td>2021-02-07 15:44:49</td>\n",
       "      <td>JC084</td>\n",
       "      <td>40.7144</td>\n",
       "      <td>-74.0666</td>\n",
       "      <td>Communipaw &amp; Berry Lane</td>\n",
       "      <td>0</td>\n",
       "      <td>member</td>\n",
       "      <td>2021</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-02-08 13:03:06</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>E7D301B8767D1015</td>\n",
       "      <td>docked_bike</td>\n",
       "      <td>2021-02-08 07:51:36</td>\n",
       "      <td>JC011</td>\n",
       "      <td>40.7165</td>\n",
       "      <td>-74.0496</td>\n",
       "      <td>JC Medical Center</td>\n",
       "      <td>0</td>\n",
       "      <td>member</td>\n",
       "      <td>2021</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-02-08 08:12:54</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2920267F4E3DDFFC</td>\n",
       "      <td>docked_bike</td>\n",
       "      <td>2021-02-08 07:58:42</td>\n",
       "      <td>JC011</td>\n",
       "      <td>40.7165</td>\n",
       "      <td>-74.0496</td>\n",
       "      <td>JC Medical Center</td>\n",
       "      <td>0</td>\n",
       "      <td>member</td>\n",
       "      <td>2021</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>552</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-02-08 13:03:06</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AD2EA0ED7DA9B9DB</td>\n",
       "      <td>docked_bike</td>\n",
       "      <td>2021-02-08 08:53:53</td>\n",
       "      <td>JC027</td>\n",
       "      <td>40.7253</td>\n",
       "      <td>-74.0456</td>\n",
       "      <td>Jersey &amp; 6th St</td>\n",
       "      <td>0</td>\n",
       "      <td>member</td>\n",
       "      <td>2021</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     bike_id  birth_year        end_datetime end_station_id  \\\n",
       "313      NaN           0 2021-02-07 08:38:56            nan   \n",
       "526      NaN           0 2021-02-08 16:44:41            nan   \n",
       "546      NaN           0 2021-02-08 13:03:06            nan   \n",
       "548      NaN           0 2021-02-08 08:12:54            nan   \n",
       "552      NaN           0 2021-02-08 13:03:06            nan   \n",
       "\n",
       "     end_station_latitude  end_station_longitude end_station_name gender  \\\n",
       "313                   NaN                    NaN              nan    NaN   \n",
       "526                   NaN                    NaN              nan    NaN   \n",
       "546                   NaN                    NaN              nan    NaN   \n",
       "548                   NaN                    NaN              nan    NaN   \n",
       "552                   NaN                    NaN              nan    NaN   \n",
       "\n",
       "              ride_id rideable_type      start_datetime start_station_id  \\\n",
       "313  9A0FF842D38924E8   docked_bike 2021-02-06 09:44:17            JC082   \n",
       "526  DF695B318E00BB00   docked_bike 2021-02-07 15:44:49            JC084   \n",
       "546  E7D301B8767D1015   docked_bike 2021-02-08 07:51:36            JC011   \n",
       "548  2920267F4E3DDFFC   docked_bike 2021-02-08 07:58:42            JC011   \n",
       "552  AD2EA0ED7DA9B9DB   docked_bike 2021-02-08 08:53:53            JC027   \n",
       "\n",
       "     start_station_latitude  start_station_longitude       start_station_name  \\\n",
       "313                 40.7216                 -74.0429             Manila & 1st   \n",
       "526                 40.7144                 -74.0666  Communipaw & Berry Lane   \n",
       "546                 40.7165                 -74.0496        JC Medical Center   \n",
       "548                 40.7165                 -74.0496        JC Medical Center   \n",
       "552                 40.7253                 -74.0456          Jersey & 6th St   \n",
       "\n",
       "     trip_duration user_type  year month  \n",
       "313              0    casual  2021     2  \n",
       "526              0    member  2021     2  \n",
       "546              0    member  2021     2  \n",
       "548              0    member  2021     2  \n",
       "552              0    member  2021     2  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf[ddf['end_station_latitude'].isna()].compute().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40691221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean trip duration: \n",
      "           trip_duration\n",
      "user_type               \n",
      "member          335.7891\n",
      "casual          674.6633\n"
     ]
    }
   ],
   "source": [
    "# Get some idea of the df content\n",
    "\n",
    "print('Mean trip duration: ')\n",
    "print(ddf.groupby('user_type').agg({'trip_duration': 'mean'}).compute())\n",
    "ddf.groupby('gender').agg({'trip_duration': 'mean'}).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0930bbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save further cleaned ddf again, overwriting\n",
    "ddf_comb.to_parquet(cleaned_dir + '/combined_dask_df.parquet', engine='pyarrow', partition_on=['year', 'month'], write_index=False, overwrite=True) # write, partitioned on year and month"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41850b0f",
   "metadata": {},
   "source": [
    "## Load collision data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a44cc3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hanna\\AppData\\Local\\Temp\\ipykernel_48936\\4130972820.py:2: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2120518, 29)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRASH DATE</th>\n",
       "      <th>CRASH TIME</th>\n",
       "      <th>BOROUGH</th>\n",
       "      <th>ZIP CODE</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>LOCATION</th>\n",
       "      <th>ON STREET NAME</th>\n",
       "      <th>CROSS STREET NAME</th>\n",
       "      <th>OFF STREET NAME</th>\n",
       "      <th>...</th>\n",
       "      <th>CONTRIBUTING FACTOR VEHICLE 2</th>\n",
       "      <th>CONTRIBUTING FACTOR VEHICLE 3</th>\n",
       "      <th>CONTRIBUTING FACTOR VEHICLE 4</th>\n",
       "      <th>CONTRIBUTING FACTOR VEHICLE 5</th>\n",
       "      <th>COLLISION_ID</th>\n",
       "      <th>VEHICLE TYPE CODE 1</th>\n",
       "      <th>VEHICLE TYPE CODE 2</th>\n",
       "      <th>VEHICLE TYPE CODE 3</th>\n",
       "      <th>VEHICLE TYPE CODE 4</th>\n",
       "      <th>VEHICLE TYPE CODE 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>09/11/2021</td>\n",
       "      <td>2:39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WHITESTONE EXPRESSWAY</td>\n",
       "      <td>20 AVENUE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4455765</td>\n",
       "      <td>Sedan</td>\n",
       "      <td>Sedan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>03/26/2022</td>\n",
       "      <td>11:45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>QUEENSBORO BRIDGE UPPER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4513547</td>\n",
       "      <td>Sedan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>06/29/2022</td>\n",
       "      <td>6:55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>THROGS NECK BRIDGE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4541903</td>\n",
       "      <td>Sedan</td>\n",
       "      <td>Pick-up Truck</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>09/11/2021</td>\n",
       "      <td>9:35</td>\n",
       "      <td>BROOKLYN</td>\n",
       "      <td>11208.00</td>\n",
       "      <td>40.67</td>\n",
       "      <td>-73.87</td>\n",
       "      <td>(40.667202, -73.8665)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1211      LORING AVENUE</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4456314</td>\n",
       "      <td>Sedan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12/14/2021</td>\n",
       "      <td>8:13</td>\n",
       "      <td>BROOKLYN</td>\n",
       "      <td>11233.00</td>\n",
       "      <td>40.68</td>\n",
       "      <td>-73.92</td>\n",
       "      <td>(40.683304, -73.917274)</td>\n",
       "      <td>SARATOGA AVENUE</td>\n",
       "      <td>DECATUR STREET</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4486609</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   CRASH DATE CRASH TIME   BOROUGH ZIP CODE  LATITUDE  LONGITUDE  \\\n",
       "0  09/11/2021       2:39       NaN      NaN       NaN        NaN   \n",
       "1  03/26/2022      11:45       NaN      NaN       NaN        NaN   \n",
       "2  06/29/2022       6:55       NaN      NaN       NaN        NaN   \n",
       "3  09/11/2021       9:35  BROOKLYN 11208.00     40.67     -73.87   \n",
       "4  12/14/2021       8:13  BROOKLYN 11233.00     40.68     -73.92   \n",
       "\n",
       "                  LOCATION           ON STREET NAME CROSS STREET NAME  \\\n",
       "0                      NaN    WHITESTONE EXPRESSWAY         20 AVENUE   \n",
       "1                      NaN  QUEENSBORO BRIDGE UPPER               NaN   \n",
       "2                      NaN       THROGS NECK BRIDGE               NaN   \n",
       "3    (40.667202, -73.8665)                      NaN               NaN   \n",
       "4  (40.683304, -73.917274)          SARATOGA AVENUE    DECATUR STREET   \n",
       "\n",
       "           OFF STREET NAME  ...  CONTRIBUTING FACTOR VEHICLE 2  \\\n",
       "0                      NaN  ...                    Unspecified   \n",
       "1                      NaN  ...                            NaN   \n",
       "2                      NaN  ...                    Unspecified   \n",
       "3  1211      LORING AVENUE  ...                            NaN   \n",
       "4                      NaN  ...                            NaN   \n",
       "\n",
       "   CONTRIBUTING FACTOR VEHICLE 3  CONTRIBUTING FACTOR VEHICLE 4  \\\n",
       "0                            NaN                            NaN   \n",
       "1                            NaN                            NaN   \n",
       "2                            NaN                            NaN   \n",
       "3                            NaN                            NaN   \n",
       "4                            NaN                            NaN   \n",
       "\n",
       "   CONTRIBUTING FACTOR VEHICLE 5  COLLISION_ID  VEHICLE TYPE CODE 1  \\\n",
       "0                            NaN       4455765                Sedan   \n",
       "1                            NaN       4513547                Sedan   \n",
       "2                            NaN       4541903                Sedan   \n",
       "3                            NaN       4456314                Sedan   \n",
       "4                            NaN       4486609                  NaN   \n",
       "\n",
       "   VEHICLE TYPE CODE 2  VEHICLE TYPE CODE 3 VEHICLE TYPE CODE 4  \\\n",
       "0                Sedan                  NaN                 NaN   \n",
       "1                  NaN                  NaN                 NaN   \n",
       "2        Pick-up Truck                  NaN                 NaN   \n",
       "3                  NaN                  NaN                 NaN   \n",
       "4                  NaN                  NaN                 NaN   \n",
       "\n",
       "  VEHICLE TYPE CODE 5  \n",
       "0                 NaN  \n",
       "1                 NaN  \n",
       "2                 NaN  \n",
       "3                 NaN  \n",
       "4                 NaN  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = current_dir + '/data/Motor_Vehicle_Collisions_-_Crashes_20240922.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "print(df.shape)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "157e976e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crash_date                        object\n",
      "crash_time                        object\n",
      "borough                           object\n",
      "zip_code                          object\n",
      "latitude                         float64\n",
      "longitude                        float64\n",
      "location                          object\n",
      "on_street_name                    object\n",
      "cross_street_name                 object\n",
      "off_street_name                   object\n",
      "number_of_persons_injured        float64\n",
      "number_of_persons_killed         float64\n",
      "number_of_pedestrians_injured      int64\n",
      "number_of_pedestrians_killed       int64\n",
      "number_of_cyclist_injured          int64\n",
      "number_of_cyclist_killed           int64\n",
      "number_of_motorist_injured         int64\n",
      "number_of_motorist_killed          int64\n",
      "contributing_factor_vehicle_1     object\n",
      "contributing_factor_vehicle_2     object\n",
      "contributing_factor_vehicle_3     object\n",
      "contributing_factor_vehicle_4     object\n",
      "contributing_factor_vehicle_5     object\n",
      "collision_id                       int64\n",
      "vehicle_type_code_1               object\n",
      "vehicle_type_code_2               object\n",
      "vehicle_type_code_3               object\n",
      "vehicle_type_code_4               object\n",
      "vehicle_type_code_5               object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_') # clean column names\n",
    "df = df.str.strip().str.lower() # clean string entries\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a86dec-ac7b-483e-8a54-9fb34db6b496",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype_dict = { # I also convert latitude and longitude to float32 here since it greatly improves efficiency- float32 can hold only 7 decimals, but this should be enough (accurate to ~10m)\n",
    "    'float32': ['borough','zip_code','number_of_persons_injured', 'number_of_persons_killed', 'latitude', 'longitude'],\n",
    "    'str': ['on_street_name','cross_street_name','off_street_name','contributing_factor_vehicle_1', 'contributing_factor_vehicle_2',\n",
    "           'contributing_factor_vehicle_3','contributing_factor_vehicle_4','contributing_factor_vehicle_5','vehicle_type_code_1',\n",
    "           'vehicle_type_code_2','vehicle_type_code_3','vehicle_type_code_4','vehicle_type_code_5'], \n",
    "    'int8': ['number_of_pedestrians_injured', 'number_of_pedestrians_killed', 'number_of_cyclist_injured', 'number_of_cyclist_killed', \n",
    "             'number_of_motorist_injured', 'number_of_motorist_killed','collision_id'],\n",
    "    'datetime64': ['start_datetime', 'end_datetime']\n",
    "}\n",
    "\n",
    "dtype_mapping = {}\n",
    "for dtype, columns in dtype_dict.items():\n",
    "    for col in columns:\n",
    "        dtype_mapping[col] = dtype\n",
    "\n",
    "for col in df.columns:\n",
    "    df[col] = df[col].astype(dtype_mapping[col])\n",
    "    if col in dtype_dict['str']: # for categorical data, replace nans with 'unknown' cat\n",
    "        df[col] = df[col].astype('category') # string to category, as it needs less memory\n",
    "        df[col] = df[col].cat.add_categories('unknown').fillna('unknown') # add unknown category for nans\n",
    "\n",
    "df['crash_datetime'] = pd.to_datetime(df['crash_date'] + ' ' + df['crash_time'])\n",
    "df.drop(['crash_date','crash_time'], inplace=True)\n",
    "df['year'] = df['crash_datetime'].dt.year.astype('int8') # add year column \n",
    "df['month'] = df['crash_datetime'].dt.month.astype('int8') # add month column \n",
    "        \n",
    "df = df.drop_duplicates().sort_values(by='start_datetime')# drop duplicates and sort to start rental time/date\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "df.to_csv(cleaned_dir + '/crashed_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91dedf0",
   "metadata": {},
   "source": [
    "## Data cleaning\n",
    "### Which columns contain nans?\n",
    "### Change data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f9d08181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Nan_count    Total\n",
      "crash_date                             0  2120518\n",
      "crash_time                             0  2120518\n",
      "borough                           659498  2120518\n",
      "zip_code                          659758  2120518\n",
      "latitude                          247820  2120518\n",
      "longitude                         247820  2120518\n",
      "location                          247820  2120518\n",
      "on_street_name                    453598  2120518\n",
      "cross_street_name                 807416  2120518\n",
      "off_street_name                  1759293  2120518\n",
      "number_of_persons_injured             18  2120518\n",
      "number_of_persons_killed              31  2120518\n",
      "number_of_pedestrians_injured          0  2120518\n",
      "number_of_pedestrians_killed           0  2120518\n",
      "number_of_cyclist_injured              0  2120518\n",
      "number_of_cyclist_killed               0  2120518\n",
      "number_of_motorist_injured             0  2120518\n",
      "number_of_motorist_killed              0  2120518\n",
      "contributing_factor_vehicle_1       7107  2120518\n",
      "contributing_factor_vehicle_2     331898  2120518\n",
      "contributing_factor_vehicle_3    1968151  2120518\n",
      "contributing_factor_vehicle_4    2085953  2120518\n",
      "contributing_factor_vehicle_5    2111123  2120518\n",
      "collision_id                           0  2120518\n",
      "vehicle_type_code_1                14384  2120518\n",
      "vehicle_type_code_2               411360  2120518\n",
      "vehicle_type_code_3              1973843  2120518\n",
      "vehicle_type_code_4              2087182  2120518\n",
      "vehicle_type_code_5              2111410  2120518\n"
     ]
    }
   ],
   "source": [
    "# Check which column has missing values (nans) and how many\n",
    "summary_table = pd.DataFrame({\n",
    "    'Nan_count': df.isna().sum(),\n",
    "    'Total': df.shape[0]\n",
    "})\n",
    "\n",
    "print(summary_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ff2d9d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  2.,  3.,  4., nan,  8.,  5.])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check types of data in columns\n",
    "#df['zip_code'].unique()\n",
    "#df['number_of_persons_injured'].unique()\n",
    "df['number_of_persons_killed'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de56243",
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_dict = {\n",
    "    'borough': str,\n",
    "}\n",
    "df = df.astype(convert_dict)\n",
    "    \n",
    "df['datetime'] = pd.to_datetime(df['date'] + ' ' + df['time']) # create datetime column\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df['year'] = df['datetime'].dt.year # create year column for easy data selection\n",
    "\n",
    "#df[col] = pd.to_numeric(df[col], errors='coerce')  # errors='coerce' will convert invalid parsing to NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e799370c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fc6039",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
