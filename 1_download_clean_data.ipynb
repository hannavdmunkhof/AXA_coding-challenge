{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1487e09e",
   "metadata": {},
   "source": [
    "# AXA coding challenge\n",
    "Data:\n",
    "1. Citibike: https://s3.amazonaws.com/tripdata/index.html\n",
    "2. NYPD:  https://data.cityofnewyork.us/Public-Safety/Motor-Vehicle-Collisions-Crashes/h9gi-nx95/about_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6be0356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory: C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge\n"
     ]
    }
   ],
   "source": [
    "# Install packages (only once)\n",
    "#!pip install selenium webdriver-manager\n",
    "\n",
    "# Import modules\n",
    "import os # basic\n",
    "import datetime\n",
    "import zipfile\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "import dask.dataframe as dd\n",
    "\n",
    "from selenium import webdriver # for downloading files automatically\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "current_dir = os.getcwd() # current dir\n",
    "print('Current directory: ' + current_dir)\n",
    "extract_dir = current_dir + '/raw_data/bike-tripdata'  # directory where extracted files from 1. will be saved\n",
    "cleaned_dir = current_dir + '/clean_data/' # directory where cleaned and concatenated df will be saved\n",
    "\n",
    "#pd.options.display.float_format = '{:.4f}'.format # set pd output to 2 decimals\n",
    "#pd.reset_option('display.float_format')\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "921e8034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "# to download files from an url\n",
    "def download_files(url, save_path):\n",
    "    response = requests.get(url, stream=True)\n",
    "    with open(save_path, 'wb') as file:\n",
    "        for chunk in response.iter_content(chunk_size=1024):\n",
    "            if chunk:\n",
    "                file.write(chunk)\n",
    "    print(f\"Downloaded {save_path}\")\n",
    "    \n",
    "# to clean column names\n",
    "def clean_column_names(df, column_mapping=None):\n",
    "    # strip whitespace, convert to lowercase, and replace spaces with underscores\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('-', '_')\n",
    "    \n",
    "    # apply manual column mapping if specified\n",
    "    if column_mapping:\n",
    "        df.rename(columns=column_mapping, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# check unique column names across all .csv files in list_files \n",
    "def list_unique_col_names(list_files):\n",
    "    unique_column_names = []\n",
    "    for csv_file in list_files:\n",
    "        file_path = os.path.join(extract_dir, csv_file)\n",
    "        df = pd.read_csv(file_path, nrows=1)\n",
    "        #print(df.columns) # visual check\n",
    "        [unique_column_names.append(col) for col in df.columns if col not in unique_column_names]\n",
    "    unique_column_names.sort()\n",
    "    \n",
    "    return unique_column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1af9203",
   "metadata": {},
   "source": [
    "## Download Citibike data automatically from url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e14c0153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "url = \"https://s3.amazonaws.com/tripdata/index.html\" # url to data files\n",
    "driver_path = 'C:/Drivers/chromedriver-win64_128/chromedriver.exe' # Chrome driver for web interaction, needed by selenium - must match Chrome version\n",
    "\n",
    "# Download files\n",
    "service = Service(driver_path) # initialize the Chrome driver\n",
    "driver = webdriver.Chrome(service=service)\n",
    "driver.get(url) # navigate to website\n",
    "time.sleep(5)  # give the page time to load the dynamic content\n",
    "html = driver.page_source # get the page source after JavaScript has executed\n",
    "soup = BeautifulSoup(html, 'html.parser') # parse the HTML\n",
    "\n",
    "# find all .zip links\n",
    "file_links = []\n",
    "for link in soup.find_all('a', href=True):\n",
    "    if link['href'].endswith('.zip'): # on this website, files are .zip format\n",
    "        file_links.append(link['href'])\n",
    "print(file_links[:2]) # check if the file paths are retrieved correctly by printing a few\n",
    "\n",
    "driver.quit() # close the browser\n",
    "\n",
    "if not os.path.exists(current_dir+'/downloads'): # directory to save the downloaded files\n",
    "    os.makedirs(current_dir+'/downloads')\n",
    "\n",
    "for file_link in file_links: # loop through all the zip links and download them\n",
    "    filename = os.path.join(current_dir+'/downloads', os.path.basename(file_link))\n",
    "    \n",
    "    if not file_link.startswith('http'): # if the link is relative, make it an absolute URL by appending the base URL\n",
    "        file_link = url + file_link\n",
    "\n",
    "    download_files(file_link, filename) # download the file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea75400",
   "metadata": {},
   "source": [
    "## Unzip & reorganize files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381447ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - alternatively (instead of next cell), unzip first and then reorganize files\n",
    "\n",
    "# Unzip files  \n",
    "# zip_dir = current_dir+'/downloads' # directory containing the zip files\n",
    "# extract_dir = current_dir+'/data' # directory where extracted files will be saved\n",
    "\n",
    "# for filename in os.listdir(zip_dir): # loop through all files in the directory\n",
    "#     if filename.endswith('.zip') :\n",
    "#         zip_file_path = os.path.join(zip_dir, filename)\n",
    "#         new_file_path = extract_dir + '/' + filename[:-4] + '.csv' # remove '.zip' and subfolders from the target path name\n",
    "#         os.makedirs(new_file_path, exist_ok=True)  # create the directory if it doesn't exist\n",
    "\n",
    "#         with zipfile.ZipFile(zip_file_path, 'r') as zip_ref: # extract the zip file\n",
    "#             for member in zip_ref.namelist():\n",
    "#                 if '_MACOSX' not in member: # skip any file or folder inside \"_MACOSX\" (for MAC computers, not needed)\n",
    "#                     zip_ref.extract(member, new_file_path) # extract to the specified directory\n",
    "\n",
    "#             print(f'Extracted: {member} to {new_file_path}')\n",
    "\n",
    "\n",
    "# # Move  files from subfolders in subfolders to 1 folder\n",
    "\n",
    "# import shutil\n",
    "\n",
    "# source_dir = current_dir + '/data'\n",
    "# destination_dir = current_dir + '/data_test'\n",
    "# os.makedirs(destination_dir, exist_ok=True)\n",
    "\n",
    "# for root, dirs, files in os.walk(source_dir):\n",
    "#     for file in files:\n",
    "#         if file.endswith('.csv') and not file.startswith('.'): # select .csv files, skip files starting with '.' \n",
    "#             if '_MACOSX' in root:\n",
    "#                 continue  # skip this directory and its contents, for MAC\n",
    "\n",
    "#             source_file = os.path.join(root, file)\n",
    "#             destination_file = os.path.join(destination_dir, file)\n",
    "            \n",
    "#             shutil.move(source_file, destination_file) # or shutil.copy\n",
    "#             print(f\"Moved: {source_file} -> {destination_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1c46ca43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 201309-citibike-tripdata.csv\n",
      "Extracted 201311-citibike-tripdata.csv\n",
      "Extracted 201307-citibike-tripdata.csv\n",
      "Extracted 201308-citibike-tripdata.csv\n",
      "Extracted 201306-citibike-tripdata.csv\n",
      "Extracted 201310-citibike-tripdata.csv\n",
      "Extracted 201312-citibike-tripdata.csv\n",
      "Extracted 201312-citibike-tripdata_1.csv\n",
      "Extracted 201311-citibike-tripdata_1.csv\n",
      "Extracted 201307-citibike-tripdata_1.csv\n",
      "Extracted 201310-citibike-tripdata_2.csv\n",
      "Extracted 201310-citibike-tripdata_1.csv\n",
      "Extracted 201309-citibike-tripdata_2.csv\n",
      "Extracted 201309-citibike-tripdata_1.csv\n",
      "Extracted 201308-citibike-tripdata_1.csv\n",
      "Extracted 201308-citibike-tripdata_2.csv\n",
      "Extracted 201306-citibike-tripdata_1.csv\n",
      "from 2013-citibike-tripdata.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted 201404-citibike-tripdata_1.csv\n",
      "Extracted 201412-citibike-tripdata_1.csv\n",
      "Extracted 201411-citibike-tripdata_1.csv\n",
      "Extracted 201407-citibike-tripdata_1.csv\n",
      "Extracted 201410-citibike-tripdata_1.csv\n",
      "Extracted 201409-citibike-tripdata_1.csv\n",
      "Extracted 201408-citibike-tripdata_1.csv\n",
      "Extracted 201406-citibike-tripdata_1.csv\n",
      "Extracted 201403-citibike-tripdata_1.csv\n",
      "Extracted 201401-citibike-tripdata_1.csv\n",
      "Extracted 201402-citibike-tripdata_1.csv\n",
      "Extracted 201405-citibike-tripdata_1.csv\n",
      "from 2014-citibike-tripdata.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted 201504-citibike-tripdata_1.csv\n",
      "Extracted 201512-citibike-tripdata_1.csv\n",
      "Extracted 201511-citibike-tripdata_1.csv\n",
      "Extracted 201507-citibike-tripdata_1.csv\n",
      "Extracted 201507-citibike-tripdata_2.csv\n",
      "Extracted 201510-citibike-tripdata_2.csv\n",
      "Extracted 201510-citibike-tripdata_1.csv\n",
      "Extracted 201509-citibike-tripdata_2.csv\n",
      "Extracted 201509-citibike-tripdata_1.csv\n",
      "Extracted 201508-citibike-tripdata_1.csv\n",
      "Extracted 201508-citibike-tripdata_2.csv\n",
      "Extracted 201506-citibike-tripdata_1.csv\n",
      "Extracted 201503-citibike-tripdata_1.csv\n",
      "Extracted 201501-citibike-tripdata_1.csv\n",
      "Extracted 201502-citibike-tripdata_1.csv\n",
      "Extracted 201505-citibike-tripdata_1.csv\n",
      "from 2015-citibike-tripdata.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted 201604-citibike-tripdata_1.csv\n",
      "Extracted 201604-citibike-tripdata_2.csv\n",
      "Extracted 201612-citibike-tripdata_1.csv\n",
      "Extracted 201611-citibike-tripdata_2.csv\n",
      "Extracted 201611-citibike-tripdata_1.csv\n",
      "Extracted 201607-citibike-tripdata_2.csv\n",
      "Extracted 201607-citibike-tripdata_1.csv\n",
      "Extracted 201610-citibike-tripdata_1.csv\n",
      "Extracted 201610-citibike-tripdata_2.csv\n",
      "Extracted 201609-citibike-tripdata_1.csv\n",
      "Extracted 201609-citibike-tripdata_2.csv\n",
      "Extracted 201608-citibike-tripdata_2.csv\n",
      "Extracted 201608-citibike-tripdata_1.csv\n",
      "Extracted 201606-citibike-tripdata_1.csv\n",
      "Extracted 201606-citibike-tripdata_2.csv\n",
      "Extracted 201603-citibike-tripdata_1.csv\n",
      "Extracted 201601-citibike-tripdata_1.csv\n",
      "Extracted 201602-citibike-tripdata_1.csv\n",
      "Extracted 201605-citibike-tripdata_2.csv\n",
      "Extracted 201605-citibike-tripdata_1.csv\n",
      "from 2016-citibike-tripdata.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted 201704-citibike-tripdata.csv_1.csv\n",
      "Extracted 201704-citibike-tripdata.csv_2.csv\n",
      "Extracted 201712-citibike-tripdata.csv_1.csv\n",
      "Extracted 201711-citibike-tripdata.csv_2.csv\n",
      "Extracted 201711-citibike-tripdata.csv_1.csv\n",
      "Extracted 201707-citibike-tripdata.csv_2.csv\n",
      "Extracted 201707-citibike-tripdata.csv_1.csv\n",
      "Extracted 201710-citibike-tripdata.csv_1.csv\n",
      "Extracted 201710-citibike-tripdata.csv_2.csv\n",
      "Extracted 201709-citibike-tripdata.csv_2.csv\n",
      "Extracted 201709-citibike-tripdata.csv_1.csv\n",
      "Extracted 201708-citibike-tripdata.csv_1.csv\n",
      "Extracted 201708-citibike-tripdata.csv_2.csv\n",
      "Extracted 201706-citibike-tripdata.csv_1.csv\n",
      "Extracted 201706-citibike-tripdata.csv_2.csv\n",
      "Extracted 201703-citibike-tripdata.csv_1.csv\n",
      "Extracted 201701-citibike-tripdata.csv_1.csv\n",
      "Extracted 201702-citibike-tripdata.csv_1.csv\n",
      "Extracted 201705-citibike-tripdata.csv_2.csv\n",
      "Extracted 201705-citibike-tripdata.csv_1.csv\n",
      "from 2017-citibike-tripdata.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted 201809-citibike-tripdata.csv\n",
      "Extracted 201801-citibike-tripdata.csv\n",
      "Extracted 201803-citibike-tripdata.csv\n",
      "Extracted 201805-citibike-tripdata.csv\n",
      "Extracted 201807-citibike-tripdata.csv\n",
      "Extracted 201811-citibike-tripdata.csv\n",
      "Extracted 201804-citibike-tripdata_1.csv\n",
      "Extracted 201804-citibike-tripdata_2.csv\n",
      "Extracted 201808-citibike-tripdata.csv\n",
      "Extracted 201802-citibike-tripdata.csv\n",
      "Extracted 201812-citibike-tripdata.csv\n",
      "Extracted 201804-citibike-tripdata.csv\n",
      "Extracted 201810-citibike-tripdata.csv\n",
      "Extracted 201806-citibike-tripdata.csv\n",
      "Extracted 201804-citibike-tripdata_1.csv\n",
      "Extracted 201804-citibike-tripdata_2.csv\n",
      "Extracted 201812-citibike-tripdata_1.csv\n",
      "Extracted 201812-citibike-tripdata_2.csv\n",
      "Extracted 201811-citibike-tripdata_2.csv\n",
      "Extracted 201811-citibike-tripdata_1.csv\n",
      "Extracted 201807-citibike-tripdata_2.csv\n",
      "Extracted 201807-citibike-tripdata_1.csv\n",
      "Extracted 201810-citibike-tripdata_1.csv\n",
      "Extracted 201810-citibike-tripdata_2.csv\n",
      "Extracted 201809-citibike-tripdata_1.csv\n",
      "Extracted 201809-citibike-tripdata_2.csv\n",
      "Extracted 201808-citibike-tripdata_2.csv\n",
      "Extracted 201808-citibike-tripdata_1.csv\n",
      "Extracted 201806-citibike-tripdata_1.csv\n",
      "Extracted 201806-citibike-tripdata_2.csv\n",
      "Extracted 201803-citibike-tripdata_1.csv\n",
      "Extracted 201801-citibike-tripdata_1.csv\n",
      "Extracted 201802-citibike-tripdata_1.csv\n",
      "Extracted 201805-citibike-tripdata_2.csv\n",
      "Extracted 201805-citibike-tripdata_1.csv\n",
      "from 2018-citibike-tripdata.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted 201904-citibike-tripdata_2.csv\n",
      "Extracted 201904-citibike-tripdata_1.csv\n",
      "Extracted 201912-citibike-tripdata_1.csv\n",
      "Extracted 201911-citibike-tripdata_1.csv\n",
      "Extracted 201911-citibike-tripdata_2.csv\n",
      "Extracted 201907-citibike-tripdata_1.csv\n",
      "Extracted 201907-citibike-tripdata_3.csv\n",
      "Extracted 201907-citibike-tripdata_2.csv\n",
      "Extracted 201910-citibike-tripdata_3.csv\n",
      "Extracted 201910-citibike-tripdata_2.csv\n",
      "Extracted 201910-citibike-tripdata_1.csv\n",
      "Extracted 201909-citibike-tripdata_3.csv\n",
      "Extracted 201909-citibike-tripdata_2.csv\n",
      "Extracted 201909-citibike-tripdata_1.csv\n",
      "Extracted 201908-citibike-tripdata_1.csv\n",
      "Extracted 201908-citibike-tripdata_2.csv\n",
      "Extracted 201908-citibike-tripdata_3.csv\n",
      "Extracted 201906-citibike-tripdata_2.csv\n",
      "Extracted 201906-citibike-tripdata_3.csv\n",
      "Extracted 201906-citibike-tripdata_1.csv\n",
      "Extracted 201903-citibike-tripdata_1.csv\n",
      "Extracted 201903-citibike-tripdata_2.csv\n",
      "Extracted 201901-citibike-tripdata_1.csv\n",
      "Extracted 201902-citibike-tripdata_1.csv\n",
      "Extracted 201905-citibike-tripdata_1.csv\n",
      "Extracted 201905-citibike-tripdata_2.csv\n",
      "from 2019-citibike-tripdata.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "from 2020-citibike-tripdata.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "from 2021-citibike-tripdata.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "from 2022-citibike-tripdata.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "from 2023-citibike-tripdata.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted 202401-citibike-tripdata.csv\n",
      "from 202401-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted 202402-citibike-tripdata.csv\n",
      "from 202402-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted 202403-citibike-tripdata.csv\n",
      "from 202403-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted 202404-citibike-tripdata.csv\n",
      "from 202404-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted 202405-citibike-tripdata_1.csv\n",
      "Extracted 202405-citibike-tripdata_2.csv\n",
      "Extracted 202405-citibike-tripdata_3.csv\n",
      "Extracted 202405-citibike-tripdata_4.csv\n",
      "Extracted 202405-citibike-tripdata_5.csv\n",
      "from 202405-citibike-tripdata.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted 202406-citibike-tripdata_5.csv\n",
      "Extracted 202406-citibike-tripdata_4.csv\n",
      "Extracted 202406-citibike-tripdata_1.csv\n",
      "Extracted 202406-citibike-tripdata_3.csv\n",
      "Extracted 202406-citibike-tripdata_2.csv\n",
      "from 202406-citibike-tripdata.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 202407-citibike-tripdata_1.csv\n",
      "Extracted 202407-citibike-tripdata_2.csv\n",
      "Extracted 202407-citibike-tripdata_3.csv\n",
      "Extracted 202407-citibike-tripdata_4.csv\n",
      "Extracted 202407-citibike-tripdata_5.csv\n",
      "from 202407-citibike-tripdata.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted 202408-citibike-tripdata_3.csv\n",
      "Extracted 202408-citibike-tripdata_2.csv\n",
      "Extracted 202408-citibike-tripdata_1.csv\n",
      "Extracted 202408-citibike-tripdata_5.csv\n",
      "Extracted 202408-citibike-tripdata_4.csv\n",
      "from 202408-citibike-tripdata.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201509-citibike-tripdata.csv\n",
      "from JC-201509-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201510-citibike-tripdata.csv\n",
      "from JC-201510-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201511-citibike-tripdata.csv\n",
      "from JC-201511-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201512-citibike-tripdata.csv\n",
      "from JC-201512-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-20161-citibike-tripdata.csv\n",
      "from JC-201601-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-20162-citibike-tripdata.csv\n",
      "from JC-201602-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-20163-citibike-tripdata.csv\n",
      "from JC-201603-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201604-citibike-tripdata.csv\n",
      "from JC-201604-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201605-citibike-tripdata.csv\n",
      "from JC-201605-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201606-citibike-tripdata.csv\n",
      "from JC-201606-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201607-citibike-tripdata.csv\n",
      "from JC-201607-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201608-citibike-tripdata.csv\n",
      "from JC-201608-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201609-citibike-tripdata.csv\n",
      "from JC-201609-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201610-citibike-tripdata.csv\n",
      "from JC-201610-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201611-citibike-tripdata.csv\n",
      "from JC-201611-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201612-citibike-tripdata.csv\n",
      "from JC-201612-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201701-citibike-tripdata.csv\n",
      "from JC-201701-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201702-citibike-tripdata.csv\n",
      "from JC-201702-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201703-citibike-tripdata.csv\n",
      "from JC-201703-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201704-citibike-tripdata.csv\n",
      "from JC-201704-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201705-citibike-tripdata.csv\n",
      "from JC-201705-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201706-citibike-tripdata.csv\n",
      "from JC-201706-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201707-citibike-tripdata.csv\n",
      "from JC-201707-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201708 citibike-tripdata.csv\n",
      "from JC-201708 citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201709-citibike-tripdata.csv\n",
      "from JC-201709-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201710-citibike-tripdata.csv\n",
      "from JC-201710-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201711-citibike-tripdata.csv\n",
      "from JC-201711-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201712-citibike-tripdata.csv\n",
      "from JC-201712-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201801-citibike-tripdata.csv\n",
      "from JC-201801-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201802-citibike-tripdata.csv\n",
      "from JC-201802-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201803-citibike-tripdata.csv\n",
      "from JC-201803-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201804-citibike-tripdata.csv\n",
      "from JC-201804-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201805-citibike-tripdata.csv\n",
      "from JC-201805-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201806-citibike-tripdata.csv\n",
      "from JC-201806-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201807-citibike-tripdata.csv\n",
      "from JC-201807-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201808-citibike-tripdata.csv\n",
      "from JC-201808-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201809-citibike-tripdata.csv\n",
      "from JC-201809-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201810-citibike-tripdata.csv\n",
      "from JC-201810-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201811-citibike-tripdata.csv\n",
      "from JC-201811-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201812-citibike-tripdata.csv\n",
      "from JC-201812-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201901-citibike-tripdata.csv\n",
      "from JC-201901-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201902-citibike-tripdata.csv\n",
      "from JC-201902-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201903-citibike-tripdata.csv\n",
      "from JC-201903-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201904-citibike-tripdata.csv\n",
      "from JC-201904-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201905-citibike-tripdata.csv\n",
      "from JC-201905-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201906-citibike-tripdata.csv\n",
      "from JC-201906-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201907-citibike-tripdata.csv\n",
      "from JC-201907-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201908-citibike-tripdata.csv\n",
      "from JC-201908-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201909-citibike-tripdata.csv\n",
      "from JC-201909-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201910-citibike-tripdata.csv\n",
      "from JC-201910-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201911-citibike-tripdata.csv\n",
      "from JC-201911-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-201912-citibike-tripdata.csv\n",
      "from JC-201912-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202001-citibike-tripdata.csv\n",
      "from JC-202001-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202002-citibike-tripdata.csv\n",
      "from JC-202002-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202003-citibike-tripdata.csv\n",
      "from JC-202003-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202004-citibike-tripdata.csv\n",
      "from JC-202004-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202005-citibike-tripdata.csv\n",
      "from JC-202005-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202006-citibike-tripdata.csv\n",
      "from JC-202006-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted JC-202007-citibike-tripdata.csv\n",
      "from JC-202007-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202008-citibike-tripdata.csv\n",
      "from JC-202008-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202009-citibike-tripdata.csv\n",
      "from JC-202009-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202010-citibike-tripdata.csv\n",
      "from JC-202010-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202011-citibike-tripdata.csv\n",
      "from JC-202011-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202012-citibike-tripdata.csv\n",
      "from JC-202012-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202101-citibike-tripdata.csv\n",
      "from JC-202101-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202102-citibike-tripdata.csv\n",
      "from JC-202102-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202103-citibike-tripdata.csv\n",
      "from JC-202103-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202104-citibike-tripdata.csv\n",
      "from JC-202104-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202105-citibike-tripdata.csv\n",
      "from JC-202105-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202106-citibike-tripdata.csv\n",
      "from JC-202106-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202107-citibike-tripdata.csv\n",
      "from JC-202107-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202108-citibike-tripdata.csv\n",
      "from JC-202108-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202109-citibike-tripdata.csv\n",
      "from JC-202109-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202110-citibike-tripdata.csv\n",
      "from JC-202110-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202111-citibike-tripdata.csv\n",
      "from JC-202111-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202112-citibike-tripdata.csv\n",
      "from JC-202112-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202201-citibike-tripdata.csv\n",
      "from JC-202201-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202202-citibike-tripdata.csv\n",
      "from JC-202202-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202203-citibike-tripdata.csv\n",
      "from JC-202203-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202204-citibike-tripdata.csv\n",
      "from JC-202204-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202205-citibike-tripdata.csv\n",
      "from JC-202205-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202206-citibike-tripdata.csv\n",
      "from JC-202206-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202207-citbike-tripdata.csv\n",
      "from JC-202207-citbike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202208-citibike-tripdata.csv\n",
      "from JC-202208-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202209-citibike-tripdata.csv\n",
      "from JC-202209-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202210-citibike-tripdata.csv\n",
      "from JC-202210-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202211-citibike-tripdata.csv\n",
      "from JC-202211-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202212-citibike-tripdata.csv\n",
      "from JC-202212-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202301-citibike-tripdata.csv\n",
      "from JC-202301-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202302-citibike-tripdata.csv\n",
      "from JC-202302-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202303-citibike-tripdata.csv\n",
      "from JC-202303-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202304-citibike-tripdata.csv\n",
      "from JC-202304-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202305-citibike-tripdata.csv\n",
      "from JC-202305-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202306-citibike-tripdata.csv\n",
      "from JC-202306-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202307-citibike-tripdata.csv\n",
      "from JC-202307-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202308-citibike-tripdata.csv\n",
      "from JC-202308-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202309-citibike-tripdata.csv\n",
      "from JC-202309-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202310-citibike-tripdata.csv\n",
      "from JC-202310-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202311-citibike-tripdata.csv\n",
      "from JC-202311-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202312-citibike-tripdata.csv\n",
      "from JC-202312-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202401-citibike-tripdata.csv\n",
      "from JC-202401-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202402-citibike-tripdata.csv\n",
      "from JC-202402-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202403-citibike-tripdata.csv\n",
      "from JC-202403-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202404-citibike-tripdata.csv\n",
      "from JC-202404-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202406-citibike-tripdata.csv\n",
      "from JC-202406-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202407-citibike-tripdata.csv\n",
      "from JC-202407-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n",
      "Extracted JC-202408-citibike-tripdata.csv\n",
      "from JC-202408-citibike-tripdata.csv.zip to C:\\Users\\Hanna\\sciebo\\AXA_coding-challenge/data/bike-tripdata\n"
     ]
    }
   ],
   "source": [
    "# Unzip files & reorganize simultaneously\n",
    "zip_dir = current_dir + '/downloads'  # directory containing the zip files\n",
    "extract_dir = current_dir + '/data/bike-tripdata'  # directory where extracted files will be saved\n",
    "\n",
    "os.makedirs(extract_dir, exist_ok=True)  # create the directory if it doesn't exist\n",
    "\n",
    "for filename in os.listdir(zip_dir):  # loop through all files in the directory\n",
    "    if filename.endswith('.zip'):\n",
    "        zip_file_path = os.path.join(zip_dir, filename)\n",
    "\n",
    "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:  # extract the zip file\n",
    "            for member in zip_ref.namelist():\n",
    "                # skip any file or folder inside \"_MACOSX\" (for MAC computers, not needed), and files that do not end with .csv\n",
    "                if '_MACOSX' not in member and member.endswith('.csv'):  \n",
    "                    # get only the base name of the file (ignore folder structure in zip)\n",
    "                    base_member = os.path.basename(member)\n",
    "                    target_path = os.path.join(extract_dir, base_member)\n",
    "                    \n",
    "                    with zip_ref.open(member) as source, open(target_path, 'wb') as target:\n",
    "                        target.write(source.read())  # write the extracted content to the single folder\n",
    "\n",
    "                    print(f'Extracted {base_member}')\n",
    "    print(f'... from {filename} to {extract_dir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7985a3c7",
   "metadata": {},
   "source": [
    "## Visualize dataset and clean\n",
    "1. Column names:\n",
    "  * Check which unique column names exist across all files (since they are inconsistent)\n",
    "  * Correct column names (strip uppercase and convert space to underscore) and create global column name list\n",
    "2. Dtypes:\n",
    "  * Check which dtypes files contain, and whether they are consistent across all files (-> they are not)\n",
    "  * Decide which columns to change to which dtype (to keep cleaned file as small as possible)\n",
    "4. For categorical data:\n",
    "  * Check which unique entries exist across files and save these in a dictionary\n",
    "5. Loop over all csv files, concatenate and save (and deal with some nans)\n",
    "  * For each file: (in pandas) clean column names, remove unwanted columns, add nans for global columns that donÂ´t exist in individual file, set desired dtype and category dealing with nans, drop duplicate rows, add 'year' and 'month' column from 'start_datetime'; (in dask) convert to dask df for more efficiency and append to big list\n",
    "\n",
    "## Concatenate files into dask parquet file\n",
    "6. Concatenate all files, remove duplicates, and save as dask parquet file (partitioned in year and month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "354591e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ride_id', 'rideable_type', 'started_at', 'ended_at', 'start_station_name', 'start_station_id', 'end_station_name', 'end_station_id', 'start_lat', 'start_lng', 'end_lat', 'end_lng', 'member_casual']\n"
     ]
    }
   ],
   "source": [
    "# Since CSV files do not contain the same column headers, check which ones exist in the dataset?\n",
    "list_files = [f for f in os.listdir(extract_dir) if f.endswith('.csv')]\n",
    "unique_column_names = []\n",
    "for csv_file in list_files:\n",
    "    file_path = os.path.join(extract_dir, csv_file)\n",
    "    df = pd.read_csv(file_path, nrows=3)\n",
    "    [unique_column_names.append(col) for col in df.columns if col not in unique_column_names]\n",
    "print(unique_column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854012b1",
   "metadata": {},
   "source": [
    "It turns out that the column names are not consistent, e.g. some files contain the column \"starttime\" while others contain the column \"Start Time\". This should be corrected. Additionally, column names should not contain spaces (\"start station latitude\" vs \"start_lat\"). Last, there are 2 strange column names which need to be checked: \"Unnamed: 0\" and \"rideable_type_duplicate_column_name_1\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d22e286",
   "metadata": {},
   "source": [
    "### 1. Column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e319ed65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique column names: \n",
      "  ['end_lat', 'end_lng', 'end_station_id', 'end_station_name', 'ended_at', 'member_casual', 'ride_id', 'rideable_type', 'start_lat', 'start_lng', 'start_station_id', 'start_station_name', 'started_at']\n",
      " \n",
      "Unique column names after cleaning: \n",
      " ['end_datetime', 'end_station_id', 'end_station_latitude', 'end_station_longitude', 'end_station_name', 'ride_id', 'rideable_type', 'start_datetime', 'start_station_id', 'start_station_latitude', 'start_station_longitude', 'start_station_name', 'user_type']\n"
     ]
    }
   ],
   "source": [
    "unique_column_names = list_unique_col_names(list_files)\n",
    "print('Unique column names: \\n  ' + str(unique_column_names))\n",
    "column_mapping = {\n",
    "    'bikeid': 'bike_id',\n",
    "    'end_lat': 'end_station_latitude',\n",
    "    'end_lng': 'end_station_longitude',\n",
    "    'ended_at': 'end_datetime',\n",
    "    'member_casual': 'user_type',\n",
    "    'rideable_type_duplicate_column_name_1': 'duplicate_col',\n",
    "    'start_lat': 'start_station_latitude',\n",
    "    'start_lng': 'start_station_longitude',\n",
    "    'starttime': 'start_datetime',\n",
    "    'start_time': 'start_datetime',\n",
    "    'started_at': 'start_datetime',\n",
    "    'stoptime': 'end_datetime',\n",
    "    'stop_time': 'end_datetime',\n",
    "    'tripduration': 'trip_duration',\n",
    "    'unnamed:_0': 'unnamed', # this is just an index column without name, present in some files -> can be discarded later\n",
    "    'usertype': 'user_type'\n",
    "}\n",
    "\n",
    "# Exploratory correction, see if it solves the inconsistencies\n",
    "unique_column_names=[]\n",
    "for csv_file in list_files:\n",
    "    file_path = os.path.join(extract_dir, csv_file)\n",
    "    df = pd.read_csv(file_path, nrows=1)\n",
    "#     if 'rideable_type_duplicate_column_name_1' in df.columns: # check what column this is -> just a duplicate -> can be discarded\n",
    "#         print(df.head(2))\n",
    "    df = clean_column_names(df, column_mapping)\n",
    "    ##print(df.columns)\n",
    "    \n",
    "    [unique_column_names.append(col) for col in df.columns if col not in unique_column_names] # save new col names for checking\n",
    "    unique_column_names.sort()\n",
    "\n",
    "remove_names = ['unnamed','duplicate_col'] # column names to remove\n",
    "final_column_names = [name for name in unique_column_names if name not in remove_names] # list with final universal column names\n",
    "print(' ')\n",
    "print('Unique column names after cleaning: \\n ' + str(final_column_names)) # -> satisfied!\n",
    "final_column_names.extend(['year','month']) # add the columns year and month, as I will add them from start_datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52b9973-3450-4d85-b4c7-aa33b948a312",
   "metadata": {},
   "source": [
    "### 2. Dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70143a60",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column: end_datetime\n",
      "Unique dtypes: {dtype('O')}\n",
      "\n",
      "Column: end_station_id\n",
      "Unique dtypes: {dtype('float64')}\n",
      "\n",
      "Column: end_station_latitude\n",
      "Unique dtypes: {dtype('float64')}\n",
      "\n",
      "Column: end_station_longitude\n",
      "Unique dtypes: {dtype('float64')}\n",
      "\n",
      "Column: end_station_name\n",
      "Unique dtypes: {dtype('O')}\n",
      "\n",
      "Column: ride_id\n",
      "Unique dtypes: {dtype('O')}\n",
      "\n",
      "Column: rideable_type\n",
      "Unique dtypes: {dtype('O')}\n",
      "\n",
      "Column: start_datetime\n",
      "Unique dtypes: {dtype('O')}\n",
      "\n",
      "Column: start_station_id\n",
      "Unique dtypes: {dtype('float64')}\n",
      "\n",
      "Column: start_station_latitude\n",
      "Unique dtypes: {dtype('float64')}\n",
      "\n",
      "Column: start_station_longitude\n",
      "Unique dtypes: {dtype('float64')}\n",
      "\n",
      "Column: start_station_name\n",
      "Unique dtypes: {dtype('O')}\n",
      "\n",
      "Column: user_type\n",
      "Unique dtypes: {dtype('O')}\n",
      "\n",
      "Column: year\n",
      "Unique dtypes: set()\n",
      "\n",
      "Column: month\n",
      "Unique dtypes: set()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check if data type of files are the same\n",
    "check_dtype = {col: [] for col in final_column_names} # create empty dict to store dtypes\n",
    "\n",
    "list_files = [f for f in os.listdir(extract_dir) if f.endswith('.csv')]\n",
    "for csv_file in list_files:\n",
    "#df.memory_usage(deep=True).sum()\n",
    "    file_path = os.path.join(extract_dir, csv_file)\n",
    "    df = pd.read_csv(file_path, nrows=3)\n",
    "    df = clean_column_names(df, column_mapping) # clean column names\n",
    "    to_remove = ['duplicate_col','unnamed'] \n",
    "    for col in to_remove:\n",
    "        if col in df.columns:\n",
    "            df.drop(col, axis=1, inplace=True) # drop these columns\n",
    "    for col in df.columns:\n",
    "        #print(f'{col}: {df[col].dtype}')\n",
    "        check_dtype[col].append(df[col].dtype)\n",
    "\n",
    "for col in check_dtype:\n",
    "    print(f\"Column: {col}\")\n",
    "    print(f\"Unique dtypes: {set(check_dtype[col])}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3e8c43f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ride_id</th>\n",
       "      <th>rideable_type</th>\n",
       "      <th>start_datetime</th>\n",
       "      <th>end_datetime</th>\n",
       "      <th>start_station_name</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>end_station_name</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>start_station_latitude</th>\n",
       "      <th>start_station_longitude</th>\n",
       "      <th>end_station_latitude</th>\n",
       "      <th>end_station_longitude</th>\n",
       "      <th>user_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5078F3D302000BD2</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2024-01-22 18:43:19.012</td>\n",
       "      <td>2024-01-22 18:48:10.708</td>\n",
       "      <td>Frederick Douglass Blvd &amp; W 145 St</td>\n",
       "      <td>7954.1200</td>\n",
       "      <td>St Nicholas Ave &amp; W 126 St</td>\n",
       "      <td>7756.1000</td>\n",
       "      <td>40.8231</td>\n",
       "      <td>-73.9417</td>\n",
       "      <td>40.8114</td>\n",
       "      <td>-73.9519</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>814337105D37302A</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2024-01-11 19:19:18.721</td>\n",
       "      <td>2024-01-11 19:47:36.007</td>\n",
       "      <td>W 54 St &amp; 6 Ave</td>\n",
       "      <td>6771.1300</td>\n",
       "      <td>E 74 St &amp; 1 Ave</td>\n",
       "      <td>6953.0800</td>\n",
       "      <td>40.7618</td>\n",
       "      <td>-73.9770</td>\n",
       "      <td>40.7690</td>\n",
       "      <td>-73.9548</td>\n",
       "      <td>member</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A33A920E2B10710C</td>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2024-01-30 19:17:41.693</td>\n",
       "      <td>2024-01-30 19:32:49.857</td>\n",
       "      <td>E 11 St &amp; Ave B</td>\n",
       "      <td>5659.1100</td>\n",
       "      <td>W 10 St &amp; Washington St</td>\n",
       "      <td>5847.0600</td>\n",
       "      <td>40.7276</td>\n",
       "      <td>-73.9798</td>\n",
       "      <td>40.7334</td>\n",
       "      <td>-74.0085</td>\n",
       "      <td>casual</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ride_id  rideable_type           start_datetime  \\\n",
       "0  5078F3D302000BD2  electric_bike  2024-01-22 18:43:19.012   \n",
       "1  814337105D37302A  electric_bike  2024-01-11 19:19:18.721   \n",
       "2  A33A920E2B10710C  electric_bike  2024-01-30 19:17:41.693   \n",
       "\n",
       "              end_datetime                  start_station_name  \\\n",
       "0  2024-01-22 18:48:10.708  Frederick Douglass Blvd & W 145 St   \n",
       "1  2024-01-11 19:47:36.007                     W 54 St & 6 Ave   \n",
       "2  2024-01-30 19:32:49.857                     E 11 St & Ave B   \n",
       "\n",
       "   start_station_id            end_station_name  end_station_id  \\\n",
       "0         7954.1200  St Nicholas Ave & W 126 St       7756.1000   \n",
       "1         6771.1300             E 74 St & 1 Ave       6953.0800   \n",
       "2         5659.1100     W 10 St & Washington St       5847.0600   \n",
       "\n",
       "   start_station_latitude  start_station_longitude  end_station_latitude  \\\n",
       "0                 40.8231                 -73.9417               40.8114   \n",
       "1                 40.7618                 -73.9770               40.7690   \n",
       "2                 40.7276                 -73.9798               40.7334   \n",
       "\n",
       "   end_station_longitude user_type  \n",
       "0               -73.9519    member  \n",
       "1               -73.9548    member  \n",
       "2               -74.0085    casual  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head() # view the last csv file, still loaded in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03d9a5c-5153-4fef-be7c-91e707783751",
   "metadata": {},
   "source": [
    "### 4. Categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b5597c0-6c2a-4563-aa1f-7cd2864331fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hanna\\AppData\\Local\\Temp\\ipykernel_24200\\3511905405.py:12: DtypeWarning: Columns (5,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'gender': set(),\n",
       " 'user_type': {'casual', 'member'},\n",
       " 'rideable_type': {'classic_bike', 'electric_bike'}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get universal entries for columns I am converting to str and then to category (global categories across csv files)\n",
    "dtype_cat =['gender', 'user_type', 'rideable_type']\n",
    "\n",
    "# get global categories (differing per file)\n",
    "categories_dict = {col: set() for col in dtype_cat }\n",
    "\n",
    "# Collect all unique categories across the DataFrames\n",
    "for csv_file in list_files:  \n",
    "    file_path = os.path.join(extract_dir, csv_file) # load individual file\n",
    "    \n",
    "    # Process the file in chunks to save memory\n",
    "    df = pd.read_csv(file_path)\n",
    "    df = clean_column_names(df, column_mapping) # clean column names\n",
    "    to_remove = ['duplicate_col','unnamed'] \n",
    "    for col in to_remove:\n",
    "        if col in df.columns:\n",
    "            df.drop(col, axis=1, inplace=True) # drop these columns\n",
    "        \n",
    "    for col in df.columns:\n",
    "        if col in categories_dict:\n",
    "            df[col].fillna('unknown').dropna()   \n",
    "            categories_dict[col].update(df[col].unique())  # update unique categories of df\n",
    "            #categories_dict[col].add('unknown') # add category 'unknown' for missing data\n",
    "categories_dict['user_type'].discard(np.nan) # for some reason still nan as category here\n",
    "categories_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78dae536",
   "metadata": {},
   "source": [
    "### 5. & 6. Loop over all csv files, concatenate and save as dask parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47bfae2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This cell executed in the Anaconda powershell (clean_concat.py), since itÂ´s faster/requires less memory on my 16GB mem laptop ###\n",
    "\n",
    "if not os.path.exists(cleaned_dir):  # directory to save the cleaned df\n",
    "    os.makedirs(cleaned_dir)\n",
    "\n",
    "dtype_dict = { # desired dtypes\n",
    "    'int32': ['birth_year', 'trip_duration'],\n",
    "    'float32': ['start_station_id', 'end_station_id'], \n",
    "    'float64': ['start_station_latitude', 'start_station_longitude', 'end_station_latitude', 'end_station_longitude'],\n",
    "    'str': ['start_station_name', 'end_station_name', 'ride_id'], \n",
    "    'category': ['gender', 'user_type', 'rideable_type'],\n",
    "    'datetime64[ns]': ['start_datetime', 'end_datetime']\n",
    "}\n",
    "\n",
    "dtype_mapping = {} # dictionary with col: dtype, for changing data types per column\n",
    "for dtype, columns in dtype_dict.items():\n",
    "    for col in columns:\n",
    "        dtype_mapping[col] = dtype\n",
    "\n",
    "list_files = [f for f in os.listdir(extract_dir) if f.endswith('.csv')] # list of csv files in dir to loop over\n",
    "chunksize = 1_000_000  # load in chunks to save memory, in case csv file is huge\n",
    "\n",
    "ddf_list = []\n",
    "for csv_file in list_files[0:2]: \n",
    "    file_path = os.path.join(extract_dir, csv_file) # load individual file\n",
    "    print('loading ' +  csv_file)\n",
    "    \n",
    "    # Process the file in chunks to save memory\n",
    "    chunk_iter = pd.read_csv(file_path, chunksize=chunksize, low_memory=True, parse_dates=True)\n",
    "\n",
    "    for n, chunk in enumerate(chunk_iter):\n",
    "        ddf = clean_column_names(chunk, column_mapping)  # Clean column names\n",
    " \n",
    "        # Convert to Dask DataFrame for larger datasets\n",
    "        #ddf = dd.from_pandas(chunk, npartitions=1) #\n",
    "        #print('to dask converted')\n",
    "        # Drop unwanted columns\n",
    "        to_remove = ['duplicate_col', 'unnamed']\n",
    "        ddf = ddf.drop(columns=[col for col in to_remove if col in ddf.columns])\n",
    "        \n",
    "        missing_cols = set(final_column_names) - set(ddf.columns) # add missing (universal) columns from final_column_names and fill with nans\n",
    "        for col in missing_cols:\n",
    "            ddf[col] = np.nan\n",
    "                \n",
    "        # Convert column dtypes\n",
    "        for col in ddf.columns:\n",
    "            if col in dtype_dict['int32']:\n",
    "                ddf[col] = ddf[col].replace('\\\\N', np.nan)  # handle missing values\n",
    "                ddf[col] = pd.to_numeric(ddf[col], errors='coerce').astype('float32')\n",
    "                ddf[col] = ddf[col].fillna(0).round(0).astype('int32') # replace nan with the place filler 0, round and convert to int\n",
    "            elif col in dtype_dict['category']: # for categorical data, replace nans with 'unknown' cat\n",
    "                ddf[col] = ddf[col].fillna('unknown')  # Replace NaNs with 'unknown'\n",
    "                ddf[col] = ddf[col].astype('str') # convert to str first\n",
    "                ddf[col] = ddf[col].astype('category') # string to category, as it needs less memory\n",
    "                ddf[col] = ddf[col].cat.set_categories(new_categories=list(categories_dict[col])) # set global categories\n",
    "            elif col in dtype_dict['str']: # - added 6.10\n",
    "                ddf[col] = ddf[col].fillna('unknown')  # Replace NaNs with 'unknown'\n",
    "                ddf[col] = ddf[col].astype(dtype_mapping[col]) # \n",
    "            elif col in dtype_mapping.keys():\n",
    "                if col in dtype_dict['float32'] or col in col in dtype_dict['float64']:\n",
    "                    ddf[col] = pd.to_numeric(ddf[col], errors='coerce')\n",
    "                ddf[col] = ddf[col].astype(dtype_mapping[col]) \n",
    "                \n",
    "        ddf = ddf.drop_duplicates().sort_values(by='start_datetime')# drop duplicates and sort to start rental time/date\n",
    "        ddf = ddf.reset_index(drop=True)\n",
    "        #ddf = ddf.set_index('start_datetime') # set start_datetime as index\n",
    "\n",
    "        ddf['year'] = ddf['start_datetime'].dt.year.astype('int32') # add year column for partitioning\n",
    "        ddf['month'] = ddf['start_datetime'].dt.month.astype('int8') # add month column for partitioning\n",
    "                                            \n",
    "        ddf = ddf[final_column_names] # ensure consistent column order   \n",
    "\n",
    "        ddf = dd.from_pandas(ddf, npartitions=5)\n",
    "        ddf_list.append(ddf)   \n",
    "    print('.... cleaned, converted to dask df and appended to ddf_list')     \n",
    "\n",
    "ddf_comb = dd.concat(ddf_list, ignore_index=True) # concatenate all dask dfs\n",
    "#del ddf_list\n",
    "ddf_comb = ddf_comb.drop_duplicates() # remove duplicates\n",
    "\n",
    "# check if all dtypes are consistent across ddfs\n",
    "check_dtype = {col: [] for col in final_column_names}\n",
    "for ddf in ddf_list:\n",
    "    for col in ddf.columns:\n",
    "        check_dtype[col].append(ddf[col].dtype)\n",
    "\n",
    "for col in check_dtype.keys():\n",
    "    print(col)\n",
    "    print(pd.Series(check_dtype[col]).unique())\n",
    "    if len(pd.Series(check_dtype[col]).unique()) > 1:\n",
    "        print(col + ' has inconsistent dtypes')\n",
    "    \n",
    "ddf_comb.to_parquet(cleaned_dir + '/combined_dask_df.parquet', engine='pyarrow', partition_on=['year', 'month'], write_index=False) # write, partitioned on year and month\n",
    "print('All files processed and saved to Parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e07be4-2642-41f6-bb3e-5d1d352fd6f9",
   "metadata": {},
   "source": [
    "## Load cleaned ddf (data bike rides)\n",
    "Some additional cleaning:\n",
    "* Set categorical entries consistently (user_type)\n",
    "* More dtype changes after inspecting the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2afc839e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bike_id</th>\n",
       "      <th>birth_year</th>\n",
       "      <th>end_datetime</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>end_station_latitude</th>\n",
       "      <th>end_station_longitude</th>\n",
       "      <th>end_station_name</th>\n",
       "      <th>gender</th>\n",
       "      <th>ride_id</th>\n",
       "      <th>rideable_type</th>\n",
       "      <th>start_datetime</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>start_station_latitude</th>\n",
       "      <th>start_station_longitude</th>\n",
       "      <th>start_station_name</th>\n",
       "      <th>trip_duration</th>\n",
       "      <th>user_type</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16449.0</td>\n",
       "      <td>1959</td>\n",
       "      <td>2013-06-01 10:14:22</td>\n",
       "      <td>303.0</td>\n",
       "      <td>40.723627</td>\n",
       "      <td>-73.999496</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-06-01 09:59:58</td>\n",
       "      <td>376.0</td>\n",
       "      <td>40.708621</td>\n",
       "      <td>-74.007222</td>\n",
       "      <td>None</td>\n",
       "      <td>864</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19370.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-06-01 13:18:16</td>\n",
       "      <td>466.0</td>\n",
       "      <td>40.743954</td>\n",
       "      <td>-73.991449</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-06-01 12:53:25</td>\n",
       "      <td>530.0</td>\n",
       "      <td>40.771522</td>\n",
       "      <td>-73.990541</td>\n",
       "      <td>None</td>\n",
       "      <td>1491</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17565.0</td>\n",
       "      <td>1986</td>\n",
       "      <td>2013-06-01 14:48:57</td>\n",
       "      <td>325.0</td>\n",
       "      <td>40.736245</td>\n",
       "      <td>-73.984738</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-06-01 14:40:05</td>\n",
       "      <td>382.0</td>\n",
       "      <td>40.734927</td>\n",
       "      <td>-73.992005</td>\n",
       "      <td>None</td>\n",
       "      <td>532</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19200.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2013-06-01 20:35:56</td>\n",
       "      <td>468.0</td>\n",
       "      <td>40.765265</td>\n",
       "      <td>-73.981923</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-06-01 20:04:40</td>\n",
       "      <td>448.0</td>\n",
       "      <td>40.756604</td>\n",
       "      <td>-73.997901</td>\n",
       "      <td>None</td>\n",
       "      <td>1876</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14784.0</td>\n",
       "      <td>1984</td>\n",
       "      <td>2013-06-01 21:49:47</td>\n",
       "      <td>514.0</td>\n",
       "      <td>40.760875</td>\n",
       "      <td>-74.002777</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-06-01 21:37:24</td>\n",
       "      <td>470.0</td>\n",
       "      <td>40.743453</td>\n",
       "      <td>-74.000040</td>\n",
       "      <td>None</td>\n",
       "      <td>743</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bike_id  birth_year        end_datetime  end_station_id  \\\n",
       "0  16449.0        1959 2013-06-01 10:14:22           303.0   \n",
       "1  19370.0           0 2013-06-01 13:18:16           466.0   \n",
       "2  17565.0        1986 2013-06-01 14:48:57           325.0   \n",
       "3  19200.0           0 2013-06-01 20:35:56           468.0   \n",
       "4  14784.0        1984 2013-06-01 21:49:47           514.0   \n",
       "\n",
       "   end_station_latitude  end_station_longitude end_station_name gender  \\\n",
       "0             40.723627             -73.999496             None    NaN   \n",
       "1             40.743954             -73.991449             None    NaN   \n",
       "2             40.736245             -73.984738             None    NaN   \n",
       "3             40.765265             -73.981923             None    NaN   \n",
       "4             40.760875             -74.002777             None    NaN   \n",
       "\n",
       "  ride_id rideable_type      start_datetime  start_station_id  \\\n",
       "0    None           NaN 2013-06-01 09:59:58             376.0   \n",
       "1    None           NaN 2013-06-01 12:53:25             530.0   \n",
       "2    None           NaN 2013-06-01 14:40:05             382.0   \n",
       "3    None           NaN 2013-06-01 20:04:40             448.0   \n",
       "4    None           NaN 2013-06-01 21:37:24             470.0   \n",
       "\n",
       "   start_station_latitude  start_station_longitude start_station_name  \\\n",
       "0               40.708621               -74.007222               None   \n",
       "1               40.771522               -73.990541               None   \n",
       "2               40.734927               -73.992005               None   \n",
       "3               40.756604               -73.997901               None   \n",
       "4               40.743453               -74.000040               None   \n",
       "\n",
       "   trip_duration user_type  year month  \n",
       "0            864       NaN  2013     6  \n",
       "1           1491       NaN  2013     6  \n",
       "2            532       NaN  2013     6  \n",
       "3           1876       NaN  2013     6  \n",
       "4            743       NaN  2013     6  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load cleaned data from saved file\n",
    "ddf = dd.read_parquet(cleaned_dir + '/bike-tripdata_cleaned/combined_dask_df_0910.parquet')\n",
    "#ddf = dd.read_parquet('path_to_parquet_file', columns=['category_column', 'numeric_column']) # read only specific columns\n",
    "#ddf_2020 = dd.read_parquet('path_to_parquet_file/year=2020') # read only specific partition\n",
    "\n",
    "ddf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6facd081-507a-4098-b5ea-aff97cec60d0",
   "metadata": {},
   "source": [
    "Now that all column names are consistent and all data is concatenated, check if categories in some columns are consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea2f9da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique categories:\n",
      "user_type - Index(['member', 'Customer', 'Subscriber', 'casual'], dtype='object')\n",
      "rideable_type - Index(['electric_bike', 'classic_bike', 'docked_bike'], dtype='object')\n",
      "Unique string:\n",
      "start_station id, unique: 3318\n",
      "start_station name, unique: 2581\n",
      "end_station_id, unique: 3359\n",
      "end_station name, unique: 2610\n"
     ]
    }
   ],
   "source": [
    "print('Unique categories:')\n",
    "print(f'user_type - {ddf[\"user_type\"].cat.as_known().cat.categories}')\n",
    "print(f'rideable_type - {ddf[\"rideable_type\"].cat.as_known().cat.categories}')\n",
    "\n",
    "print('Unique string:')\n",
    "print('start_station id, unique: '+str(ddf.start_station_id.nunique().compute()))\n",
    "print('start_station name, unique: '+str(ddf.start_station_name.nunique().compute()))\n",
    "print('end_station_id, unique: '+str(ddf.end_station_id.nunique().compute()))\n",
    "print('end_station name, unique: '+str(ddf.end_station_name.nunique().compute()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "287d6885-941c-46aa-832b-0afc6e16cab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bike_id                            float64\n",
      "birth_year                           int32\n",
      "end_datetime                datetime64[ns]\n",
      "end_station_id                     float32\n",
      "end_station_latitude               float64\n",
      "end_station_longitude              float64\n",
      "end_station_name           string[pyarrow]\n",
      "gender                            category\n",
      "ride_id                    string[pyarrow]\n",
      "rideable_type                     category\n",
      "start_datetime              datetime64[ns]\n",
      "start_station_id                   float32\n",
      "start_station_latitude             float64\n",
      "start_station_longitude            float64\n",
      "start_station_name         string[pyarrow]\n",
      "trip_duration                        int32\n",
      "user_type                         category\n",
      "year                              category\n",
      "month                             category\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(ddf.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a83501c2-a573-4a9a-82ee-91557b7b83bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory usage of ddf: 17.28 GB\n"
     ]
    }
   ],
   "source": [
    "# Check memory usage of ddf\n",
    "memory_usage = ddf.memory_usage(deep=True).compute()\n",
    "total_memory_usage = memory_usage.sum() # total memory usage in bytes\n",
    "print(f\"Total memory usage of ddf: {total_memory_usage / (1024**3):.2f} GB\") # convert to GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ccbd18",
   "metadata": {},
   "source": [
    "Okay, this shows that the ddf still requires a lot of memory, therefore I will see if I can further reduce this (by changing to the most efficient dtype, and rounding floats)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eedd504c-5fe0-4f72-a3f7-6292b905da37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bike_id (min, max): 14529.0, 49985.0\n",
      "start_station_id (min, max): 72.0, 8897.0498046875\n",
      "end_station_id (min, max): 72.0, 8897.0498046875\n",
      "trip_duration (min, max): 0, 20260212\n"
     ]
    }
   ],
   "source": [
    "# Additional adjustments of dtype - check min and max, to decide which dtype can represent all values\n",
    "\n",
    "print(f\"bike_id (min, max): {ddf['bike_id'].dropna().min().compute()}, {ddf['bike_id'].dropna().max().compute()}\")\n",
    "print(f\"start_station_id (min, max): {ddf['start_station_id'].dropna().min().compute()}, {ddf['start_station_id'].dropna().max().compute()}\")\n",
    "print(f\"end_station_id (min, max): {ddf['end_station_id'].dropna().min().compute()}, {ddf['end_station_id'].dropna().max().compute()}\")\n",
    "print(f\"trip_duration (min, max): {ddf['trip_duration'].dropna().min().compute()}, {ddf['trip_duration'].dropna().max().compute()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bea7c17e-bf93-4571-b38a-56a222064104",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   3206.0000\n",
       "0   8451.0703\n",
       "0   3383.0000\n",
       "0   6889.1201\n",
       "0   4122.0298\n",
       "       ...   \n",
       "0   6190.0298\n",
       "0   8485.0098\n",
       "0   3900.0000\n",
       "1   7820.0498\n",
       "0   8472.0596\n",
       "Name: start_station_id, Length: 3319, dtype: float32"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ddf['start_station_id'].unique().compute() # -> decimals, I didnt expect that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b332602-d447-4cc6-af30-d9eca417448e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bike_id</th>\n",
       "      <th>birth_year</th>\n",
       "      <th>end_datetime</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>end_station_latitude</th>\n",
       "      <th>end_station_longitude</th>\n",
       "      <th>end_station_name</th>\n",
       "      <th>gender</th>\n",
       "      <th>ride_id</th>\n",
       "      <th>rideable_type</th>\n",
       "      <th>start_datetime</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>start_station_latitude</th>\n",
       "      <th>start_station_longitude</th>\n",
       "      <th>start_station_name</th>\n",
       "      <th>trip_duration</th>\n",
       "      <th>user_type</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19678</td>\n",
       "      <td>1983</td>\n",
       "      <td>2013-06-01 00:11:36</td>\n",
       "      <td>434</td>\n",
       "      <td>40.743174</td>\n",
       "      <td>-74.003664</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-06-01 00:00:01</td>\n",
       "      <td>444</td>\n",
       "      <td>40.742354</td>\n",
       "      <td>-73.989151</td>\n",
       "      <td>None</td>\n",
       "      <td>695</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16649</td>\n",
       "      <td>1984</td>\n",
       "      <td>2013-06-01 00:11:41</td>\n",
       "      <td>434</td>\n",
       "      <td>40.743174</td>\n",
       "      <td>-74.003664</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-06-01 00:00:08</td>\n",
       "      <td>444</td>\n",
       "      <td>40.742354</td>\n",
       "      <td>-73.989151</td>\n",
       "      <td>None</td>\n",
       "      <td>693</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bike_id  birth_year        end_datetime  end_station_id  \\\n",
       "0    19678        1983 2013-06-01 00:11:36             434   \n",
       "0    16649        1984 2013-06-01 00:11:41             434   \n",
       "\n",
       "   end_station_latitude  end_station_longitude end_station_name gender  \\\n",
       "0             40.743174             -74.003664             None    NaN   \n",
       "0             40.743174             -74.003664             None    NaN   \n",
       "\n",
       "  ride_id rideable_type      start_datetime  start_station_id  \\\n",
       "0    None           NaN 2013-06-01 00:00:01               444   \n",
       "0    None           NaN 2013-06-01 00:00:08               444   \n",
       "\n",
       "   start_station_latitude  start_station_longitude start_station_name  \\\n",
       "0               40.742354               -73.989151               None   \n",
       "0               40.742354               -73.989151               None   \n",
       "\n",
       "   trip_duration user_type  year month  \n",
       "0            695       NaN  2013     6  \n",
       "0            693       NaN  2013     6  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Additional adjustments of dtype, based on min and max value in the dataset\n",
    "\n",
    "# bike_id -> overlooked. Ideally, do it in the previous cleaning step, but since that takes a long time to run and I donÂ´t have much time left, IÂ´m doing it here for now\n",
    "ddf['bike_id'] = pd.to_numeric(ddf['bike_id'], errors='coerce')\n",
    "ddf['bike_id'] = ddf['bike_id'].round(0).astype('Int64')  \n",
    "\n",
    "# start_station_id and end_station_id -> Int64 which can handle NaNs (I didnÂ´t know earlier), and decimals are not needed\n",
    "ddf['start_station_id'] = ddf['start_station_id'].round(0).astype('Int64')\n",
    "ddf['end_station_id'] = ddf['end_station_id'].round(0).astype('Int64')\n",
    "\n",
    "# birth_year can be int16 (range: -32768 to 32767) as NaNs are set to 0 already\n",
    "ddf['birth_year'] = ddf['birth_year'].astype('int16')\n",
    "ddf['trip_duration'] = ddf['trip_duration'].astype('int32') #\n",
    "\n",
    "ddf = ddf.sort_values(by='start_datetime')# sort again to start rental time/date, since ddf does not seem sorted (time starts at 10am instead of midnight)\n",
    "\n",
    "#ddf.compute()\n",
    "ddf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e8bd4a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique categories, after changing \"subscriber\" to \"member\" and \"customer\" to \"casual\", for consistency:\n",
      "user_type - Index(['member', 'casual'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bike_id</th>\n",
       "      <th>birth_year</th>\n",
       "      <th>end_datetime</th>\n",
       "      <th>end_station_id</th>\n",
       "      <th>end_station_latitude</th>\n",
       "      <th>end_station_longitude</th>\n",
       "      <th>end_station_name</th>\n",
       "      <th>gender</th>\n",
       "      <th>ride_id</th>\n",
       "      <th>rideable_type</th>\n",
       "      <th>start_datetime</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>start_station_latitude</th>\n",
       "      <th>start_station_longitude</th>\n",
       "      <th>start_station_name</th>\n",
       "      <th>trip_duration</th>\n",
       "      <th>user_type</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19678</td>\n",
       "      <td>1983</td>\n",
       "      <td>2013-06-01 00:11:36</td>\n",
       "      <td>434</td>\n",
       "      <td>40.743174</td>\n",
       "      <td>-74.003664</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-06-01 00:00:01</td>\n",
       "      <td>444</td>\n",
       "      <td>40.742354</td>\n",
       "      <td>-73.989151</td>\n",
       "      <td>None</td>\n",
       "      <td>695</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16649</td>\n",
       "      <td>1984</td>\n",
       "      <td>2013-06-01 00:11:41</td>\n",
       "      <td>434</td>\n",
       "      <td>40.743174</td>\n",
       "      <td>-74.003664</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-06-01 00:00:08</td>\n",
       "      <td>444</td>\n",
       "      <td>40.742354</td>\n",
       "      <td>-73.989151</td>\n",
       "      <td>None</td>\n",
       "      <td>693</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bike_id  birth_year        end_datetime  end_station_id  \\\n",
       "0    19678        1983 2013-06-01 00:11:36             434   \n",
       "0    16649        1984 2013-06-01 00:11:41             434   \n",
       "\n",
       "   end_station_latitude  end_station_longitude end_station_name gender  \\\n",
       "0             40.743174             -74.003664             None    NaN   \n",
       "0             40.743174             -74.003664             None    NaN   \n",
       "\n",
       "  ride_id rideable_type      start_datetime  start_station_id  \\\n",
       "0    None           NaN 2013-06-01 00:00:01               444   \n",
       "0    None           NaN 2013-06-01 00:00:08               444   \n",
       "\n",
       "   start_station_latitude  start_station_longitude start_station_name  \\\n",
       "0               40.742354               -73.989151               None   \n",
       "0               40.742354               -73.989151               None   \n",
       "\n",
       "   trip_duration user_type  year month  \n",
       "0            695       NaN  2013     6  \n",
       "0            693       NaN  2013     6  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unify string in rideable_type and user_type columns\n",
    "\n",
    "#ddf['rideable_type'] = ddf['rideable_type'].astype('str').str.strip().str.lower().astype('category')# if needed\n",
    "\n",
    "def replace_user_type(df):\n",
    "    df = df.copy()  # make a copy to avoid SettingWithCopyWarning\n",
    "    df['user_type'] = df['user_type'].astype(str).str.strip().str.lower() # temporarily convert to string (object)\n",
    "    \n",
    "    # Replace 'subscriber' with 'member' and 'customer' with 'casual'\n",
    "    df.loc[df['user_type'] == 'subscriber', 'user_type'] = 'member'\n",
    "    df.loc[df['user_type'] == 'customer', 'user_type'] = 'casual'\n",
    "\n",
    "    df['user_type'] = pd.Categorical(df['user_type'], categories=['member', 'casual']) # convert back to category\n",
    "\n",
    "    return df\n",
    "\n",
    "ddf = ddf.map_partitions(replace_user_type)\n",
    "\n",
    "print('Unique categories, after changing \"subscriber\" to \"member\" and \"customer\" to \"casual\", for consistency:')\n",
    "print(f'user_type - {ddf[\"user_type\"].cat.as_known().cat.categories}')\n",
    "ddf.head(2) # check if it worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b82eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf.isna().sum().compute() # check how many nans in which columns -- taking too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754151e6-911e-4d16-a23a-790732d3569a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows when nans in important columns\n",
    "# important_col = ['end_datetime','start_datetime','end_station_latitude','start_station_latitude',\n",
    "#                  'gender','birth_year','rideable_type','trip_duration','user_type']\n",
    "# ddf.dropna(subset=important_col, inplace=True) # drop all rows where entry is NaN in important columns (that I will use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce0e356-438f-42f8-ad15-eb6eac93ec7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ddf[ddf['end_station_latitude'].isna()].compute().head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40691221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get some idea of the df content\n",
    "\n",
    "# print('Mean trip duration: ')\n",
    "# print(ddf.groupby('user_type').agg({'trip_duration': 'mean'}).compute())\n",
    "# ddf.groupby('gender').agg({'trip_duration': 'mean'}).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0930bbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save further cleaned ddf again, overwriting\n",
    "ddf.to_parquet(cleaned_dir + '/combined_dask_df_cleaned.parquet', engine='pyarrow', partition_on=['year', 'month'], write_index=False) # write, partitioned on year and month\n",
    "print('ddf saved to parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c728ab19-32e3-4cc2-a243-09b22ab12fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If dataset is too big for the code to run smoothly you can save a random sample here\n",
    "df_sample = ddf.sample(frac=0.01)\n",
    "df_sample.to_parquet(os.path.join(cleaned_dir,'df_rides_1010_sampled1%.parquet'), engine='pyarrow', write_index=False)\n",
    "print('ddf saved to sampled1%')\n",
    "df_sample = ddf.sample(frac=0.001)\n",
    "df_sample.to_parquet(os.path.join(cleaned_dir,'df_rides_1010_sampled0_1%.parquet'), engine='pyarrow', write_index=False)\n",
    "print('ddf saved to sampled0.1%')\n",
    "df_sample = ddf.sample(frac=0.0001)\n",
    "df_sample.to_parquet(os.path.join(cleaned_dir,'df_rides_1010_sampled0_01%.parquet'), engine='pyarrow', write_index=False)\n",
    "print('ddf saved to sampled0.001%')\n",
    "df_sample = ddf.sample(frac=0.1)\n",
    "df_sample.to_parquet(os.path.join(cleaned_dir,'df_rides_1010_sampled10%.parquet'), engine='pyarrow', partition_on=['year'], write_index=False)\n",
    "print('ddf saved to sampled10%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62184dca-ebd7-4f90-b244-4b960aeb574a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check memory usage of ddf after additional data cleaning\n",
    "total_memory_usage = ddf.memory_usage(deep=True).sum().compute() # total memory usage in bytes\n",
    "print(f\"Total memory usage of ddf after additional data cleaning: {total_memory_usage / (1024**3):.2f} GB\") # convert to GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41850b0f",
   "metadata": {},
   "source": [
    "## Collision data\n",
    "### 1. Data inspection\n",
    "### 2. Cleaning: column names, entries, dtype, missing data\n",
    "### 3. Add column 'bike_involved' to access bike accident\n",
    "### 4. Save cleaned data as csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8f1dea-a94a-40e8-9142-a2e1da86ae52",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = current_dir + '/raw_data/Motor_Vehicle_Collisions_-_Crashes_20240922.csv' # load file\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069ebd01-8ab1-4187-8603-efc51e688392",
   "metadata": {},
   "source": [
    "### 1. Data inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a44cc3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2120518, 29)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRASH DATE</th>\n",
       "      <th>CRASH TIME</th>\n",
       "      <th>BOROUGH</th>\n",
       "      <th>ZIP CODE</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>LOCATION</th>\n",
       "      <th>ON STREET NAME</th>\n",
       "      <th>CROSS STREET NAME</th>\n",
       "      <th>OFF STREET NAME</th>\n",
       "      <th>...</th>\n",
       "      <th>CONTRIBUTING FACTOR VEHICLE 2</th>\n",
       "      <th>CONTRIBUTING FACTOR VEHICLE 3</th>\n",
       "      <th>CONTRIBUTING FACTOR VEHICLE 4</th>\n",
       "      <th>CONTRIBUTING FACTOR VEHICLE 5</th>\n",
       "      <th>COLLISION_ID</th>\n",
       "      <th>VEHICLE TYPE CODE 1</th>\n",
       "      <th>VEHICLE TYPE CODE 2</th>\n",
       "      <th>VEHICLE TYPE CODE 3</th>\n",
       "      <th>VEHICLE TYPE CODE 4</th>\n",
       "      <th>VEHICLE TYPE CODE 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>09/11/2021</td>\n",
       "      <td>2:39</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WHITESTONE EXPRESSWAY</td>\n",
       "      <td>20 AVENUE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4455765</td>\n",
       "      <td>Sedan</td>\n",
       "      <td>Sedan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>03/26/2022</td>\n",
       "      <td>11:45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>QUEENSBORO BRIDGE UPPER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4513547</td>\n",
       "      <td>Sedan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>06/29/2022</td>\n",
       "      <td>6:55</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>THROGS NECK BRIDGE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Unspecified</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4541903</td>\n",
       "      <td>Sedan</td>\n",
       "      <td>Pick-up Truck</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>09/11/2021</td>\n",
       "      <td>9:35</td>\n",
       "      <td>BROOKLYN</td>\n",
       "      <td>11208.0</td>\n",
       "      <td>40.667202</td>\n",
       "      <td>-73.866500</td>\n",
       "      <td>(40.667202, -73.8665)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1211      LORING AVENUE</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4456314</td>\n",
       "      <td>Sedan</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12/14/2021</td>\n",
       "      <td>8:13</td>\n",
       "      <td>BROOKLYN</td>\n",
       "      <td>11233.0</td>\n",
       "      <td>40.683304</td>\n",
       "      <td>-73.917274</td>\n",
       "      <td>(40.683304, -73.917274)</td>\n",
       "      <td>SARATOGA AVENUE</td>\n",
       "      <td>DECATUR STREET</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4486609</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   CRASH DATE CRASH TIME   BOROUGH ZIP CODE   LATITUDE  LONGITUDE  \\\n",
       "0  09/11/2021       2:39       NaN      NaN        NaN        NaN   \n",
       "1  03/26/2022      11:45       NaN      NaN        NaN        NaN   \n",
       "2  06/29/2022       6:55       NaN      NaN        NaN        NaN   \n",
       "3  09/11/2021       9:35  BROOKLYN  11208.0  40.667202 -73.866500   \n",
       "4  12/14/2021       8:13  BROOKLYN  11233.0  40.683304 -73.917274   \n",
       "\n",
       "                  LOCATION           ON STREET NAME CROSS STREET NAME  \\\n",
       "0                      NaN    WHITESTONE EXPRESSWAY         20 AVENUE   \n",
       "1                      NaN  QUEENSBORO BRIDGE UPPER               NaN   \n",
       "2                      NaN       THROGS NECK BRIDGE               NaN   \n",
       "3    (40.667202, -73.8665)                      NaN               NaN   \n",
       "4  (40.683304, -73.917274)          SARATOGA AVENUE    DECATUR STREET   \n",
       "\n",
       "           OFF STREET NAME  ...  CONTRIBUTING FACTOR VEHICLE 2  \\\n",
       "0                      NaN  ...                    Unspecified   \n",
       "1                      NaN  ...                            NaN   \n",
       "2                      NaN  ...                    Unspecified   \n",
       "3  1211      LORING AVENUE  ...                            NaN   \n",
       "4                      NaN  ...                            NaN   \n",
       "\n",
       "   CONTRIBUTING FACTOR VEHICLE 3  CONTRIBUTING FACTOR VEHICLE 4  \\\n",
       "0                            NaN                            NaN   \n",
       "1                            NaN                            NaN   \n",
       "2                            NaN                            NaN   \n",
       "3                            NaN                            NaN   \n",
       "4                            NaN                            NaN   \n",
       "\n",
       "   CONTRIBUTING FACTOR VEHICLE 5  COLLISION_ID  VEHICLE TYPE CODE 1  \\\n",
       "0                            NaN       4455765                Sedan   \n",
       "1                            NaN       4513547                Sedan   \n",
       "2                            NaN       4541903                Sedan   \n",
       "3                            NaN       4456314                Sedan   \n",
       "4                            NaN       4486609                  NaN   \n",
       "\n",
       "   VEHICLE TYPE CODE 2  VEHICLE TYPE CODE 3 VEHICLE TYPE CODE 4  \\\n",
       "0                Sedan                  NaN                 NaN   \n",
       "1                  NaN                  NaN                 NaN   \n",
       "2        Pick-up Truck                  NaN                 NaN   \n",
       "3                  NaN                  NaN                 NaN   \n",
       "4                  NaN                  NaN                 NaN   \n",
       "\n",
       "  VEHICLE TYPE CODE 5  \n",
       "0                 NaN  \n",
       "1                 NaN  \n",
       "2                 NaN  \n",
       "3                 NaN  \n",
       "4                 NaN  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0014fe40-d02e-477e-b793-95e43d10521b",
   "metadata": {},
   "source": [
    "### 2. Cleaning: column names, entries, dtype, missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "157e976e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['crash_date', 'crash_time', 'borough', 'zip_code', 'latitude',\n",
      "       'longitude', 'location', 'on_street_name', 'cross_street_name',\n",
      "       'off_street_name', 'number_of_persons_injured',\n",
      "       'number_of_persons_killed', 'number_of_pedestrians_injured',\n",
      "       'number_of_pedestrians_killed', 'number_of_cyclist_injured',\n",
      "       'number_of_cyclist_killed', 'number_of_motorist_injured',\n",
      "       'number_of_motorist_killed', 'contributing_factor_vehicle_1',\n",
      "       'contributing_factor_vehicle_2', 'contributing_factor_vehicle_3',\n",
      "       'contributing_factor_vehicle_4', 'contributing_factor_vehicle_5',\n",
      "       'collision_id', 'vehicle_type_code_1', 'vehicle_type_code_2',\n",
      "       'vehicle_type_code_3', 'vehicle_type_code_4', 'vehicle_type_code_5'],\n",
      "      dtype='object')\n",
      "crash_date                        object\n",
      "crash_time                        object\n",
      "borough                           object\n",
      "zip_code                          object\n",
      "latitude                         float64\n",
      "longitude                        float64\n",
      "location                          object\n",
      "on_street_name                    object\n",
      "cross_street_name                 object\n",
      "off_street_name                   object\n",
      "number_of_persons_injured        float64\n",
      "number_of_persons_killed         float64\n",
      "number_of_pedestrians_injured      int64\n",
      "number_of_pedestrians_killed       int64\n",
      "number_of_cyclist_injured          int64\n",
      "number_of_cyclist_killed           int64\n",
      "number_of_motorist_injured         int64\n",
      "number_of_motorist_killed          int64\n",
      "contributing_factor_vehicle_1     object\n",
      "contributing_factor_vehicle_2     object\n",
      "contributing_factor_vehicle_3     object\n",
      "contributing_factor_vehicle_4     object\n",
      "contributing_factor_vehicle_5     object\n",
      "collision_id                       int64\n",
      "vehicle_type_code_1               object\n",
      "vehicle_type_code_2               object\n",
      "vehicle_type_code_3               object\n",
      "vehicle_type_code_4               object\n",
      "vehicle_type_code_5               object\n",
      "dtype: object\n",
      "                               Nan_count    Total\n",
      "crash_date                             0  2120518\n",
      "crash_time                             0  2120518\n",
      "borough                           659498  2120518\n",
      "zip_code                          659758  2120518\n",
      "latitude                          247820  2120518\n",
      "longitude                         247820  2120518\n",
      "location                          247820  2120518\n",
      "on_street_name                    453598  2120518\n",
      "cross_street_name                 807416  2120518\n",
      "off_street_name                  1759293  2120518\n",
      "number_of_persons_injured             18  2120518\n",
      "number_of_persons_killed              31  2120518\n",
      "number_of_pedestrians_injured          0  2120518\n",
      "number_of_pedestrians_killed           0  2120518\n",
      "number_of_cyclist_injured              0  2120518\n",
      "number_of_cyclist_killed               0  2120518\n",
      "number_of_motorist_injured             0  2120518\n",
      "number_of_motorist_killed              0  2120518\n",
      "contributing_factor_vehicle_1       7107  2120518\n",
      "contributing_factor_vehicle_2     331898  2120518\n",
      "contributing_factor_vehicle_3    1968151  2120518\n",
      "contributing_factor_vehicle_4    2085953  2120518\n",
      "contributing_factor_vehicle_5    2111123  2120518\n",
      "collision_id                           0  2120518\n",
      "vehicle_type_code_1                14384  2120518\n",
      "vehicle_type_code_2               411361  2120518\n",
      "vehicle_type_code_3              1973843  2120518\n",
      "vehicle_type_code_4              2087182  2120518\n",
      "vehicle_type_code_5              2111410  2120518\n"
     ]
    }
   ],
   "source": [
    "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_') # clean column names\n",
    "print(df.columns)\n",
    "print(df.dtypes)\n",
    "\n",
    "# How many missing values (nans) and where\n",
    "summary_table = pd.DataFrame({\n",
    "    'Nan_count': df.isna().sum(),\n",
    "    'Total': df.shape[0]\n",
    "})\n",
    "print(summary_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fefd3a97-b41d-4074-b286-7ba293ffc6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        number_of_persons_injured  number_of_pedestrians_injured  \\\n",
      "182614                        NaN                              0   \n",
      "569936                        NaN                              0   \n",
      "619341                        NaN                              0   \n",
      "669416                        NaN                              0   \n",
      "712527                        NaN                              0   \n",
      "\n",
      "        number_of_cyclist_injured  number_of_motorist_injured  \n",
      "182614                          1                           0  \n",
      "569936                          0                           1  \n",
      "619341                          0                           1  \n",
      "669416                          0                           0  \n",
      "712527                          0                           1  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crash_date</th>\n",
       "      <th>crash_time</th>\n",
       "      <th>borough</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>location</th>\n",
       "      <th>on_street_name</th>\n",
       "      <th>cross_street_name</th>\n",
       "      <th>off_street_name</th>\n",
       "      <th>...</th>\n",
       "      <th>contributing_factor_vehicle_2</th>\n",
       "      <th>contributing_factor_vehicle_3</th>\n",
       "      <th>contributing_factor_vehicle_4</th>\n",
       "      <th>contributing_factor_vehicle_5</th>\n",
       "      <th>collision_id</th>\n",
       "      <th>vehicle_type_code_1</th>\n",
       "      <th>vehicle_type_code_2</th>\n",
       "      <th>vehicle_type_code_3</th>\n",
       "      <th>vehicle_type_code_4</th>\n",
       "      <th>vehicle_type_code_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [crash_date, crash_time, borough, zip_code, latitude, longitude, location, on_street_name, cross_street_name, off_street_name, number_of_persons_injured, number_of_persons_killed, number_of_pedestrians_injured, number_of_pedestrians_killed, number_of_cyclist_injured, number_of_cyclist_killed, number_of_motorist_injured, number_of_motorist_killed, contributing_factor_vehicle_1, contributing_factor_vehicle_2, contributing_factor_vehicle_3, contributing_factor_vehicle_4, contributing_factor_vehicle_5, collision_id, vehicle_type_code_1, vehicle_type_code_2, vehicle_type_code_3, vehicle_type_code_4, vehicle_type_code_5]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 29 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correct the columns number_of_persons_injured and number_of_persons_killed, that contain nan\n",
    "print(df.loc[df['number_of_persons_injured'].isna(),['number_of_persons_injured','number_of_pedestrians_injured','number_of_cyclist_injured',\n",
    "                                                'number_of_motorist_injured']].head()) # show \n",
    "\n",
    "df.loc[df['number_of_persons_injured'].isna(), 'number_of_persons_injured'] = df['number_of_pedestrians_injured'] + df['number_of_cyclist_injured'] + df['number_of_motorist_injured']\n",
    "df.loc[df['number_of_persons_killed'].isna(), 'number_of_persons_killed'] = df['number_of_pedestrians_killed'] + df['number_of_cyclist_killed'] + df['number_of_motorist_killed']\n",
    "                                                                               \n",
    "df[df['number_of_persons_injured'].isna()] # show again after correction                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "99a6dc1f-8efc-4f14-b2a8-b1268e29f874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change column names 'longitude' and 'latitude' for later combining with the bike ride dataset, and number_of_/cyclist/motorist to avoid confusion\n",
    "df = df.rename(columns={'latitude': 'accident_latitude', 'longitude': 'accident_longitude', 'number_of_cyclist_injured':'number_of_cyclists_injured',\n",
    "                        'number_of_cyclist_killed':'number_of_cyclists_killed','number_of_motorist_injured':'number_of_motorists_injured',\n",
    "                       'number_of_motorist_killed':'number_of_motorists_killed'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff2d9d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan 11208.0 11233.0 10475.0 11207.0 10017.0 11413.0 11434.0 11217.0\n",
      " 11226.0]\n",
      "[ 2.  1.  0.  4.  3.  5.  7.  6.  9. 18.  8. 11. 17. 10. 14. 15. 12. 13.\n",
      " 40. 16. 20. 22. 31. 19. 27. 32. 24. 43. 21. 23. 34. 25.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2., 3., 4., 8., 5.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check format of entries in columns\n",
    "print(df['zip_code'].unique()[0:10])\n",
    "print(df['number_of_persons_injured'].unique())\n",
    "df['number_of_persons_killed'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "90a86dec-ac7b-483e-8a54-9fb34db6b496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 duplicates removed\n",
      " Unique values in zip_code: \n",
      "<IntegerArray>\n",
      "[ <NA>, 10036, 11223, 11215, 10451, 11234, 11375, 11233, 10007, 10017,\n",
      " ...\n",
      " 10281, 11695, 10282, 11359, 10111, 10803, 10153, 10048, 10168, 10162]\n",
      "Length: 194, dtype: Int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>borough</th>\n",
       "      <th>zip_code</th>\n",
       "      <th>accident_latitude</th>\n",
       "      <th>accident_longitude</th>\n",
       "      <th>location</th>\n",
       "      <th>on_street_name</th>\n",
       "      <th>cross_street_name</th>\n",
       "      <th>off_street_name</th>\n",
       "      <th>number_of_persons_injured</th>\n",
       "      <th>number_of_persons_killed</th>\n",
       "      <th>...</th>\n",
       "      <th>contributing_factor_vehicle_5</th>\n",
       "      <th>collision_id</th>\n",
       "      <th>vehicle_type_code_1</th>\n",
       "      <th>vehicle_type_code_2</th>\n",
       "      <th>vehicle_type_code_3</th>\n",
       "      <th>vehicle_type_code_4</th>\n",
       "      <th>vehicle_type_code_5</th>\n",
       "      <th>crash_datetime</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>unknown</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>40.697753</td>\n",
       "      <td>-73.813916</td>\n",
       "      <td>(40.6977532, -73.8139159)</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>2999940</td>\n",
       "      <td>passenger vehicle</td>\n",
       "      <td>passenger vehicle</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>2012-07-01 00:05:00</td>\n",
       "      <td>2012</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>manhattan</td>\n",
       "      <td>10036</td>\n",
       "      <td>40.762127</td>\n",
       "      <td>-73.997387</td>\n",
       "      <td>(40.7621266, -73.9973865)</td>\n",
       "      <td>11 avenue</td>\n",
       "      <td>west 44 street</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>37632</td>\n",
       "      <td>passenger vehicle</td>\n",
       "      <td>bus</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>2012-07-01 00:05:00</td>\n",
       "      <td>2012</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>brooklyn</td>\n",
       "      <td>11223</td>\n",
       "      <td>40.588868</td>\n",
       "      <td>-73.972745</td>\n",
       "      <td>(40.5888678, -73.9727446)</td>\n",
       "      <td>west 3 street</td>\n",
       "      <td>bouck court</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>116256</td>\n",
       "      <td>passenger vehicle</td>\n",
       "      <td>sport utility / station wagon</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>2012-07-01 00:10:00</td>\n",
       "      <td>2012</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>unknown</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>40.733610</td>\n",
       "      <td>-73.923840</td>\n",
       "      <td>(40.73361, -73.9238405)</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>3044659</td>\n",
       "      <td>passenger vehicle</td>\n",
       "      <td>passenger vehicle</td>\n",
       "      <td>passenger vehicle</td>\n",
       "      <td>passenger vehicle</td>\n",
       "      <td>unknown</td>\n",
       "      <td>2012-07-01 00:10:00</td>\n",
       "      <td>2012</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>brooklyn</td>\n",
       "      <td>11215</td>\n",
       "      <td>40.677406</td>\n",
       "      <td>-73.983048</td>\n",
       "      <td>(40.6774056, -73.9830482)</td>\n",
       "      <td>4 avenue</td>\n",
       "      <td>union street</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>175808</td>\n",
       "      <td>unknown</td>\n",
       "      <td>bicycle</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>unknown</td>\n",
       "      <td>2012-07-01 00:20:00</td>\n",
       "      <td>2012</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     borough  zip_code  accident_latitude  accident_longitude  \\\n",
       "0    unknown      <NA>          40.697753          -73.813916   \n",
       "1  manhattan     10036          40.762127          -73.997387   \n",
       "2   brooklyn     11223          40.588868          -73.972745   \n",
       "3    unknown      <NA>          40.733610          -73.923840   \n",
       "4   brooklyn     11215          40.677406          -73.983048   \n",
       "\n",
       "                    location on_street_name cross_street_name off_street_name  \\\n",
       "0  (40.6977532, -73.8139159)        unknown           unknown         unknown   \n",
       "1  (40.7621266, -73.9973865)      11 avenue    west 44 street         unknown   \n",
       "2  (40.5888678, -73.9727446)  west 3 street       bouck court         unknown   \n",
       "3    (40.73361, -73.9238405)        unknown           unknown         unknown   \n",
       "4  (40.6774056, -73.9830482)       4 avenue      union street         unknown   \n",
       "\n",
       "   number_of_persons_injured  number_of_persons_killed  ...  \\\n",
       "0                          1                         0  ...   \n",
       "1                          0                         0  ...   \n",
       "2                          0                         0  ...   \n",
       "3                          1                         0  ...   \n",
       "4                          0                         0  ...   \n",
       "\n",
       "   contributing_factor_vehicle_5  collision_id  vehicle_type_code_1  \\\n",
       "0                        unknown       2999940    passenger vehicle   \n",
       "1                        unknown         37632    passenger vehicle   \n",
       "2                        unknown        116256    passenger vehicle   \n",
       "3                        unknown       3044659    passenger vehicle   \n",
       "4                        unknown        175808              unknown   \n",
       "\n",
       "             vehicle_type_code_2  vehicle_type_code_3  vehicle_type_code_4  \\\n",
       "0              passenger vehicle              unknown              unknown   \n",
       "1                            bus              unknown              unknown   \n",
       "2  sport utility / station wagon              unknown              unknown   \n",
       "3              passenger vehicle    passenger vehicle    passenger vehicle   \n",
       "4                        bicycle              unknown              unknown   \n",
       "\n",
       "  vehicle_type_code_5      crash_datetime  year month  \n",
       "0             unknown 2012-07-01 00:05:00  2012     7  \n",
       "1             unknown 2012-07-01 00:05:00  2012     7  \n",
       "2             unknown 2012-07-01 00:10:00  2012     7  \n",
       "3             unknown 2012-07-01 00:10:00  2012     7  \n",
       "4             unknown 2012-07-01 00:20:00  2012     7  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtype_dict = { \n",
    "    'Int64': ['zip_code'],\n",
    "    'float64': ['accident_latitude', 'accident_longitude'],\n",
    "    'str': ['borough','on_street_name','cross_street_name','off_street_name','contributing_factor_vehicle_1', 'contributing_factor_vehicle_2',\n",
    "           'contributing_factor_vehicle_3','contributing_factor_vehicle_4','contributing_factor_vehicle_5','vehicle_type_code_1',\n",
    "           'vehicle_type_code_2','vehicle_type_code_3','vehicle_type_code_4','vehicle_type_code_5'], \n",
    "    'int8': ['number_of_persons_injured', 'number_of_persons_killed', 'number_of_pedestrians_injured', 'number_of_pedestrians_killed', \n",
    "             'number_of_cyclists_injured', 'number_of_cyclists_killed', 'number_of_motorists_injured', 'number_of_motorists_killed'],\n",
    "    'datetime64[ns]' : ['crash_datetime']\n",
    "}\n",
    "\n",
    "dtype_mapping = {}\n",
    "for dtype, columns in dtype_dict.items():\n",
    "    for col in columns:\n",
    "        dtype_mapping[col] = dtype\n",
    "\n",
    "for col in df.columns:\n",
    "    if col not in dtype_mapping.keys():\n",
    "        continue\n",
    "    elif col == 'zip_code': # canÂ´t convert to int directly because it has string with empty entries\n",
    "        df[col] = pd.to_numeric(df[col].str.strip(), errors='coerce').astype('float32')\n",
    "        df[col] = df[col].round(0).astype('Int64');\n",
    "    elif col in dtype_dict['str']: # for categorical data, replace nans with 'unknown' cat\n",
    "        df[col].fillna('unknown', inplace=True)\n",
    "        df[col] = df[col].astype(dtype_mapping[col])\n",
    "        df[col]  = df[col].str.strip().str.lower() # clean string entries\n",
    "        # df[col] = df[col].astype('category') # string to category, as it needs less memory\n",
    "        # df[col] = df[col].cat.add_categories('unknown').fillna('unknown') # add unknown category for nans\n",
    "    elif col in dtype_dict['int8']:\n",
    "        df[col] = df[col].astype(dtype_mapping[col])\n",
    "        df[col].round(0)\n",
    "    else: \n",
    "        df[col] = df[col].astype(dtype_mapping[col])\n",
    "        df[col].fillna(np.nan, inplace=True)\n",
    "            \n",
    "df['crash_datetime'] = pd.to_datetime(df['crash_date'] + ' ' + df['crash_time']) #\n",
    "df.drop(['crash_date','crash_time'], axis=1, inplace=True)\n",
    "df['year'] = df['crash_datetime'].dt.year.astype('int32') # add year column \n",
    "df['month'] = df['crash_datetime'].dt.month.astype('int8') # add month column \n",
    "\n",
    "dupl_before =df.shape[0]\n",
    "df = df.drop_duplicates().sort_values(by='crash_datetime')# drop duplicates and sort to start rental time/date\n",
    "print(f'{dupl_before - df.shape[0]} duplicates removed')\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "print(' Unique values in zip_code: '); print(df.zip_code.unique())\n",
    "df.head() # show cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f9d08181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Nan_count    Total\n",
      "borough                                0  2120518\n",
      "zip_code                         1721072  2120518\n",
      "accident_latitude                 247820  2120518\n",
      "accident_longitude                247820  2120518\n",
      "location                          247820  2120518\n",
      "on_street_name                         0  2120518\n",
      "cross_street_name                      0  2120518\n",
      "off_street_name                        0  2120518\n",
      "number_of_persons_injured              0  2120518\n",
      "number_of_persons_killed               0  2120518\n",
      "number_of_pedestrians_injured          0  2120518\n",
      "number_of_pedestrians_killed           0  2120518\n",
      "number_of_cyclists_injured             0  2120518\n",
      "number_of_cyclists_killed              0  2120518\n",
      "number_of_motorists_injured            0  2120518\n",
      "number_of_motorists_killed             0  2120518\n",
      "contributing_factor_vehicle_1          0  2120518\n",
      "contributing_factor_vehicle_2          0  2120518\n",
      "contributing_factor_vehicle_3          0  2120518\n",
      "contributing_factor_vehicle_4          0  2120518\n",
      "contributing_factor_vehicle_5          0  2120518\n",
      "collision_id                           0  2120518\n",
      "vehicle_type_code_1                    0  2120518\n",
      "vehicle_type_code_2                    0  2120518\n",
      "vehicle_type_code_3                    0  2120518\n",
      "vehicle_type_code_4                    0  2120518\n",
      "vehicle_type_code_5                    0  2120518\n",
      "crash_datetime                         0  2120518\n",
      "year                                   0  2120518\n",
      "month                                  0  2120518\n"
     ]
    }
   ],
   "source": [
    "# Check if df still has missing values (nans) and how many\n",
    "summary_table = pd.DataFrame({\n",
    "    'Nan_count': df.isna().sum(),\n",
    "    'Total': df.shape[0]\n",
    "})\n",
    "\n",
    "print(summary_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa76ab8-4560-4546-83c1-889c204081c3",
   "metadata": {},
   "source": [
    "Missing data in string columns handled well -> all replaced with 'unknown', no nans left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e799370c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zip_code</th>\n",
       "      <th>accident_latitude</th>\n",
       "      <th>accident_longitude</th>\n",
       "      <th>number_of_persons_injured</th>\n",
       "      <th>number_of_persons_killed</th>\n",
       "      <th>number_of_pedestrians_injured</th>\n",
       "      <th>number_of_pedestrians_killed</th>\n",
       "      <th>number_of_cyclists_injured</th>\n",
       "      <th>number_of_cyclists_killed</th>\n",
       "      <th>number_of_motorists_injured</th>\n",
       "      <th>number_of_motorists_killed</th>\n",
       "      <th>collision_id</th>\n",
       "      <th>crash_datetime</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>399446.0</td>\n",
       "      <td>1.872698e+06</td>\n",
       "      <td>1.872698e+06</td>\n",
       "      <td>2.120518e+06</td>\n",
       "      <td>2.120518e+06</td>\n",
       "      <td>2.120518e+06</td>\n",
       "      <td>2.120518e+06</td>\n",
       "      <td>2.120518e+06</td>\n",
       "      <td>2.120518e+06</td>\n",
       "      <td>2.120518e+06</td>\n",
       "      <td>2.120518e+06</td>\n",
       "      <td>2.120518e+06</td>\n",
       "      <td>2120518</td>\n",
       "      <td>2.120518e+06</td>\n",
       "      <td>2.120518e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>10801.207059</td>\n",
       "      <td>4.062109e+01</td>\n",
       "      <td>-7.374009e+01</td>\n",
       "      <td>3.160567e-01</td>\n",
       "      <td>1.526042e-03</td>\n",
       "      <td>5.728270e-02</td>\n",
       "      <td>7.531179e-04</td>\n",
       "      <td>2.758383e-02</td>\n",
       "      <td>1.188389e-04</td>\n",
       "      <td>2.271728e-01</td>\n",
       "      <td>6.305063e-04</td>\n",
       "      <td>3.193116e+06</td>\n",
       "      <td>2017-09-11 06:15:02.005152512</td>\n",
       "      <td>2.017182e+03</td>\n",
       "      <td>6.671396e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>10000.0</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-2.013600e+02</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.200000e+01</td>\n",
       "      <td>2012-07-01 00:05:00</td>\n",
       "      <td>2.012000e+03</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>10065.0</td>\n",
       "      <td>4.066767e+01</td>\n",
       "      <td>-7.397478e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.166249e+06</td>\n",
       "      <td>2015-02-07 09:15:45</td>\n",
       "      <td>2.015000e+03</td>\n",
       "      <td>4.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>11204.0</td>\n",
       "      <td>4.072065e+01</td>\n",
       "      <td>-7.392715e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>3.696518e+06</td>\n",
       "      <td>2017-06-16 16:00:00</td>\n",
       "      <td>2.017000e+03</td>\n",
       "      <td>7.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>11235.0</td>\n",
       "      <td>4.076963e+01</td>\n",
       "      <td>-7.386673e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.226882e+06</td>\n",
       "      <td>2019-10-19 06:29:30</td>\n",
       "      <td>2.019000e+03</td>\n",
       "      <td>1.000000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>11697.0</td>\n",
       "      <td>4.334444e+01</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>4.300000e+01</td>\n",
       "      <td>8.000000e+00</td>\n",
       "      <td>2.700000e+01</td>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>4.000000e+00</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>4.300000e+01</td>\n",
       "      <td>5.000000e+00</td>\n",
       "      <td>4.757303e+06</td>\n",
       "      <td>2024-09-17 23:45:00</td>\n",
       "      <td>2.024000e+03</td>\n",
       "      <td>1.200000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>568.096756</td>\n",
       "      <td>2.046708e+00</td>\n",
       "      <td>3.837215e+00</td>\n",
       "      <td>7.055802e-01</td>\n",
       "      <td>4.124354e-02</td>\n",
       "      <td>2.455219e-01</td>\n",
       "      <td>2.801105e-02</td>\n",
       "      <td>1.658659e-01</td>\n",
       "      <td>1.094386e-02</td>\n",
       "      <td>6.668334e-01</td>\n",
       "      <td>2.745294e-02</td>\n",
       "      <td>1.506296e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.203649e+00</td>\n",
       "      <td>3.397637e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           zip_code  accident_latitude  accident_longitude  \\\n",
       "count      399446.0       1.872698e+06        1.872698e+06   \n",
       "mean   10801.207059       4.062109e+01       -7.374009e+01   \n",
       "min         10000.0       0.000000e+00       -2.013600e+02   \n",
       "25%         10065.0       4.066767e+01       -7.397478e+01   \n",
       "50%         11204.0       4.072065e+01       -7.392715e+01   \n",
       "75%         11235.0       4.076963e+01       -7.386673e+01   \n",
       "max         11697.0       4.334444e+01        0.000000e+00   \n",
       "std      568.096756       2.046708e+00        3.837215e+00   \n",
       "\n",
       "       number_of_persons_injured  number_of_persons_killed  \\\n",
       "count               2.120518e+06              2.120518e+06   \n",
       "mean                3.160567e-01              1.526042e-03   \n",
       "min                 0.000000e+00              0.000000e+00   \n",
       "25%                 0.000000e+00              0.000000e+00   \n",
       "50%                 0.000000e+00              0.000000e+00   \n",
       "75%                 0.000000e+00              0.000000e+00   \n",
       "max                 4.300000e+01              8.000000e+00   \n",
       "std                 7.055802e-01              4.124354e-02   \n",
       "\n",
       "       number_of_pedestrians_injured  number_of_pedestrians_killed  \\\n",
       "count                   2.120518e+06                  2.120518e+06   \n",
       "mean                    5.728270e-02                  7.531179e-04   \n",
       "min                     0.000000e+00                  0.000000e+00   \n",
       "25%                     0.000000e+00                  0.000000e+00   \n",
       "50%                     0.000000e+00                  0.000000e+00   \n",
       "75%                     0.000000e+00                  0.000000e+00   \n",
       "max                     2.700000e+01                  6.000000e+00   \n",
       "std                     2.455219e-01                  2.801105e-02   \n",
       "\n",
       "       number_of_cyclists_injured  number_of_cyclists_killed  \\\n",
       "count                2.120518e+06               2.120518e+06   \n",
       "mean                 2.758383e-02               1.188389e-04   \n",
       "min                  0.000000e+00               0.000000e+00   \n",
       "25%                  0.000000e+00               0.000000e+00   \n",
       "50%                  0.000000e+00               0.000000e+00   \n",
       "75%                  0.000000e+00               0.000000e+00   \n",
       "max                  4.000000e+00               2.000000e+00   \n",
       "std                  1.658659e-01               1.094386e-02   \n",
       "\n",
       "       number_of_motorists_injured  number_of_motorists_killed  collision_id  \\\n",
       "count                 2.120518e+06                2.120518e+06  2.120518e+06   \n",
       "mean                  2.271728e-01                6.305063e-04  3.193116e+06   \n",
       "min                   0.000000e+00                0.000000e+00  2.200000e+01   \n",
       "25%                   0.000000e+00                0.000000e+00  3.166249e+06   \n",
       "50%                   0.000000e+00                0.000000e+00  3.696518e+06   \n",
       "75%                   0.000000e+00                0.000000e+00  4.226882e+06   \n",
       "max                   4.300000e+01                5.000000e+00  4.757303e+06   \n",
       "std                   6.668334e-01                2.745294e-02  1.506296e+06   \n",
       "\n",
       "                      crash_datetime          year         month  \n",
       "count                        2120518  2.120518e+06  2.120518e+06  \n",
       "mean   2017-09-11 06:15:02.005152512  2.017182e+03  6.671396e+00  \n",
       "min              2012-07-01 00:05:00  2.012000e+03  1.000000e+00  \n",
       "25%              2015-02-07 09:15:45  2.015000e+03  4.000000e+00  \n",
       "50%              2017-06-16 16:00:00  2.017000e+03  7.000000e+00  \n",
       "75%              2019-10-19 06:29:30  2.019000e+03  1.000000e+01  \n",
       "max              2024-09-17 23:45:00  2.024000e+03  1.200000e+01  \n",
       "std                              NaN  3.203649e+00  3.397637e+00  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da316c6-24d0-423a-82cb-04aa59f6daa8",
   "metadata": {},
   "source": [
    "### 3. Add column 'bike_involved' to access bike accidents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0974109f-dc6f-49da-982d-31a38e8240ef",
   "metadata": {},
   "source": [
    "Check how many collisions involved a bike; list all entries that contain 'bi', as probably the entries are inconsistent, \n",
    "and they could be 'bike','bicycle' or 'e-bike'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68ae2d38-64b0-4601-9202-3c7b331465ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vehicle_type_code_1\n",
      "['bicycle' 'bike' 'motorbike' 'e bik' 'minibike' 'ebike' 'mobil' 'e- bi'\n",
      " 'e-bik' 'snowmobile' 'cabin' 'bicyc' 'e-bike' 'dirt bike' 'e bike'\n",
      " 'e bike uni' 'e bike w p' 'combinatio' 'liabitiy' 'pedal bike'\n",
      " 'mobility s']\n",
      "vehicle_type_code_2\n",
      "['bicycle' 'bike' 'motorbike' 'minibike' 'e bik' 'ebike' 'e-bik'\n",
      " 'snowmobile' 'mobil' 'big r' 'e/bik' 'e-bike' 'mobile foo' 'dirt bike'\n",
      " 'mobile' 'dirtbike' 'uni e-bike' 'e bike' 'gas bicycl' 'ambiance'\n",
      " 'dart bike' 'moped bike' 'gas bike' 'mobility s' 'citibike' 'e bike w p'\n",
      " 'scooter bi']\n",
      "vehicle_type_code_3\n",
      "['bicycle' 'bike' 'motorbike' 'e-bik' 'e-bike' 'dirt bike']\n",
      "vehicle_type_code_4\n",
      "['bicycle' 'bike' 'e-bike' 'snowmobile' 'motorbike']\n"
     ]
    }
   ],
   "source": [
    "for col in ['vehicle_type_code_1','vehicle_type_code_2','vehicle_type_code_3','vehicle_type_code_4']:\n",
    "    bike_entries = df[df[col].str.contains('bi', case=False, na=False)]\n",
    "    print(col); print(bike_entries[col].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fc5f75-33c7-480d-82a8-781c6dc8d2cf",
   "metadata": {},
   "source": [
    "As we can see, a lot of different words are used to describe 'bike', with many spelling mistakes. To filter out all bikes, we can select all entries containing 'bik' or 'bic' but exclude 'motorbike'. I will now correct these entries and create an additional column with 'bike'/'no_bike', when any of these 4 columns contain a bike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c8f2c2c8-4330-4002-8dec-da2c991d79b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['no_bike' 'bike']\n",
      "  bike_involved vehicle_type_code_1            vehicle_type_code_2  \\\n",
      "0       no_bike   passenger vehicle              passenger vehicle   \n",
      "1       no_bike   passenger vehicle                            bus   \n",
      "2       no_bike   passenger vehicle  sport utility / station wagon   \n",
      "\n",
      "  vehicle_type_code_3 vehicle_type_code_4  \n",
      "0             unknown             unknown  \n",
      "1             unknown             unknown  \n",
      "2             unknown             unknown  \n",
      "   bike_involved vehicle_type_code_1 vehicle_type_code_2 vehicle_type_code_3  \\\n",
      "4           bike             unknown             bicycle             unknown   \n",
      "42          bike                taxi             bicycle             unknown   \n",
      "98          bike   passenger vehicle             bicycle             unknown   \n",
      "\n",
      "   vehicle_type_code_4  \n",
      "4              unknown  \n",
      "42             unknown  \n",
      "98             unknown  \n"
     ]
    }
   ],
   "source": [
    "# Create a mask that checks for 'bik' or 'bic' in any of the 4 columns, excluding 'motorbike'\n",
    "df['bike_involved'] = 'no_bike'\n",
    "# Create a mask that checks for 'bik' or 'bic' in any of the 4 columns, excluding 'motorbike'\n",
    "mask = (\n",
    "    df[['vehicle_type_code_1', 'vehicle_type_code_2', 'vehicle_type_code_3', 'vehicle_type_code_4']]\n",
    "    .apply(lambda col: col.str.contains('bik|bic', case=False, na=False))\n",
    "    .any(axis=1)  # Check if any column contains 'bik' or 'bic'\n",
    ") & (\n",
    "    ~df[['vehicle_type_code_1', 'vehicle_type_code_2', 'vehicle_type_code_3', 'vehicle_type_code_4']]\n",
    "    .apply(lambda col: col.str.contains('motorbike', case=False, na=False))\n",
    "    .any(axis=1)  # Exclude rows where 'motorbike' appears\n",
    ")\n",
    "\n",
    "# Set 'bike_involved' to 'bike' where the condition is met\n",
    "df.loc[mask, 'bike_involved'] = 'bike'\n",
    "\n",
    "print(df['bike_involved'].unique())\n",
    "print(df.loc[df['bike_involved'] == 'no_bike',['bike_involved','vehicle_type_code_1', 'vehicle_type_code_2', 'vehicle_type_code_3', 'vehicle_type_code_4']].head(3))\n",
    "print(df.loc[df['bike_involved'] == 'bike',['bike_involved','vehicle_type_code_1', 'vehicle_type_code_2', 'vehicle_type_code_3', 'vehicle_type_code_4']].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "67ab395c-7ea9-4f85-a9dd-2c3c591a06e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "borough                                  object\n",
      "zip_code                                  Int64\n",
      "accident_latitude                       float64\n",
      "accident_longitude                      float64\n",
      "location                                 object\n",
      "on_street_name                           object\n",
      "cross_street_name                        object\n",
      "off_street_name                          object\n",
      "number_of_persons_injured                  int8\n",
      "number_of_persons_killed                   int8\n",
      "number_of_pedestrians_injured              int8\n",
      "number_of_pedestrians_killed               int8\n",
      "number_of_cyclists_injured                 int8\n",
      "number_of_cyclists_killed                  int8\n",
      "number_of_motorists_injured                int8\n",
      "number_of_motorists_killed                 int8\n",
      "contributing_factor_vehicle_1            object\n",
      "contributing_factor_vehicle_2            object\n",
      "contributing_factor_vehicle_3            object\n",
      "contributing_factor_vehicle_4            object\n",
      "contributing_factor_vehicle_5            object\n",
      "collision_id                              int64\n",
      "vehicle_type_code_1                      object\n",
      "vehicle_type_code_2                      object\n",
      "vehicle_type_code_3                      object\n",
      "vehicle_type_code_4                      object\n",
      "vehicle_type_code_5                      object\n",
      "crash_datetime                   datetime64[ns]\n",
      "year                                      int32\n",
      "month                                      int8\n",
      "bike_involved                            object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e764ba-c8f2-4462-8dfe-1944524d8652",
   "metadata": {},
   "source": [
    "### 4. Save cleaned data as csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3cd92008-4764-48c4-ba6f-a5491bc9836a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(cleaned_dir + '/collisions_cleaned.csv') # save cleaned version"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
